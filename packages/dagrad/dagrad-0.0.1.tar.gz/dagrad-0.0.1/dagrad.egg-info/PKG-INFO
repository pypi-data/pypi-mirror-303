Metadata-Version: 2.1
Name: dagrad
Version: 0.0.1
Summary: A project for DAG learning
Home-page: https://github.com/Duntrain/DAGlearner
Author: Chang Deng
Author-email: changdeng@uchicago.edu
License: MIT
Project-URL: Homepage, https://github.com/Duntrain/DAGlearner
Project-URL: Documentation, https://readthedocs.org
Project-URL: Repository, https://github.com/Duntrain/DAGlearner
Project-URL: Issues, https://github.com/Duntrain/DAGlearner/issues
Keywords: dagma,notears,topo,causal discovery,bayesian network,structure learning
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Mathematics
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: dagma
Requires-Dist: scipy
Requires-Dist: python-igraph
Requires-Dist: torch
Requires-Dist: scikit-learn

# dagrad: Gradient-based structure learning for causal DAGs

## tl;dr

The `dagrad` Python library is built to provide researchers with an extensible, modular platform for developing and experimenting with continuous score-based structure learning methods, building upon the introduction of the [NOTEARS][notears] framework for differentiable structure learning. It also functions as an updated repository of [state-of-the-art implementations][#features] for various methods.

`dagrad` is a comprehensive suite of state-of-the-art __continuous score-based__ structure learning (aka causal discovery) methods, integrating them into a __unified__ framework. Users have the flexibility to use predefined loss functions or customize their own, along with different acyclicity constraints, regularizers, and optimizers with ease. Additionally, the library supports GPU acceleration, enabling faster computations.

## Introduction 

A directed acyclic graphical model (also known as a Bayesian network) with `d` nodes represents a distribution over a random vector of size `d`. The focus of this library is on Bayesian Network Structure Learning (BNSL): Given samples $\mathbf{X}$ from a distribution, how can we estimate the underlying graph `G`?

The problem can be formulated as the following differentiable continuous optimization:

```math
\begin{array}{cl}
\min _{W \in \mathbb{R}^{d \times d}} & Q(W;\mathbf{X}) \\
\text { subject to } & h(W) = 0,
\end{array}
```
This formulation is versatile enough to encompass both linear and nonlinear models with any smooth objective (e.g. log-likelihood, least-squares, cross-entropy, etc.).

`dagrad` provides a unified framework that allows users to employ either predefined or customized loss functions $Q(W;\mathbf{X})$, acyclicity constraints $h(W)$, and choose between first-order or second-order optimizers:
```math
loss~function + acyclicity~constraint + optimizer => structure~learning
```

GPU acceleration is also supported.


## Run a simple demo
Use `setup.py` to install the dependencies (we recommend using virtualenv or conda). The simplest way to try out `dagrad` is to run the following example:

```bash
$ git clone https://github.com/Duntrain/DAGlearner.git
$ cd DAGlearner/
$ pip install -e .
$ cd dagrad
$ python -m dagrad.daglearner
```
The above runs the original NOTEARS method [[1]][notears] on a randomly generated 10-node Erdos-Renyi graph with 1000 samples. The output should look like the below:
```
{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 10}
```

Want to try more examples? See an example in this [iPython notebook][examples].

## Features
Below is an overview of the functionalities provided by the package:


| __Method(`method`)__ | __Model(`model`)__ |__Loss(`loss_fn`)__ |__Regularizers(`reg`)__|__h(`h_fn`)__ |__Optimizer(`optimizer`)__  | __Computation Library(`compute_lib`)__ |__Device(`device`)__|
| --------   | --------  |----|---|-------------------|----| ----------| --------------| 
|`'notears'`[1]    | `'linear'`,<br>`'nonlinear'`   |`'l2'`, `'logll'`, `'user_loss'`|`'l1'`<br> `'l2'`<br> `'mcp'`<br> `'none'`<br>`'user_reg'` |`'h_exp_sq'`<br>`'h_poly_sq'`<br>`'h_poly_abs'`<br>`'user_h'` |Adam(`'adam'`),<br>LBFGS(`'lbfgs'`)        |  Numpy(`'numpy'`),<br>Torch(`'torch'`),  |  CPU(`'cpu'`)<br>CUDA(`'cuda'`)      | 
| `'dagma'`[2]      | `'linear'`,<br>`'nonlinear'`    |`'l2'`, <br>  `'logll'`, <br> `'user_loss'`|`'l1'`<br> `'l2'`<br> `'mcp'`<br> `'none'`, `'user_reg'`| `'h_logdet_sq'`<br>`'h_logdet_abs'`<br>`'user_h'` |Adam(`'adam'`)            |  Numpy(`'numpy'`)<br>Torch(`'torch'`)  |  CPU(`'cpu'`)<br>CUDA(`'cuda'`)      |
| `'topo'`[3]       |`'linear'`,<br>`'nonlinear'`   |`'l2'`,<br> `'logll'`,<br>`'user_loss'`| `'l1'`<br> `'l2'`<br> `'mcp'`<br> `'none'`<br> `'user_reg'` |`'h_exp_topo'`<br>`'h_logdet_topo'`<br>`'h_poly_topo'`<br>`'user_h'` |Adam(`'adam'`),<br> LBFGS(`'lbfgs'`)|  Numpy(`'numpy'`) for linear <br> Torch(`'torch'`) for nonlinear |  CPU(`'cpu'`)     | 


<!-- | __Method(`method`)__ | __Model(`model`)__ |__Loss(`loss_fn`)__ |__Regularizers(`reg`)__|__h(`h_fn`)__ |__Optimizer(`optimizer`)__ | __Computation Library(`compute_lib`)__|__Device(`device`)__|
| --------   | --------  |----|---|-------------------|----| ----------| --------------| 
| NOTEARS<br>(`'notears'`)[1]     | NonLinear(`'nonlinear'`)   |`'l2'`<br> `'logll'`<br> `'user_loss'`|`'l1'`<br> `'l2'`<br> `'mcp'`<br>`'none'`<br> `'user_reg'` |`'h_exp_sq'`<br>`'h_poly_sq'`<br>`'h_poly_abs'`<br>`'user_h'` |Adam(`'adam'`)<br>LBFGS(`lbfgs`)        |  Torch(`'torch'`)  |  CPU(`'cpu'`)<br>CUDA(`'cuda'`)      | 
| DAGAM<br>(`'dagma'`)[2]       | NonLinear(`'nonlinear'`)     |`'l2'`<br> `'logll'`<br> `'user_loss'`|`'l1'`<br> `'l2'`<br> `'mcp'`<br> `'none'`<br> `'user_reg'`| `'h_logdet_sq'`<br>`'h_logdet_abs'`<br>`'user_h'` |Adam(`'adam'`)<br>LBFGS(`lbfgs`)            |  Torch(`'torch'`)  |  CPU(`'cpu'`)<br>CUDA(`'cuda'`)      |
| TOPO<br>(`'topo'`)[3]      | NonLinear(`'nonlinear'`)     |`'l2'`<br> `'logll'`<br> `'user_loss'`| `'l1'`<br> `'l2'`<br> `'mcp'`<br>`'none'`<br> `'user_reg'` |`'h_exp_topo'`<br>`'h_logdet_topo'`<br>`'h_poly_topo'`<br>`'user_h'` |Adam(`'adam'`)<br>LBFGS(`lbfgs`)|  Torch(`'torch'`) |  CPU(`'cpu'`)     |  -->

- For the linear (`'linear'`) model, the loss function (`loss_fn`) can be configured as logistic loss (`'logistic'`) for all three methods.
- In the linear (`'linear'`) model, the default optimizer (`'optimizer'`) for TOPO (`'topo'`) is [scikit-learn](https://scikit-learn.org/stable/) (`'sklearn'`), a state-of-the-art package for solving linear model problems.
- In the linear (`'linear'`) model, NOTEARS (`'notears'`) and DAGMA (`'dagma'`) also support computation libraries (`compute_lib`) such as Torch (`'torch'`), and can perform computations on either CPU (`'cpu'`) or GPU (`'cuda'`).
## Requirements
- Python 3.7+
- `numpy`
- `scipy`
- `scikit-learn`
- `python-igraph`
- `tqdm`
- `dagma`
- `notears`: installed from github repo
- `torch`: Used for models with GPU acceleration


<!-- ## Contents and Structure
<span style="color:red">This part need be modified once the final structure is determined.</span>
```plaintext
├── README.md               # Project overview and documentation
├── src/                    # Source code directory
│   ├── main.py             # Main entry point of the application
│   ├── utils.py            # Utility functions
│   └── module/             # Additional modules
│       ├── module1.py      # Description of module1
│       └── module2.py      # Description of module2
├── data/                   # Directory for datasets
│   ├── raw/                # Raw, unprocessed data
│   └── processed/          # Processed data ready for analysis
├── tests/                  # Test suite
│   ├── test_main.py        # Tests for main.py
│   └── test_module1.py     # Tests for module1
├── requirements.txt        # Python dependencies
└── docs/                   # Documentation files
    ├── index.md            # Main documentation file
    └── API.md              # API documentation -->


## References

[1] Zheng X, Aragam B, Ravikumar P, & Xing EP [DAGs with NO TEARS: Continuous optimization for structure learning][notears] (NeurIPS 2018, Spotlight).

[2] Zheng X, Dan C, Aragam B, Ravikumar P, & Xing EP [Learning sparse nonparametric DAGs][notearsmlp] (AISTATS 2020).

[3] Bello K, Aragam B, Ravikumar P [DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity Characterization][dagma] (NeurIPS 2022). 

[4] Deng C, Bello K, Aragam B, Ravikumar P [Optimizing NOTEARS Objectives via Topological Swaps][topo] (ICML 2023).

[5] Deng C, Bello K, Ravikumar P, Aragam B [Likelihood-based differentiable structure learning][logll] (NeurIPS 2024).

[notears]: https://arxiv.org/abs/1803.01422
[notearsmlp]: https://arxiv.org/abs/1909.13189
[dagma]: https://arxiv.org/abs/2209.08037
[topo]: https://arxiv.org/abs/2305.17277
[examples]: https://github.com/Duntrain/DAGlearner/blob/master/examples/examples.ipynb
[logll]: https://arxiv.org/pdf/2410.06163?
