# 3. Context and Scope

## 3.1 Context

Engineered Prompts are designed as a generic and scalable approach for communicating with Large Language Models (LLMs). This approach allows them to be used across a wide range of business use cases within an organization, providing developers with the flexibility to implement various patterns and practices that meet their specific needs.

### 3.2 Scope of Use

Engineered Prompts are not limited to any specific domain or application but are intended to be a versatile tool that can be leveraged across different departments and business functions. Whether it’s customer service, content generation, or data analysis, Engineered Prompts can be adapted to suit the particular requirements of each use case.

### 3.3 Organizational Integration

The scalability of Engineered Prompts makes them suitable for organizations looking to expand their AI-driven capabilities. As organizations grow and evolve, Engineered Prompts provide a consistent and reliable framework for interacting with LLMs, enabling the development of an extensive ecosystem that can adapt to new challenges and opportunities.

## 3.4 Open Source and Cloud Models

With the recent addition of support for locally hosted models via the Ollama framework, PromptArchitect has expanded its versatility. It now supports a broader range of AI models, including cloud-based models from providers like Anthropic and OpenAI, as well as open-source models such as Gemma2, Llama3.1, and Mistral, which can be run locally. This integration allows users to choose between leveraging powerful cloud resources or maintaining full control over their data and costs by running models on local hardware.

### System Boundaries

- **Internal System Components**:
  - **Prompt Engine**: Core component responsible for executing engineered prompts across different AI models.
  - **Ollama Integration**: New component enabling support for locally hosted AI models via the Ollama framework.
  - **Configuration Manager**: Handles the setup and management of model configurations, including those for Ollama models.

- **External Systems**:
  - **AI Model Providers**: External APIs from providers like OpenAI and Anthropic, alongside locally hosted models managed by Ollama.
  - **User Interfaces**: External tools and systems that utilize the outputs generated by PromptArchitect, such as web applications, data processing pipelines, and automation scripts.

### Interaction with External Systems

- **Cloud-based AI Models**: PromptArchitect interfaces with external AI models through API calls, sending engineered prompts and receiving processed outputs.
- **Locally Hosted AI Models (Ollama)**: With the new Ollama integration, PromptArchitect can now interact with AI models running on local hardware. This interaction is managed through direct system calls or API interactions with the Ollama framework, allowing for seamless execution of prompts in a local environment.

## 3.5 Scope

### In-Scope

- **Engineered Prompts**: The design, execution, and management of prompts that interact with various AI models, including the newly supported locally hosted models via Ollama.
- **Model Configuration**: Managing configurations for different AI models, ensuring compatibility and optimal performance across cloud-based and local environments.
- **Prompt Execution**: The actual processing of prompts, including integration with both cloud-based and locally hosted models.
- - **Command Line Interface (CLI)**: The ability to run PromptArchitect directly from the CLI, allowing users to specify paths for prompts, completions, templates, and reports.
- **Custom Themes**: The capability to customize the dashboard appearance with themes like the `github-pajamas-theme` and `blue-theme`.
- **Versioning and Documentation**: Comprehensive tracking and documentation of prompt versions and configurations, particularly important when managing different environments (cloud vs. local).
- **Testing and Validation**: Ensuring all prompts are rigorously tested, including new test cases for prompts executed via Ollama-supported models.

### Out-of-Scope

- **Development of AI Models**: The development or fine-tuning of AI models themselves is not within the scope of PromptArchitect. The tool is focused on the interaction with pre-existing models, whether they are cloud-based or locally hosted.
- **Hardware Management**: While PromptArchitect now supports locally hosted models via Ollama, the management of the underlying hardware (e.g., ensuring sufficient computational resources for model execution) is not within its scope.

## 3.6 Assumptions and Dependencies

- **Ollama Framework Availability**: It is assumed that users who wish to run models locally have the Ollama framework properly installed and configured on their hardware.
- **AI Model API Stability**: The continued compatibility of engineered prompts depends on the stability of the APIs provided by external AI model providers, including those integrated via Ollama.
- **Hardware Capabilities**: The successful execution of locally hosted models assumes that the user’s hardware meets the minimum requirements specified by Ollama for running models like Gemma2, Llama3.1, and Mistral.

---
