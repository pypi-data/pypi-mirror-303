1
00:00:00,001 --> 00:00:05,920
Hoi, leuk dat je weer luistert naar een nieuwe aflevering van AIToday Live.

2
00:00:05,920 --> 00:00:08,880
Met vandaag vriend van de show, Marc van Meel.

3
00:00:08,880 --> 00:00:10,040
De derde keer toch?

4
00:00:10,040 --> 00:00:11,880
De derde keer, driemaal scheepsrecht.

5
00:00:11,880 --> 00:00:14,600
Hopelijk ook niet de laatste keer.

6
00:00:14,600 --> 00:00:17,400
Mijn naam is Joop Snijder, CTO bij Aigency.

7
00:00:17,400 --> 00:00:20,440
Mijn naam Niels Naglé, Area Lead, Data & AI bij Info Support.

8
00:00:20,440 --> 00:00:22,840
Ja Niels, Niels?

9
00:00:22,840 --> 00:00:26,480
Nee, sorry, Marc.

10
00:00:26,480 --> 00:00:27,480
Edit.

11
00:00:27,480 --> 00:00:32,280
Marc, zou je je eerst willen voorstellen aan de luisteraars, diegenen die je nog niet

12
00:00:32,280 --> 00:00:33,280
kennen?

13
00:00:33,280 --> 00:00:35,800
Uiteraard, dank dat ik weer te gast mag zijn allereerst.

14
00:00:35,800 --> 00:00:36,800
Altijd leuk.

15
00:00:36,800 --> 00:00:40,000
Mijn naam is Marc van Meel, ik ben Manager Responsible AI bij KPMG.

16
00:00:40,000 --> 00:00:44,720
En ja, wij helpen onze klanten met, het woord zegt het eigenlijk al, Responsible AI.

17
00:00:44,720 --> 00:00:47,320
Dus wij helpen klanten met AI, Artificial Intelligence.

18
00:00:47,320 --> 00:00:50,320
Hoe doe je dat nou op een goede manier, vanuit een technisch perspectief?

19
00:00:50,320 --> 00:00:55,720
Hoe doe je het nou op een manier dat je compliant bent, in overeenstemming met intern en extern

20
00:00:55,720 --> 00:01:00,160
en wet en regelgeving, met een derde gegeven dat je iets kan, gegeven dat je iets mag.

21
00:01:00,160 --> 00:01:03,280
Je zou nu ook moeten willen doen, dat is de ethische dimensie.

22
00:01:03,280 --> 00:01:07,200
En over die drie assen helpen wij onze klanten om verantwoord AI te doen.

23
00:01:07,200 --> 00:01:09,600
Het meeste waarde uit AI oplossingen te halen eigenlijk.

24
00:01:09,600 --> 00:01:12,600
En dat op een goede verantwoorde manier.

25
00:01:12,600 --> 00:01:16,080
Ja, dat je dus niet negatief in het nieuws komt, zou ik kunnen zeggen.

26
00:01:16,080 --> 00:01:20,200
Nou, we hebben een nieuw onderdeel, de vraag van de leek.

27
00:01:20,200 --> 00:01:23,640
En die gaat hier denk ik ook wel een beetje over.

28
00:01:23,640 --> 00:01:25,640
Zo dan!

29
00:01:25,640 --> 00:01:29,640
Dat was hem niet.

30
00:01:29,640 --> 00:01:33,040
Het is even wennen.

31
00:01:33,040 --> 00:01:36,040
Ja, daar is hij.

32
00:01:36,040 --> 00:01:41,880
Als kunstenaar zie ik hele mooie dingen voorbijkomen die gemaakt zijn door AI.

33
00:01:41,880 --> 00:01:45,160
En die ik zelf niet op die manier zou kunnen maken.

34
00:01:45,160 --> 00:01:50,040
Wat betekent het eigenlijk voor de kunst, AI?

35
00:01:50,040 --> 00:01:54,000
Is het de vraag van de week of de leek?

36
00:01:54,000 --> 00:01:55,000
Ja, allebei.

37
00:01:55,000 --> 00:01:58,720
De vraag van de leek is deze week.

38
00:01:58,720 --> 00:02:03,880
Wat is de impact van AI op kunst?

39
00:02:03,880 --> 00:02:04,880
Goeie vraag.

40
00:02:04,880 --> 00:02:07,360
Meteen een beetje een existentiële vraag.

41
00:02:07,360 --> 00:02:11,280
Ik kan het over meerdere assen beantwoorden.

42
00:02:11,280 --> 00:02:13,440
Ik kan iets vertellen over de jobmarket misschien.

43
00:02:13,440 --> 00:02:17,760
En ik kan iets vertellen over wat creativiteit dan zou betekenen.

44
00:02:17,760 --> 00:02:18,760
Dat zou mooi zijn.

45
00:02:18,760 --> 00:02:20,120
Misschien over die assen.

46
00:02:20,120 --> 00:02:24,200
Ik denk wel, AI heeft natuurlijk transformatieve effecten op onze samenleving.

47
00:02:24,200 --> 00:02:26,320
En dat betekent ook de banen die mensen hebben.

48
00:02:26,320 --> 00:02:28,320
Dat heeft het internet ook gehad natuurlijk.

49
00:02:28,320 --> 00:02:29,480
Dat hebben alle technologieën gehad.

50
00:02:29,480 --> 00:02:32,360
Vroeger had je iemand die de lift bediende bijvoorbeeld.

51
00:02:32,360 --> 00:02:33,360
Die bestaat niet meer.

52
00:02:33,360 --> 00:02:35,280
Dus ja, zullen de banen verdwijnen?

53
00:02:35,280 --> 00:02:36,280
Ongetwijfeld.

54
00:02:36,280 --> 00:02:41,120
Je ziet in Amerika eigenlijk al de eerste tekenen van scriptwriters die zich zorgen maken

55
00:02:41,120 --> 00:02:44,080
om de impact van AI op scriptwriting.

56
00:02:44,080 --> 00:02:49,200
Acteurs die bang zijn dat hun gelijkenis verkocht wordt, permanent.

57
00:02:49,200 --> 00:02:51,680
En dat ze moeilijker aan de bak kunnen komen.

58
00:02:51,680 --> 00:02:56,960
Ik denk dat er allerlei beroepen zijn die daadwerkelijk door een combinatie van AI,

59
00:02:56,960 --> 00:02:59,080
maar ook augmented reality vervangen kunnen worden.

60
00:02:59,080 --> 00:03:01,000
Bijvoorbeeld handmodellen.

61
00:03:01,000 --> 00:03:03,960
Heel veel dingen kan je al renderen in reclames.

62
00:03:03,960 --> 00:03:05,600
Handmodellen, oh echt.

63
00:03:05,600 --> 00:03:09,040
Dat je foto's of reclame met alleen een hand.

64
00:03:09,040 --> 00:03:11,920
Ja, je hebt dus mensen die zijn dan handmodel.

65
00:03:11,920 --> 00:03:14,280
Hun hand is dan in een reclame.

66
00:03:14,280 --> 00:03:16,880
Die hebben niet een heel lange toekomst meer.

67
00:03:16,880 --> 00:03:18,960
Dat kan je nou gewoon genereren.

68
00:03:18,960 --> 00:03:20,520
Met zes vingers.

69
00:03:20,520 --> 00:03:22,280
Nou, dat is dus voorbij.

70
00:03:22,280 --> 00:03:26,360
Tegenwoordig hebben ze weer vijf vingers.

71
00:03:26,360 --> 00:03:28,240
Generaal is zover gekomen.

72
00:03:28,240 --> 00:03:30,440
Er zijn weer andere dingen waar je nu op moet letten.

73
00:03:30,440 --> 00:03:35,800
Wat het betekent, dat betekent even saai gezegd voor de jobmarket.

74
00:03:35,800 --> 00:03:39,160
Er was recent ook een reëel, volgens mij een comicboek.

75
00:03:39,160 --> 00:03:40,560
Volgens mij was het van Batman.

76
00:03:40,560 --> 00:03:46,920
Er was een reel over omdat een lezer had, dacht hij, Gen AI art gespot.

77
00:03:46,920 --> 00:03:50,360
Dus hij had een comicboek gekocht en hij had gezegd, hé maar wacht even.

78
00:03:50,360 --> 00:03:57,920
Volgens mij gebruiken jullie als uitgever, als artiest heb jij waarschijnlijk Gen AI

79
00:03:57,920 --> 00:03:59,960
gebruikt om bepaalde afbeeldingen te genereren.

80
00:03:59,960 --> 00:04:00,960
Dat bleek ook zo te zijn.

81
00:04:00,960 --> 00:04:04,600
En dat creëerde een beetje een controverse over.

82
00:04:04,600 --> 00:04:05,600
Wat is kunst?

83
00:04:05,600 --> 00:04:06,600
Is het creativiteit?

84
00:04:06,600 --> 00:04:07,600
Is het ook auteursrecht?

85
00:04:07,600 --> 00:04:10,680
Moet het helemaal door een mens zijn gemaakt?

86
00:04:10,680 --> 00:04:13,960
Mag het misschien ook assisted zijn door Gen AI?

87
00:04:13,960 --> 00:04:19,440
Overigens, veel animatiestudio's maken al gebruik van computersoftware natuurlijk.

88
00:04:19,440 --> 00:04:23,240
Adobe is natuurlijk ook een softwareproduct waarin mensen kunst maken.

89
00:04:23,240 --> 00:04:25,080
Gemaakt voor Star Wars.

90
00:04:25,080 --> 00:04:26,080
Adobe?

91
00:04:26,080 --> 00:04:31,440
Ja, de Photoshop is gemaakt voor de Star Wars films.

92
00:04:31,440 --> 00:04:34,560
George Lucas is daar eigenlijk de initiatiefnemer van.

93
00:04:34,560 --> 00:04:35,560
Oh dat wist ik niet.

94
00:04:35,560 --> 00:04:37,760
Ik heb dat geleerd.

95
00:04:37,760 --> 00:04:38,760
Oh tof.

96
00:04:38,760 --> 00:04:39,760
Oh gaaf.

97
00:04:39,760 --> 00:04:43,240
Dat geeft al aan, Star Wars is al best wel oud natuurlijk.

98
00:04:43,240 --> 00:04:45,040
Ik ben zwaar fan trouwens hoor.

99
00:04:45,040 --> 00:04:46,040
Ik ook.

100
00:04:46,040 --> 00:04:47,040
Laten we het niet over de nieuwe films hebben.

101
00:04:47,040 --> 00:04:49,600
Of de Acolyte, de nieuwste serie.

102
00:04:49,600 --> 00:04:50,600
Heel slecht.

103
00:04:50,600 --> 00:04:55,440
Nee maar inderdaad, volgens mij is er al een lange historie van de impact van technologie

104
00:04:55,440 --> 00:04:56,440
op banen.

105
00:04:56,440 --> 00:04:59,080
En ook op, in dit geval, kunst, creativiteit.

106
00:04:59,080 --> 00:05:00,800
Ik denk dat het ook weer nieuwe deuren opent.

107
00:05:00,800 --> 00:05:02,960
Dus nieuwe mogelijkheden om creatiever te zijn.

108
00:05:02,960 --> 00:05:08,800
Om bijvoorbeeld met behulp van Gen AI afbeeldingen te maken, kunst te maken, filmpjes te maken.

109
00:05:08,800 --> 00:05:13,200
Ik denk dat het de threshold, de barrière verlaagt voor mensen om zelf iets te kunnen

110
00:05:13,200 --> 00:05:14,200
gaan doen.

111
00:05:14,200 --> 00:05:18,920
Vroeger had je allemaal duur equipement nodig voordat je kon schilderen.

112
00:05:18,920 --> 00:05:20,160
Dat was eigenlijk een beetje voor de elites natuurlijk.

113
00:05:20,160 --> 00:05:23,480
Nou met Adobe Photoshop inderdaad heeft het het eigenlijk al makkelijker gemaakt voor

114
00:05:23,480 --> 00:05:27,240
iedereen om bijvoorbeeld kunst te maken, digitale kunst, vector art.

115
00:05:27,240 --> 00:05:32,480
En met Gen AI maakt het eigenlijk alleen maar makkelijker om zelf filmpjes te maken, afbeeldingen,

116
00:05:32,480 --> 00:05:33,480
memes.

117
00:05:33,480 --> 00:05:34,480
Muziek.

118
00:05:34,480 --> 00:05:35,480
Ja, muziek.

119
00:05:35,480 --> 00:05:38,200
Jullie hadden ook een digitale assistent volgens mij hè?

120
00:05:38,200 --> 00:05:39,200
Ja.

121
00:05:39,200 --> 00:05:42,720
Als je het even hebt over creativiteit, je zou natuurlijk vragen kunnen stellen, is het

122
00:05:42,720 --> 00:05:46,440
als het volledig gemaakt is door een machine, of door een AI systeem, hoe je het ook wil

123
00:05:46,440 --> 00:05:48,800
noemen, is het dan nog creatief?

124
00:05:48,800 --> 00:05:51,920
Daar zou je natuurlijk al een filosofische discussie over…

125
00:05:51,920 --> 00:05:53,760
Wat vind jij?

126
00:05:53,760 --> 00:05:55,560
Goeie vraag.

127
00:05:55,560 --> 00:05:59,360
Ik denk dat wij mensen over het algemeen ook geen nieuwe dingen verzinnen.

128
00:05:59,360 --> 00:06:04,680
Dus als jij met iets nieuws komt, quote-on-quote nieuws, dan is dat vaak een combinatie van

129
00:06:04,680 --> 00:06:05,680
twee bestaande dingen.

130
00:06:05,680 --> 00:06:08,480
Dus jij verzint bijvoorbeeld niet zomaar een nieuwe figuur ofzo.

131
00:06:08,480 --> 00:06:11,800
Als jij een nieuwe vorm of een figuur verzint is dat waarschijnlijk een combinatie van al

132
00:06:11,800 --> 00:06:12,800
bestaande figuren.

133
00:06:12,800 --> 00:06:16,200
Dus creativiteit is eigenlijk A plus B is C.

134
00:06:16,200 --> 00:06:19,800
Of iets wat bestaat bekijken door een andere lens.

135
00:06:19,800 --> 00:06:21,520
Dus je combineert eigenlijk dingen.

136
00:06:21,520 --> 00:06:24,160
We stand on the shoulders of giants, weet je wel.

137
00:06:24,160 --> 00:06:28,240
Dus in dat opzicht zou je AI technologie die bijvoorbeeld afbeeldingen of kunst produceert

138
00:06:28,240 --> 00:06:29,840
op dezelfde wijze kunnen beschouwen.

139
00:06:29,840 --> 00:06:32,120
Dat kijkt ook naar instanties uit het verleden.

140
00:06:32,120 --> 00:06:33,960
Dus je zou dat inspiratie kunnen noemen.

141
00:06:33,960 --> 00:06:38,160
Dus je kan een heel interessante filosofische discussie houden over wat creativiteit is.

142
00:06:38,160 --> 00:06:42,040
Ik vind het niet de meest interessante discussie.

143
00:06:42,040 --> 00:06:43,040
Nee, ik ook niet.

144
00:06:43,040 --> 00:06:46,120
Maar als het gaat over intellectual property rights is het wel interessant.

145
00:06:46,120 --> 00:06:49,960
Ik weet wel, in Europa hebben we iets relaxter intellectual property rights.

146
00:06:49,960 --> 00:06:54,240
In de zin van, in Amerika moet volgens mij de eigenaar altijd een natuurlijk persoon zijn.

147
00:06:54,240 --> 00:06:55,960
Volgens mij hoeft dat in Europa niet.

148
00:06:55,960 --> 00:07:00,920
Volgens mij was er wat discussie rondom toen met die selfie van die aap.

149
00:07:00,920 --> 00:07:02,480
Misschien hebben jullie dat meegekregen.

150
00:07:02,480 --> 00:07:05,440
In Amerika zou die nooit de eigenaar van de foto zijn.

151
00:07:05,440 --> 00:07:11,920
Want er was een fotograaf en die had een camera ergens neergezet en de aap die had eigenlijk

152
00:07:11,920 --> 00:07:16,160
gewoon zelf op het knopje gedrukt waardoor die een selfie had gemaakt.

153
00:07:16,160 --> 00:07:19,360
Dus daarmee is eigenlijk ook de aap de fotograaf.

154
00:07:19,360 --> 00:07:22,920
Dus de IP-houder van de foto.

155
00:07:22,920 --> 00:07:25,160
Dat was de vraag dan.

156
00:07:25,160 --> 00:07:27,400
Naast het portretrecht zou je nog kunnen afvragen.

157
00:07:27,400 --> 00:07:32,880
Maar inderdaad, de aap had zelf op het knopje gedrukt en dan ben je de fotograaf.

158
00:07:32,880 --> 00:07:35,720
Ja, maar is de aap dan ook de eigenaar van de foto?

159
00:07:35,720 --> 00:07:37,480
Ja, dat was het hele vraagstuk.

160
00:07:37,480 --> 00:07:38,840
Dat is het hele vraagstuk.

161
00:07:38,840 --> 00:07:41,120
En in Amerika was dat volgens mij niet het geval.

162
00:07:41,120 --> 00:07:43,200
In Europa is dat dus nog maar de vraag.

163
00:07:43,200 --> 00:07:48,400
Dat speelt nu ook in de stripboekenwereld, waarin er ook een stripboek is geweest in

164
00:07:48,400 --> 00:07:52,720
het verleden, waarin de auteur het volgens mij ook met Jenna J had gegenereerd, de afbeeldingen.

165
00:07:52,720 --> 00:07:55,040
Volgens mij heette het 'Sire of the Dawn'-stripboek.

166
00:07:55,040 --> 00:07:58,880
En zijn claim op de rechter was daarop ook afgewezen.

167
00:07:58,880 --> 00:08:03,880
Omdat hij volgens mij dan onvoldoende zelf creatieve inbreng had gehad in het eindproduct.

168
00:08:03,880 --> 00:08:04,880
Ja, precies.

169
00:08:04,880 --> 00:08:11,360
En als je dit nou terugbrengt, dit vraagstuk, naar wat jij in je dagelijks werk doet, over

170
00:08:11,360 --> 00:08:12,920
Responsible AI.

171
00:08:12,920 --> 00:08:18,200
We zijn nu ruim anderhalf jaar verder met de introductie van ChatGPT.

172
00:08:18,200 --> 00:08:21,400
Heel veel mensen zien dit trouwens als AI.

173
00:08:21,400 --> 00:08:24,440
De vorm van AI.

174
00:08:24,440 --> 00:08:35,680
Hoe kijk je nu, anderhalf jaar later, naar Gen AI, ChatGPT en hoe past dat onder de

175
00:08:35,680 --> 00:08:37,800
paraplu van Responsible AI?

176
00:08:37,800 --> 00:08:38,800
Goeie vraag.

177
00:08:38,800 --> 00:08:45,360
Ik vind het ten eerste een heel snelle, maar ook wonderlijke fenomeen.

178
00:08:45,360 --> 00:08:50,680
Heel veel organisaties zitten bijna in een soort van race om iets met Gen AI te doen.

179
00:08:50,680 --> 00:08:54,120
Ik neem maar even Gen AI en taalmodellen als leidend voorbeeld.

180
00:08:54,120 --> 00:08:55,320
Daar komt het eigenlijk altijd weer op neer.

181
00:08:55,320 --> 00:08:58,640
Jullie weten, AI is natuurlijk breder dan dat.

182
00:08:58,640 --> 00:09:00,400
Dat is even hot nu.

183
00:09:00,400 --> 00:09:03,920
Je ziet heel veel organisaties, die zijn bijna aan het rennen, aan het racen om business

184
00:09:03,920 --> 00:09:06,200
cases te verzinnen om Gen AI te implementeren.

185
00:09:06,200 --> 00:09:08,600
De eerste is natuurlijk een soort van digitale assistent.

186
00:09:08,600 --> 00:09:13,880
Ik denk Microsoft, dat was er ook een heel goed voorbeeld van recent, met die recall

187
00:09:13,880 --> 00:09:14,880
functionaliteit.

188
00:09:14,880 --> 00:09:15,880
Hebben jullie misschien ook wel meegekregen?

189
00:09:15,880 --> 00:09:18,960
Ja, misschien kan je dat even uitleggen, wat ze deden daar.

190
00:09:18,960 --> 00:09:24,280
Microsoft had functionaliteiten geïmplementeerd in de trend van, eigenlijk loggen ze alles

191
00:09:24,280 --> 00:09:25,280
wat je deed.

192
00:09:25,280 --> 00:09:29,080
Ze maakten een soort van screenshots, screen apps van alles wat jij deed op jouw computer,

193
00:09:29,080 --> 00:09:30,080
op jouw pc.

194
00:09:30,080 --> 00:09:34,440
Waardoor je dan eigenlijk makkelijk heel achteraf kon achterhalen wat je had gedaan.

195
00:09:34,440 --> 00:09:38,360
Dus stel jij was iets vergeten, dan kan je heel makkelijk vragen, ook met een Gen AI,

196
00:09:38,360 --> 00:09:40,920
chat interface, wat was dat recept ook alweer?

197
00:09:40,920 --> 00:09:44,680
Of wat had Jantje mij ook alweer verteld, vorige week?

198
00:09:44,680 --> 00:09:52,160
Dat model kan dat beantwoorden, omdat hij dus beeld en tekstueel materiaal heeft verzameld

199
00:09:52,160 --> 00:09:54,480
en heeft opgeslagen van alles wat je hebt gedaan.

200
00:09:54,480 --> 00:09:56,080
Klinkt interessant, toch?

201
00:09:56,080 --> 00:09:59,680
Ik denk dat best wel de luisteraars nu zeggen, oh ja, dat zou ik best willen.

202
00:09:59,680 --> 00:10:02,400
Bijna een soort second brain-achtige functionaliteit.

203
00:10:02,400 --> 00:10:06,120
Uiteindelijk hebben ze die teruggetrokken, die functionaliteit, of moet ik het zeggen,

204
00:10:06,120 --> 00:10:07,120
ze hebben het uitgesteld.

205
00:10:07,120 --> 00:10:09,920
Wat dus betekent, die komt in een volgende release.

206
00:10:09,920 --> 00:10:17,640
Omdat het natuurlijk vragen waren, terechte vraagtekens met betrekking tot privacy concerns.

207
00:10:17,640 --> 00:10:22,480
Is dit eigenlijk een use case, ook als je erover nadenkt, is het nou een use case die

208
00:10:22,480 --> 00:10:23,480
marktgedreven is?

209
00:10:23,480 --> 00:10:25,720
Zitten consumenten hier daadwerkelijk op te wachten?

210
00:10:25,720 --> 00:10:31,160
Of is dit een use case die voorkomt uit de eerste te willen zijn, om iets met Gen AI

211
00:10:31,160 --> 00:10:32,160
te willen doen?

212
00:10:32,160 --> 00:10:34,680
Dat is altijd, denk ik, heel erg belangrijk om jezelf af te vragen.

213
00:10:34,680 --> 00:10:37,080
Komt dit wel bij consumenten vandaan?

214
00:10:37,080 --> 00:10:40,600
Of wil jij simpelweg de eerste zijn om te kunnen laten zien van, ik heb weer nieuwe

215
00:10:40,600 --> 00:10:42,240
functionaliteit met AI.

216
00:10:42,240 --> 00:10:46,520
Ik zie heel veel organisaties nu in een soort van race om iets met Gen AI te willen gaan

217
00:10:46,520 --> 00:10:47,520
doen.

218
00:10:47,520 --> 00:10:50,920
Soms om goede redenen, maar soms ook om redenen waar je vraagtekens bij kan stellen.

219
00:10:50,920 --> 00:10:58,520
Ik zeg altijd, let op, want er zitten heel veel voordelen aan, maar ook een aantal haken

220
00:10:58,520 --> 00:10:59,520
ogen.

221
00:10:59,520 --> 00:11:02,760
Het beste voorbeeld, we hadden het net al over intellectual property, bijvoorbeeld ook

222
00:11:02,760 --> 00:11:03,760
tegen open AI.

223
00:11:03,760 --> 00:11:05,600
Er lopen nog tal van rechtszaken op dit moment.

224
00:11:05,600 --> 00:11:10,360
Dus je ziet, organisaties zitten bijna in een soort van race om die functionaliteit te

225
00:11:10,360 --> 00:11:13,160
implementeren, om bijvoorbeeld modellen van open AI te procuren.

226
00:11:13,160 --> 00:11:15,400
Terwijl er nog tal van rechtszaken lopen.

227
00:11:15,400 --> 00:11:19,360
Onduidelijkheden zijn bij betrekking tot intellectual property, waar de trainingsdata vandaan komt.

228
00:11:19,360 --> 00:11:24,320
Safety concerns zijn bij betrekking tot de reliability, de betrouwbaarheid van de antwoorden

229
00:11:24,320 --> 00:11:25,320
bijvoorbeeld.

230
00:11:25,320 --> 00:11:27,560
Ja, het is wonderlijk om te zien.

231
00:11:27,560 --> 00:11:28,560
Ja toch?

232
00:11:28,560 --> 00:11:35,120
Ik heb er ook wel eens over nagedacht, is er daarvoor wel eens een situatie geweest,

233
00:11:35,120 --> 00:11:41,320
waarbij, wat jij zegt, er zijn eigenlijk nog zoveel risico's, onduidelijkheden, vragen,

234
00:11:41,320 --> 00:11:46,360
waarbij de adoptie zo groot is, en zo snel gaat.

235
00:11:46,360 --> 00:11:49,520
Ik kon in zekere zin niks zomaar verzinnen.

236
00:11:49,520 --> 00:11:52,120
Ik denk social media is misschien een goed voorbeeld.

237
00:11:52,120 --> 00:11:55,200
Ik kan me nog heel goed herinneren, in de early days, toen hadden we Hives.

238
00:11:55,200 --> 00:11:58,480
Op een gegeven moment nam Facebook het een beetje over in Nederland.

239
00:11:58,480 --> 00:12:06,040
Mensen hadden eigenlijk in een race om zoveel mogelijk persoonlijke informatie op het internet

240
00:12:06,040 --> 00:12:07,040
te gooien.

241
00:12:07,040 --> 00:12:08,600
Als je erop terugkijkt, eigenlijk heel erg wonderlijk.

242
00:12:08,600 --> 00:12:14,000
Dat mensen allerlei foto's, soms ook niet de meest charmante foto's, gewoon uploaden.

243
00:12:14,000 --> 00:12:18,800
Liefdesverklaringen, haatverklaringen, gewoon publiek aan publiek online gooiden.

244
00:12:18,800 --> 00:12:25,200
Wat natuurlijk weer voer was voor organisaties als Meta, bedrijven die data vergaarden, data

245
00:12:25,200 --> 00:12:26,200
brokers.

246
00:12:26,200 --> 00:12:30,960
Als je erop terugkijkt, eigenlijk een wonderlijke situatie waar we toen in beland waren.

247
00:12:30,960 --> 00:12:36,640
Er was helemaal geen terughoudendheid voor het grote publiek om grote velen persoonlijke

248
00:12:36,640 --> 00:12:38,160
data online te gooien.

249
00:12:38,160 --> 00:12:41,680
Ja, maar dat waren dan nog individuen.

250
00:12:41,680 --> 00:12:45,600
Trouwens, die bedrijven deden dat eigenlijk ook wel.

251
00:12:45,600 --> 00:12:48,080
Die promoten ook alles.

252
00:12:48,080 --> 00:12:54,480
Er was geloof ik laatst ook een heel kritische stuk bij, ik dacht dat het Jeugdjournaal was,

253
00:12:54,480 --> 00:12:56,480
had ik over gelezen.

254
00:12:56,480 --> 00:13:02,160
In ieder geval een programma die had een heel kritisch stuk over social media en wat er allemaal

255
00:13:02,160 --> 00:13:03,160
aan mis was.

256
00:13:03,160 --> 00:13:06,080
En bij de afsluiting zeiden ze "volg ons op".

257
00:13:06,080 --> 00:13:08,920
En vervolgens kwamen ze allemaal weer voorbij.

258
00:13:08,920 --> 00:13:12,360
Er zit ook een hele rare kronkel in.

259
00:13:12,360 --> 00:13:16,360
En die zie je denk ik nu weer terug rondom dat January.

260
00:13:16,360 --> 00:13:20,920
Dus aan de ene kant zeggen we "pas op" en aan de andere kant zeggen we "laten we er zo

261
00:13:20,920 --> 00:13:22,680
hard mogelijk mee gaan".

262
00:13:22,680 --> 00:13:27,480
Dus de technologie maakt het mogelijk, maar omdat het mogelijk is moeten we het niet per

263
00:13:27,480 --> 00:13:29,240
se met die snelheid oppakken.

264
00:13:29,240 --> 00:13:32,440
Maar zijn er zaken waar we dan rekening mee kunnen houden, waar luisteraars rekening mee

265
00:13:32,440 --> 00:13:33,440
kunnen houden?

266
00:13:33,440 --> 00:13:40,280
Door eerst over na te denken, maar misschien dat je nog andere tips hebt dan alleen nadenken,

267
00:13:40,280 --> 00:13:42,520
om te voorkomen dat we dezelfde kant op gaan met social media.

268
00:13:42,520 --> 00:13:46,240
Dat we over drie jaar zeggen "ja we zitten nog meer op het schermpje" of "we zitten op

269
00:13:46,240 --> 00:13:47,400
alle nadelige effecten".

270
00:13:47,400 --> 00:13:50,120
Hoe kijk je daar tegenaan?

271
00:13:50,120 --> 00:13:55,320
Terecht de opmerking, dat we even over een businessperspectief praten in plaats van van

272
00:13:55,320 --> 00:13:56,320
consumenten.

273
00:13:56,320 --> 00:14:01,520
Ik geloof fundamenteel natuurlijk dat AI, ik ben fundamenteel tech-optimistisch.

274
00:14:01,520 --> 00:14:05,120
Ik geloof in de kracht van technologie, ik denk dat het heel veel waarde kan brengen voor

275
00:14:05,120 --> 00:14:06,120
organisaties.

276
00:14:06,120 --> 00:14:07,920
Ik denk niet dat je moet innoveren om het te innoveren.

277
00:14:07,920 --> 00:14:12,400
Dus puur om het feit dat je kan innoveren of dat je een nieuw speeltje hebt, betekent

278
00:14:12,400 --> 00:14:15,000
niet per definitie dat je daar dan heel veel resources in moet stoppen.

279
00:14:15,000 --> 00:14:20,040
Ik denk dat je jezelf moet afvragen als organisatie, oké wat is de meerwaarde van deze technologie

280
00:14:20,040 --> 00:14:22,280
voor de doelen van mijn organisatie die ik wil bereiken.

281
00:14:22,280 --> 00:14:27,440
Je kan het alignment noemen, is die technologie wel aligned met mijn business goals?

282
00:14:27,440 --> 00:14:28,480
Om het even in het Engels te zeggen.

283
00:14:28,480 --> 00:14:32,600
Hoe je dat natuurlijk kan formuleren is door middel van een strategie.

284
00:14:32,600 --> 00:14:35,640
Dat is wat een data-strategie of een AI-strategie doet.

285
00:14:35,640 --> 00:14:39,920
Die gaat onderzoeken en beschrijven van oké hoe is deze technologie nou aligned met mijn

286
00:14:39,920 --> 00:14:44,600
doelen, hoe kan ik mijn doelen beter bereiken, hoe kan ik een data-gedreven organisatie worden

287
00:14:44,600 --> 00:14:48,480
zodat ik nog betere dienstverlening kan bieden aan mijn klanten en nog meer waarde kan realiseren.

288
00:14:48,480 --> 00:14:50,960
Dat doe je dus niet om het innoveren om het innoveren.

289
00:14:50,960 --> 00:14:54,640
Dus je zal je denk ik eerst moeten afvragen van inderdaad wat is mijn AI-strategie.

290
00:14:54,640 --> 00:14:58,480
Dus oké we hebben allemaal nieuwe technologieën, we hebben nieuw speelgoed, hoe kan ik die

291
00:14:58,480 --> 00:15:02,080
inzetten op zo'n manier dat het waardevol is voor mijn medewerkers, maar ook voor mijn

292
00:15:02,080 --> 00:15:03,640
klanten, hoe kan ik meer waarde creëren.

293
00:15:03,640 --> 00:15:08,200
Een taalmodel zou daarin kunnen helpen.

294
00:15:08,200 --> 00:15:12,360
Dat zou bijvoorbeeld jouw medewerkers kunnen ontlasten, die kunnen sneller misschien kennis

295
00:15:12,360 --> 00:15:17,400
vergaren, databronnen combineren, kan ook in de dienstverlening naar jouw klanten beter

296
00:15:17,400 --> 00:15:21,720
werken, sneller, betere antwoorden verschaffen, dus even in de zin van taalmodellen.

297
00:15:21,720 --> 00:15:28,720
Dat is allemaal goed mogelijk, maar hou dan ook wel rekening met het feit dat inderdaad

298
00:15:28,720 --> 00:15:32,200
als jij bijvoorbeeld GenAI gaat implementeren, zal je rekening moeten houden met bepaalde

299
00:15:32,200 --> 00:15:36,360
safeguards, bepaalde vangrailen, omdat die technologie ook weer gepaard gaat met nieuwe

300
00:15:36,360 --> 00:15:37,360
risico's.

301
00:15:37,360 --> 00:15:40,120
Dus bijvoorbeeld de onbetrouwbaarheid van antwoorden.

302
00:15:40,120 --> 00:15:44,480
Misschien ik kan wel één goed voorbeeld geven.

303
00:15:44,480 --> 00:15:48,920
Ik zie heel veel organisaties op dit moment zo'n GenAI use case willen implementeren,

304
00:15:48,920 --> 00:15:53,040
dus een soort digitale assistent, waarin ze interne databronnen gaan combineren om dan

305
00:15:53,040 --> 00:15:56,680
slimmer en sneller antwoorden te kunnen verschaffen aan medewerkers of aan klanten.

306
00:15:56,680 --> 00:16:01,480
Maar die organisaties worden nu geconfronteerd met het feit dat hun data lake eigenlijk een

307
00:16:01,480 --> 00:16:02,480
data swamp is.

308
00:16:02,480 --> 00:16:04,800
Dus wat ik daarmee bedoel, ze hebben hun data management niet op orde.

309
00:16:04,800 --> 00:16:09,240
Dus ze hebben niet accuraat inzicht of overzicht van hun data.

310
00:16:09,240 --> 00:16:12,760
Labels zijn bijvoorbeeld niet voorhanden met betrekking tot vertrouwelijkheidsniveau.

311
00:16:12,760 --> 00:16:16,680
Dus ze hebben niet goed scherp, oké deze data is vertrouwelijk, deze data is niet vertrouwelijk.

312
00:16:16,680 --> 00:16:18,800
Dit zijn bijvoorbeeld persoonsgegevens, dit niet.

313
00:16:18,800 --> 00:16:23,360
Nou als je dat allemaal aan zo'n taalmodel gaat voeren, laat ik het zo zeggen, alles

314
00:16:23,360 --> 00:16:24,960
wat je erin stopt kan je terugverwachten.

315
00:16:24,960 --> 00:16:30,800
Dus als jij persoonsgegevens bijvoorbeeld over medewerkers aan een taalmodel voert,

316
00:16:30,800 --> 00:16:35,320
kan je niet uitsluiten dat dat met de juiste prompt, dus die juiste query, weer achterhaald

317
00:16:35,320 --> 00:16:37,040
kan worden als output data.

318
00:16:37,040 --> 00:16:41,600
Dus heel veel organisaties die zijn nu een soort van reddingsoperaties, hersteloperaties,

319
00:16:41,600 --> 00:16:46,040
voorjaarsschoonmaak aan het uitvoeren om hun interne data management weer op orde te krijgen.

320
00:16:46,040 --> 00:16:49,920
Iets wat natuurlijk eigenlijk al fijn was geweest als dat eerder op orde was.

321
00:16:49,920 --> 00:16:50,920
Ik roep het al jaren.

322
00:16:50,920 --> 00:16:55,800
Ik blijf het nog jaren roepen en met veel plezier inderdaad, want ik zie de meerwaarde en de

323
00:16:55,800 --> 00:16:56,800
belangen van in.

324
00:16:56,800 --> 00:17:02,040
En zeker met de versnelling die we zien door het aan GenAI te gaan voeren, neemt het belang

325
00:17:02,040 --> 00:17:08,120
alleen maar toe over datakwaliteit, nou toch maar even, de governance, dus de processen

326
00:17:08,120 --> 00:17:10,840
en afstemming daarop is van essentieel belang.

327
00:17:10,840 --> 00:17:14,480
En ik denk dat het essentieel gaat worden dat we daar tijd aan gaan doen en dat het

328
00:17:14,480 --> 00:17:17,960
een strategische optie moet zijn om dit op de juiste manier in orde te gaan brengen.

329
00:17:17,960 --> 00:17:19,720
Dus ik sluit me daar volledig bij aan.

330
00:17:19,720 --> 00:17:23,920
En ik ben ook data scientist, dus ik laat het nooit helemaal los.

331
00:17:23,920 --> 00:17:27,120
Ik ga wat ik ben, "recovering" data scientist.

332
00:17:27,120 --> 00:17:28,800
Het is natuurlijk niet zo sexy.

333
00:17:28,800 --> 00:17:32,560
Kijk, dus ik ben ook data scientist en je vindt het leuk om iets met data te doen en

334
00:17:32,560 --> 00:17:35,120
wat je wilt doen, je wilt natuurlijk modellen bouwen die dingen voorspellen.

335
00:17:35,120 --> 00:17:36,120
Dat is leuk.

336
00:17:36,120 --> 00:17:37,120
Dat is het leukste.

337
00:17:37,120 --> 00:17:40,240
Maar het is dus helemaal niet zo sexy om dan weer te zeggen, ja maar je moet wel goede

338
00:17:40,240 --> 00:17:44,080
data hebben en die data management moet goed op orde zijn en die data lineage, je moet weten

339
00:17:44,080 --> 00:17:49,400
waar het vandaan komt, wie het verzameld heeft, wanneer, demografische kenmerken, ook wel

340
00:17:49,400 --> 00:17:50,400
handig om te weten.

341
00:17:50,400 --> 00:17:54,000
Stel je hebt data van Amerikanen, maar je gaat een product bouwen voor Europeanen, dan

342
00:17:54,000 --> 00:17:55,520
hoeft het niet per definitie hetzelfde te zijn.

343
00:17:55,520 --> 00:18:01,400
Ja inderdaad goed data management, labeling, vertrouwelijkheidsniveau, data security, kwaliteit.

344
00:18:01,400 --> 00:18:06,200
Nogmaals, niet de meest sexy onderwerpen, maar echt cruciale randvoorwaarden om die

345
00:18:06,200 --> 00:18:07,920
gen AI use cases te kunnen enablen.

346
00:18:07,920 --> 00:18:13,560
Ja wat mij betreft moet er een stempel komen op AI ready data en als dat niet AI ready

347
00:18:13,560 --> 00:18:16,920
is, dan moet je daar eerst nog werk aan doen voordat je dat daarvoor gaat gebruiken.

348
00:18:16,920 --> 00:18:17,920
Maar ik zie Joop.

349
00:18:17,920 --> 00:18:22,800
Ja ik ga advocaat van de duivel spelen, want het is natuurlijk geweldig om data te verzamelen,

350
00:18:22,800 --> 00:18:23,800
die heb je.

351
00:18:23,800 --> 00:18:29,520
En wat je gaat krijgen is namelijk precies wat Mark zegt, je kan die data kan je geven

352
00:18:29,520 --> 00:18:33,440
aan zo'n taalmodel en die taalmodel, dat taalmodel maakt daar chocola van.

353
00:18:33,440 --> 00:18:40,960
Dus eigenlijk is het voor mij veel handiger om veel minder tijd te besteden aan al dat

354
00:18:40,960 --> 00:18:44,200
structureren van die data en waar het vandaan komt.

355
00:18:44,200 --> 00:18:47,560
Ik bedoel die taalmodellen die weten toch ook niet waar hun data vandaan komt.

356
00:18:47,560 --> 00:18:51,800
Dus ik als ondernemer, ik heb al die data, ik gooi het erin.

357
00:18:51,800 --> 00:18:56,920
Het is nog veel makkelijker om zo'n ongestructureerde data om die er allemaal in te gooien.

358
00:18:56,920 --> 00:19:02,000
Ja oké, dus jij zegt eigenlijk, gen AI is misschien de manier om van ongestructureerde

359
00:19:02,000 --> 00:19:04,240
data gestructureerde data te maken.

360
00:19:04,240 --> 00:19:07,840
Ik denk persoonlijk als jij in een organisatie bent en je hebt alleen maar ongestructureerde

361
00:19:07,840 --> 00:19:11,640
data, dus met andere woorden je hebt verschillende data bronnen, maar je hebt niet echt zicht

362
00:19:11,640 --> 00:19:14,880
waar ze vandaan komen of de datakwaliteit of je mist labels.

363
00:19:14,880 --> 00:19:18,680
Dan denk ik persoonlijk, voordat je dus AI use cases gaat verkennen, dat je veel beter

364
00:19:18,680 --> 00:19:22,360
af bent met eerst wat simpelere vorm van AI.

365
00:19:22,360 --> 00:19:29,320
Bijvoorbeeld verkennende analyse, beschrijvende analyse, een supervised learning.

366
00:19:29,320 --> 00:19:35,240
Dan heb je dus geen labels, dus je weet eigenlijk niet wat de uitkomst bijvoorbeeld was van

367
00:19:35,240 --> 00:19:37,160
een transactie of een proces.

368
00:19:37,160 --> 00:19:39,120
Maar dat ga je dus eerst eens in kaart brengen.

369
00:19:39,120 --> 00:19:44,360
Als je een structured data hebt, ja dan grote kans dat je niet eens weet wat er aan de hand

370
00:19:44,360 --> 00:19:48,360
is of dat je verkeerde aannames maakt over processen of hoe mensen werken.

371
00:19:48,360 --> 00:19:52,920
Voordat je aan een AI use case, een gen AI use case waagt.

372
00:19:52,920 --> 00:19:54,200
Ik zeg altijd klein beginnen.

373
00:19:54,200 --> 00:19:57,840
Als je ongestructureerde data hebt of je hebt nog niet zo heel veel data, natuurlijk

374
00:19:57,840 --> 00:20:02,280
is het aantrekkelijk om meteen voorschrijvende of voorspellende analyses te gaan doen.

375
00:20:02,280 --> 00:20:04,480
Misschien moet je eerst een beschrijvende analyse doen.

376
00:20:04,480 --> 00:20:06,440
Oh ja, dat is een goede…

377
00:20:06,440 --> 00:20:07,960
Meten, weten, verbeteren.

378
00:20:07,960 --> 00:20:12,120
Maar als je iets gaat meten, moet je weten hoe je het gaat meten, wat je meet.

379
00:20:12,120 --> 00:20:16,920
En als je dus ongestructureerde data erin gaat gooien, nou dan is het gewoon, vraag

380
00:20:16,920 --> 00:20:17,920
maar raak voor een knaak.

381
00:20:17,920 --> 00:20:19,280
En ja, wat eruit komt, komt eruit.

382
00:20:19,280 --> 00:20:22,880
Maar je kan het ook niet valideren en controleren.

383
00:20:22,880 --> 00:20:26,200
En ik ben nog van het tijdperk eerst zelf doen.

384
00:20:26,200 --> 00:20:32,040
Dan weet je wat er gebeurt en dan kan je kijken of automatisering de weg naar de oplossing

385
00:20:32,040 --> 00:20:33,040
is.

386
00:20:33,040 --> 00:20:36,600
Want anders zitten we de technologie altijd tegenaan te gooien zonder dat we zelf begrijpen

387
00:20:36,600 --> 00:20:39,520
of enigszins de richting weten waar we heen willen.

388
00:20:39,520 --> 00:20:42,200
Ja, en uiteindelijk ben ik het met jullie eens.

389
00:20:42,200 --> 00:20:46,960
Maar ik denk dat er echt wel scenario's gaan komen waar gekozen wordt voor de weg van de

390
00:20:46,960 --> 00:20:47,960
minste weerstand.

391
00:20:47,960 --> 00:20:54,000
En als je begint over al deze vormen van wat er met je data moet gebeuren, dat het denk

392
00:20:54,000 --> 00:20:55,920
ik regelmatig zal gaan optreden.

393
00:20:55,920 --> 00:21:00,000
Dat mensen denken van ja, weet je, er komt best wel iets logisch uit.

394
00:21:00,000 --> 00:21:01,360
Laten we daar gewoon eens even mee beginnen.

395
00:21:01,360 --> 00:21:06,520
Ik ben het eens en ik zeg niet dat je niet mee moet experimenteren, maar wel weer met

396
00:21:06,520 --> 00:21:12,440
wat wil ik bereiken, wat is mijn doel en is dit de weg ernaartoe toereikend voor wat

397
00:21:12,440 --> 00:21:13,440
ik wil bereiken.

398
00:21:13,440 --> 00:21:18,040
En daar zal op een gegeven moment zijn inderdaad, gooi er maar in, ga maar kijken wat eruit

399
00:21:18,040 --> 00:21:19,040
komt.

400
00:21:19,040 --> 00:21:23,840
Maar als de impact of het risico groot genoeg is, dan wil je daar toch een kwaliteitsslag

401
00:21:23,840 --> 00:21:24,840
op gaan doen.

402
00:21:24,840 --> 00:21:29,000
En ik ben het ook met je eens, maar daar zie ik nog wel heel veel challenges, is dat AI

403
00:21:29,000 --> 00:21:33,600
ook gaat helpen om die datakwaliteit, die labeling en dergelijke op orde te gaan krijgen.

404
00:21:33,600 --> 00:21:35,960
Niet allemaal GenAI, misschien sommige delen met GenAI.

405
00:21:35,960 --> 00:21:39,800
Dus het gaat zeker ook weer een hulpmiddel zijn, maar dan zit het in een stukje automatisering

406
00:21:39,800 --> 00:21:42,160
van de data engineering en dat soort zaken.

407
00:21:42,160 --> 00:21:44,320
Ja, bijvoorbeeld het creëren van metadata.

408
00:21:44,320 --> 00:21:47,040
Daar is GenAI dan weer heel goed voor.

409
00:21:47,040 --> 00:21:52,480
Dus stel ik heb geen metadata of ik heb geen labels, dan zou dat met GenAI weer bij kunnen

410
00:21:52,480 --> 00:21:53,480
helpen.

411
00:21:53,480 --> 00:21:56,760
Jij maakte het inderdaad heel wetenschappelijk, dus ik heb een hypothese en die ga ik dan

412
00:21:56,760 --> 00:21:57,760
valideren, weet je wel.

413
00:21:57,760 --> 00:22:01,000
Dus volgens mij is dat the way to go, wat je zou moeten willen doen.

414
00:22:01,000 --> 00:22:04,600
Maar ook vanuit data science perspectief wil je vaak kijken, oké, ik heb een probleem,

415
00:22:04,600 --> 00:22:05,600
hoe ga ik dat oplossen?

416
00:22:05,600 --> 00:22:07,040
Zou een mens het kunnen?

417
00:22:07,040 --> 00:22:10,840
Wat is dan de baseline performance, hoe goed doet een mens het?

418
00:22:10,840 --> 00:22:12,600
Wat zijn de kosten die eraan verbonden zijn?

419
00:22:12,600 --> 00:22:15,280
Oh, kan ik het misschien automatiseren met traditionele IT?

420
00:22:15,280 --> 00:22:16,680
Kan ik het programmeren?

421
00:22:16,680 --> 00:22:20,760
Als dat misschien niet helemaal lekker werkt of te duur is, dan zou je misschien eens naar

422
00:22:20,760 --> 00:22:21,880
machine learning kunnen kijken.

423
00:22:21,880 --> 00:22:25,120
Dus kan ik met statistiek, statistische methodes, oplossen dit probleem?

424
00:22:25,120 --> 00:22:29,720
En als dat niet werkt, dan kan ik altijd nog neurale netwerken er tegen aangooien, die

425
00:22:29,720 --> 00:22:33,160
vaak ook geschikt zijn voor problemen die voor ons te moeilijk zijn.

426
00:22:33,160 --> 00:22:36,920
Dat is ook het idee van neurale netwerken.

427
00:22:36,920 --> 00:22:39,360
Problemen zijn of te complex voor ons of unfeasible.

428
00:22:39,360 --> 00:22:41,480
Het kost te veel tijd voor ons zelf om op te lossen.

429
00:22:41,480 --> 00:22:46,080
Dus dan ga ik die neurale netwerken, wat Gen-AI is natuurlijk, er tegen aangooien.

430
00:22:46,080 --> 00:22:49,760
Maar vanuit die piramide, die gelaagdheid, zou ik altijd een probleem benaderen.

431
00:22:49,760 --> 00:22:51,560
En ik zou niet bij het kanon beginnen.

432
00:22:51,560 --> 00:22:53,720
Nee, dat lijkt me een hele goede…

433
00:22:53,720 --> 00:23:00,680
Wat we natuurlijk ook zien is dat Gen-AI wordt natuurlijk gebruikt voor het automatiseren

434
00:23:00,680 --> 00:23:02,320
van repetitieve taken.

435
00:23:02,320 --> 00:23:11,160
Vaak zijn dat ook niet de meest waardevolle taken.

436
00:23:11,160 --> 00:23:14,040
Die wil je dan geautomatiseerd hebben.

437
00:23:14,040 --> 00:23:19,520
Hoe zie jij dat in het licht van een responsible AI?

438
00:23:19,520 --> 00:23:22,480
Want dan neem je taken bij mensen weg.

439
00:23:22,480 --> 00:23:25,280
Wat houden we dan over?

440
00:23:25,280 --> 00:23:28,040
Je had het over die jobmarket.

441
00:23:28,040 --> 00:23:30,920
Blijven die mensen wel hun werk doen?

442
00:23:30,920 --> 00:23:31,920
Verandert er wat?

443
00:23:31,920 --> 00:23:34,920
Gaan we andere banen daar tegenover krijgen?

444
00:23:34,920 --> 00:23:36,680
Wat is jouw visie daarop?

445
00:23:36,680 --> 00:23:39,200
Op de handmodellen na natuurlijk.

446
00:23:39,200 --> 00:23:44,800
Ik zou me niet zo heel snel druk maken over dat jouw baan vervangen wordt.

447
00:23:44,800 --> 00:23:49,760
Dus ik geloof dat het eerst een soort hybride, dus AI-assisted situatie zal er ontstaan,

448
00:23:49,760 --> 00:23:52,600
waarin jij gebruik maakt van die modellen als hulpmiddelen.

449
00:23:52,600 --> 00:23:58,800
Waardoor jij simpelweg sneller, efficiënter, meer datagedreven output kan produceren.

450
00:23:58,800 --> 00:24:05,200
Of dat nou een terugkoppeling is aan een medewerker, of een slide, of een e-mail.

451
00:24:05,200 --> 00:24:10,640
Ik geloof dat als jij handig bent met die modellen, kan je veel effectiever, veel productiever

452
00:24:10,640 --> 00:24:11,640
worden.

453
00:24:11,640 --> 00:24:12,640
Dat geldt ook voor coderen overigens.

454
00:24:12,640 --> 00:24:18,320
Ik geloof niet dat je helemaal vervanger zal worden als jij een beroep doet wat ook intellectueel

455
00:24:18,320 --> 00:24:19,640
denkvermogen beoefent.

456
00:24:19,640 --> 00:24:24,640
Programmeren vergt ook, je moet wel weten wat er aan de hand is.

457
00:24:24,640 --> 00:24:28,720
Ik kan code genereren, dat heb ik ook wel eens gedaan in het verleden door die taalmodellen.

458
00:24:28,720 --> 00:24:30,720
Klopt vaak wel, maar niet 100%.

459
00:24:30,720 --> 00:24:35,200
Dus je moet zelf alsnog denken van, oké, wat betekent dit als output?

460
00:24:35,200 --> 00:24:36,680
Ook met e-mails.

461
00:24:36,680 --> 00:24:41,520
Ik denk niet dat jullie blind e-mails zullen gaan versturen op basis van taalmodellen.

462
00:24:41,520 --> 00:24:42,520
Verlopig niet.

463
00:24:42,520 --> 00:24:47,640
Maar nou had ik toevallig wel, we hebben kaartspel, zullen we zo direct op uitkomen.

464
00:24:47,640 --> 00:24:51,080
En dan ga ik ook visuals maken voor deze podcast.

465
00:24:51,080 --> 00:24:57,800
Dan kreeg ik bijvoorbeeld een pdf en daar staan dan 56 afbeeldingen in.

466
00:24:57,800 --> 00:25:01,360
En die had ik gewoon individueel nodig.

467
00:25:01,360 --> 00:25:09,120
Heb ik aan Chad JPT gevraagd van schrijf even een partnerscriptje om dat eruit te halen.

468
00:25:09,120 --> 00:25:12,520
Dat was echt draaien, runnen, lopen.

469
00:25:12,520 --> 00:25:18,400
Was wel dat ik lage resolutie fotootjes eruit kreeg.

470
00:25:18,400 --> 00:25:21,280
Dus ik moest nog even vragen, oh je zet ze even in een hogere resolutie.

471
00:25:21,280 --> 00:25:22,960
Kreeg ik er een argumentje bij.

472
00:25:22,960 --> 00:25:23,960
En dan was lopen klaar.

473
00:25:23,960 --> 00:25:27,280
En dat zijn een soort van scriptjes, die had ik eenmalig nodig.

474
00:25:27,280 --> 00:25:30,720
Gooi ik weg, gaat niet in productie, heb ik geen testen bij nodig.

475
00:25:30,720 --> 00:25:32,760
Maar daar is het echt waanzinnig voor.

476
00:25:32,760 --> 00:25:35,480
Ik had nooit geweten waar ik had moeten beginnen.

477
00:25:35,480 --> 00:25:39,280
Klopt, dat heb ik ook wel eens voor gebruikt.

478
00:25:39,280 --> 00:25:42,040
Of code te confronteren van de ene taal naar de andere.

479
00:25:42,040 --> 00:25:44,960
Ik weet hoe ik het in Python moet doen, maar ik weet niet hoe ik het in een andere taal

480
00:25:44,960 --> 00:25:45,960
moet doen.

481
00:25:45,960 --> 00:25:49,520
Dat is nog makkelijker, want dan heb je al input natuurlijk.

482
00:25:49,520 --> 00:25:54,160
Ik denk inderdaad, het verlaagt de threshold om bepaalde dingen zelf te kunnen doen.

483
00:25:54,160 --> 00:25:57,760
En dat zie je ook in het bedrijfsleven.

484
00:25:57,760 --> 00:26:04,280
Dus moet jij je zorgen gaan maken, bijvoorbeeld over concurrenten die opeens ook dingen kunnen

485
00:26:04,280 --> 00:26:05,280
die jij kan.

486
00:26:05,280 --> 00:26:08,320
Enerzijds wel, in de zin van het kan de threshold verlagen.

487
00:26:08,320 --> 00:26:11,600
Het kan makkelijker worden voor een klein aantal mensen om hetzelfde te doen wat jij

488
00:26:11,600 --> 00:26:12,600
kan.

489
00:26:12,600 --> 00:26:16,400
Maar jij wordt ook nog beter in wat je doet met behulp van AI.

490
00:26:16,400 --> 00:26:21,200
Dus puur het feit dat je nu bijvoorbeeld zelf marketinganalyses kan doen met AI, betekent

491
00:26:21,200 --> 00:26:23,320
niet dat je ook je eigen marketing moet gaan doen.

492
00:26:23,320 --> 00:26:26,880
Want al die marketingbureaus die AI gebruiken, die worden nog beter in markten.

493
00:26:26,880 --> 00:26:27,880
Precies, alles schuift op.

494
00:26:27,880 --> 00:26:28,880
Ja, precies.

495
00:26:28,880 --> 00:26:31,240
Dus blijf bij je lees, zou ik ook zeggen.

496
00:26:31,240 --> 00:26:34,480
Dus nogmaals, focus je op de core doelen van je organisatie.

497
00:26:34,480 --> 00:26:35,920
Dus wat ben jij voor organisatie?

498
00:26:35,920 --> 00:26:36,920
Wat wil jij bereiken?

499
00:26:36,920 --> 00:26:40,040
Wat is de waarde, de value die jij levert aan jouw klanten?

500
00:26:40,040 --> 00:26:42,960
Of wat is een ander doel dat je wil bereiken?

501
00:26:42,960 --> 00:26:44,200
Als je geen klanten hebt, kan ook.

502
00:26:44,200 --> 00:26:45,520
Of geen winstoogmerken hebt.

503
00:26:45,520 --> 00:26:48,560
En hoe ga je dat nou met AI beter maken?

504
00:26:48,560 --> 00:26:50,880
Hoe ga je dat data gedreven maken?

505
00:26:50,880 --> 00:26:54,720
Hoe ga je ervoor zorgen dat je nog beter, sneller, efficiënter die waarde kan bieden?

506
00:26:54,720 --> 00:26:58,400
En daar hoort natuurlijk altijd ook bij een analyse van wat kan ik zelf en wat moet ik

507
00:26:58,400 --> 00:27:00,000
misschien uit handen geven.

508
00:27:00,000 --> 00:27:01,480
Ja, dat lijkt me een mooie.

509
00:27:01,480 --> 00:27:08,160
Wat we in de praktijk ook ongeveer vaak tegenkomen, is dat het eerste business case wordt gemaakt.

510
00:27:08,160 --> 00:27:15,200
We gaan een aantal FTE verminderen en vervolgens denken we dat we de benefit hebben binnen hebben.

511
00:27:15,200 --> 00:27:18,440
Maar AI is niet een project, data is niet een project.

512
00:27:18,440 --> 00:27:21,600
Daarna is er natuurlijk nog monitoring, zaken in de gaten houden.

513
00:27:21,600 --> 00:27:27,600
Hoe kunnen bedrijven geholpen worden bij die eerste stap van de business case, maar even

514
00:27:27,600 --> 00:27:29,320
doordenken wat er nog achter nodig is.

515
00:27:29,320 --> 00:27:33,120
Want ik denk dat dat heel vaak niet helemaal helder is voor de mensen die hier mee bezig zijn.

516
00:27:33,120 --> 00:27:34,120
Nee, klopt.

517
00:27:34,120 --> 00:27:35,120
Goed punt.

518
00:27:35,120 --> 00:27:38,160
Ik denk dat er vaak vanuit een soort optimisme ontstaan die use cases hebben.

519
00:27:38,160 --> 00:27:41,600
AI, we kunnen er iets mee doen, we kunnen dingen verbeteren, we kunnen FTE's besparen.

520
00:27:41,600 --> 00:27:47,560
Inderdaad, wat men vaak vergeet is dat het gewoon een software project is vaak.

521
00:27:47,560 --> 00:27:53,800
Dus een AI systeem is onderdeel van een software oplossing en er komen ook allerlei kosten

522
00:27:53,800 --> 00:27:54,800
bij.

523
00:27:54,800 --> 00:27:59,520
Ook kosten die gepaard gaan bijvoorbeeld met monitoring of beheersing van die technologie.

524
00:27:59,520 --> 00:28:01,400
Wij noemen dat de costs of control.

525
00:28:01,400 --> 00:28:07,280
Dus wat zijn nou bijvoorbeeld de kosten die gepaard gaan met het adequaat monitoren van

526
00:28:07,280 --> 00:28:11,400
de output, misschien het hertrainen van het model, evaluatie.

527
00:28:11,400 --> 00:28:16,760
Dat heeft dan weer qua kosten een negatieve impact op de business case.

528
00:28:16,760 --> 00:28:18,080
Maar die zijn wel heel belangrijk.

529
00:28:18,080 --> 00:28:21,680
Als jij bijvoorbeeld negatief in het nieuws komt, reputatieschade ondervindt omdat jouw

530
00:28:21,680 --> 00:28:25,920
model uit de pas is gaan lopen, of discrimineert, ik noem maar wat, omdat je daar geen rekening

531
00:28:25,920 --> 00:28:28,960
mee gehouden hebt, je hebt niet de juiste safeguards, waarborgen, ingebouwd.

532
00:28:28,960 --> 00:28:34,880
Ja, dan kan bijvoorbeeld de reputatieschade zo groot zijn, dan vallen de voordelen weer

533
00:28:34,880 --> 00:28:36,360
bij in het niet.

534
00:28:36,360 --> 00:28:40,120
Daar hebben we natuurlijk in Nederland heel veel voorbeelden van gehad, waarin producten

535
00:28:40,120 --> 00:28:45,480
zijn stopgezet, software oplossingen zijn uitgezet eigenlijk, omdat ze negatief het

536
00:28:45,480 --> 00:28:46,480
nieuws hebben gehaald.

537
00:28:46,480 --> 00:28:51,320
Ik denk overigens wel, er zijn natuurlijk wel een aantal safeguards die je kan inbouwen.

538
00:28:51,320 --> 00:28:55,720
Je zou bijvoorbeeld, we hebben het al over de databronnen gehad, dus de data die je in

539
00:28:55,720 --> 00:28:57,600
die modellen stopt, die zou je kunnen cureren.

540
00:28:57,600 --> 00:29:02,080
Dus je zou bijvoorbeeld kunnen zorgen dat het alleen betrouwbare bronnen zijn, je zou

541
00:29:02,080 --> 00:29:06,840
taalmodellen kunnen instrueren, je mag alleen maar antwoord geven op basis van die bronnen,

542
00:29:06,840 --> 00:29:10,520
of wat ik er zelf in stop, niet op basis van wat jij geleerd hebt van het internet.

543
00:29:10,520 --> 00:29:15,040
Dan zou je misschien het open AI probleem een beetje kunnen omzeilen.

544
00:29:15,040 --> 00:29:18,160
Je zou ook de prompt zelf nog kunnen cureren, dat kan ook.

545
00:29:18,160 --> 00:29:22,040
De outputkwaliteit is vaak afhankelijk van de kwaliteit van jouw inputprompt.

546
00:29:22,040 --> 00:29:26,040
Dus je ziet vaak mensen met een beetje vergelijkbare vragen, maar de een krijgt een heel goed antwoord

547
00:29:26,040 --> 00:29:27,040
en de ander minder.

548
00:29:27,040 --> 00:29:32,320
Dus wat je ook zou kunnen doen is vergelijkbare prompts opslaan, verzamelen, verbeteren.

549
00:29:32,320 --> 00:29:36,220
Dus als jij een bepaalde vraag stelt, dat daar eigenlijk een soort van slag overheen

550
00:29:36,220 --> 00:29:39,080
wordt gemaakt van "oh waarschijnlijk bedoel je dit, dus we gaan jouw prompt verrijken",

551
00:29:39,080 --> 00:29:41,040
waardoor jij een beter antwoord krijgt.

552
00:29:41,040 --> 00:29:45,000
En nogmaals dat een kans kleiner wordt op dat jij foutieve informatie bijvoorbeeld toegespeeld

553
00:29:45,000 --> 00:29:46,000
krijgt.

554
00:29:46,000 --> 00:29:49,840
En dat is denk ik wel belangrijk, want het fundamentele risico is natuurlijk, als het

555
00:29:49,840 --> 00:29:53,400
fout gaat met die modellen, gaat het vaak op grote schaal fout.

556
00:29:53,400 --> 00:29:55,560
Want heel de business case is natuurlijk automatisering.

557
00:29:55,560 --> 00:30:00,320
Je wil iedereen toegang geven tot zo'n model, maar in het verleden stel een medewerker krijgt

558
00:30:00,320 --> 00:30:04,780
een verkeerd antwoord, of is verkeerd geïnformeerd, kan gebeuren, is vaak niet eens aansprakelijk

559
00:30:04,780 --> 00:30:05,780
voor zoiets.

560
00:30:05,780 --> 00:30:09,920
Maar stel dat iedereen over jouw hele organisatie massaal bijvoorbeeld hetzelfde verkeerde antwoord

561
00:30:09,920 --> 00:30:14,720
voorgeschoteld krijgt, zei het code, zei terugkoppeling aan iemand die aan de telefoon hangt, ja

562
00:30:14,720 --> 00:30:15,720
dan heb je echt een probleem.

563
00:30:15,720 --> 00:30:16,720
Absoluut.

564
00:30:16,720 --> 00:30:21,640
Er zijn best wel heel veel vraagstukken rondom generatieve AI.

565
00:30:21,640 --> 00:30:26,240
Dus we hebben een kaartspel ontwikkeld, de AI Game Changer.

566
00:30:26,240 --> 00:30:30,480
En het leuke is, die zit vol met stellingen over verschillende categorieën.

567
00:30:30,480 --> 00:30:35,360
En daar kan je dan onderling met elkaar, kan je daarover discussiëren.

568
00:30:35,360 --> 00:30:41,160
En de winst is eigenlijk het praten over de stellingen, dat je van elkaar weet hoe je

569
00:30:41,160 --> 00:30:42,160
erover denkt.

570
00:30:42,160 --> 00:30:44,480
En hopelijk komen er dan acties uit.

571
00:30:44,480 --> 00:30:46,880
Dus wij willen jou eigenlijk ook zo'n stelling gaan voorleggen.

572
00:30:46,880 --> 00:30:48,320
Kom maar op.

573
00:30:48,320 --> 00:31:09,120
Ik heb de kaart geschud, zag je.

574
00:31:09,120 --> 00:31:11,640
Dus het is in die zin willekeurig.

575
00:31:11,640 --> 00:31:19,560
En de categorie voor jou Mark is technologie en innovatie met de stelling, bedrijven die

576
00:31:19,560 --> 00:31:26,320
generatieve AI integreren in een bedrijfsuitvoering, zullen een aanzienlijk concurrentievoordeel

577
00:31:26,320 --> 00:31:27,320
hebben.

578
00:31:27,320 --> 00:31:29,080
Dat is een goede categorie.

579
00:31:29,080 --> 00:31:30,080
Ja toch?

580
00:31:30,080 --> 00:31:31,080
Eens of oneens?

581
00:31:31,080 --> 00:31:33,960
We hebben eigenlijk al een klein beetje besproken.

582
00:31:33,960 --> 00:31:37,680
Dus laat ik dan maar eens zeggen.

583
00:31:37,680 --> 00:31:39,720
Met de juiste waarborgen natuurlijk.

584
00:31:39,720 --> 00:31:42,560
Fundamenteel ben ik optimistisch over technologie.

585
00:31:42,560 --> 00:31:48,480
Dus ik geloof als er een goede use case is, om genAI te implementeren, en ik kan er best

586
00:31:48,480 --> 00:31:52,000
wel wat voor zinnen, dan denk ik dat je daar inderdaad een competitief voordeel uit gaat

587
00:31:52,000 --> 00:31:53,360
halen als organisatie.

588
00:31:53,360 --> 00:31:56,520
Mis je natuurlijk de juiste waarborgen in acht neemt.

589
00:31:56,520 --> 00:31:58,280
En dat op een responsible manier doet.

590
00:31:58,280 --> 00:31:59,280
Ja.

591
00:31:59,280 --> 00:32:00,280
Mooi.

592
00:32:00,280 --> 00:32:04,000
Nou ja, we hebben denk ik ook een, voor de luisteraars is het wel leuk als je hier meer

593
00:32:04,000 --> 00:32:05,000
over wil weten.

594
00:32:05,000 --> 00:32:07,280
We hebben Erik van Hal gehad.

595
00:32:07,280 --> 00:32:09,520
En die heeft een marketingbureau.

596
00:32:09,520 --> 00:32:14,080
En heel vroeg is hij begonnen, dat hij zag van ik zie dit als kans.

597
00:32:14,080 --> 00:32:18,880
Dus die heeft eigenlijk zijn hele business omgegooid, zijn business model omgegooid,

598
00:32:18,880 --> 00:32:20,760
om inderdaad daar concurrentievoordeel uit te halen.

599
00:32:20,760 --> 00:32:28,240
Maar als je alleen maar denkt van we zetten dit in, want dan hebben we een stempeltje

600
00:32:28,240 --> 00:32:31,760
op de doos, dan zou ik er niet aan beginnen toch?

601
00:32:31,760 --> 00:32:35,520
Nee, kijk, ik denk ook dat het goed is om te realiseren dat de meeste organisaties zijn

602
00:32:35,520 --> 00:32:36,520
al IT-organisaties.

603
00:32:36,520 --> 00:32:40,960
Soms moet ik ook nog wel eens een organisatie overtuigen van nee, jullie zijn echt een IT-organisatie.

604
00:32:40,960 --> 00:32:44,360
Want jullie medewerkers doen niet zomaar random iets, als het goed is.

605
00:32:44,360 --> 00:32:45,360
Dat mag ik hopen.

606
00:32:45,360 --> 00:32:50,160
Jullie werken al met Excel, jullie maken eigenlijk al impliciet data-gedreven besluiten.

607
00:32:50,160 --> 00:32:53,960
Je informeert jezelf al, je hebt meerdere databonnen, Excel, nou goed, prima.

608
00:32:53,960 --> 00:32:56,960
Excel is not a database, maar oké.

609
00:32:56,960 --> 00:32:59,040
Dus eigenlijk ben je al een IT-organisatie.

610
00:32:59,040 --> 00:33:02,800
Als de IT neergaat, ligt vaak heel je organisatie al op zijn gat.

611
00:33:02,800 --> 00:33:05,680
Of je bent misschien de bakker op de hoek, oké.

612
00:33:05,680 --> 00:33:09,960
En als je een IT-organisatie bent, je zal vanzelf een AI-organisatie worden.

613
00:33:09,960 --> 00:33:11,560
Eigenlijk om twee redenen.

614
00:33:11,560 --> 00:33:14,400
Eén, jouw concurrenten gaan het ook worden.

615
00:33:14,400 --> 00:33:17,960
Dus puur om het feit dat je niet achter kan lopen op je concurrenten, zal je er iets mee

616
00:33:17,960 --> 00:33:18,960
moeten doen.

617
00:33:18,960 --> 00:33:24,240
En twee, bijvoorbeeld taalmodellen, gen-AI-oplossingen, die vinden steeds meer z'n weg, ook in traditionele

618
00:33:24,240 --> 00:33:25,240
software-applicaties.

619
00:33:25,240 --> 00:33:28,720
Dus stel, je bent afnemer van Microsoft-producten.

620
00:33:28,720 --> 00:33:34,120
Het feit dat Microsoft, je gaat integreren in haar producten, je zou dat AI-infused software

621
00:33:34,120 --> 00:33:37,520
kunnen noemen, dan ga je ook een AI-organisatie maken.

622
00:33:37,520 --> 00:33:40,320
Dus op den duur zal je er toch aan moeten geloven.

623
00:33:40,320 --> 00:33:42,560
Lijkt me een mooie antwoord op de stelling.

624
00:33:42,560 --> 00:34:01,920
Mark, je bent voor het derde jaar op rij in onze podcast.

625
00:34:01,920 --> 00:34:08,400
En wat ik eigenlijk benieuwd naar was, is heb jij een verandering gezien over de beleving

626
00:34:08,400 --> 00:34:11,560
van ethiek rondom dit onderwerp?

627
00:34:11,560 --> 00:34:14,080
De beleving van ethiek?

628
00:34:14,080 --> 00:34:15,080
Ja.

629
00:34:15,080 --> 00:34:17,080
Rondom AI bedoel je?

630
00:34:17,080 --> 00:34:20,080
Ja, ja, ja, zeker.

631
00:34:20,080 --> 00:34:24,080
Tuurlijk, het is ten eerste heel leuk om drie jaar op een rij te gast te zijn.

632
00:34:24,080 --> 00:34:25,080
Dat zie ik als een grote eer.

633
00:34:25,080 --> 00:34:26,080
Echt leuk.

634
00:34:26,080 --> 00:34:29,760
Ik luister jullie podcast trouwens ook heel vaak, om op de date te blijven.

635
00:34:29,760 --> 00:34:30,760
Oh, wat leuk.

636
00:34:30,760 --> 00:34:31,760
Dankjewel.

637
00:34:31,760 --> 00:34:34,840
Ik heb ook een vraag over backstage, over informatievoorziening.

638
00:34:34,840 --> 00:34:35,840
Hoe blijf je nou up-to-date?

639
00:34:35,840 --> 00:34:39,600
Jullie podcast is daar een heel nuttig instrument voor.

640
00:34:39,600 --> 00:34:40,600
Zo veel nieuws.

641
00:34:40,600 --> 00:34:43,400
En dat zeg ik omdat er zo veel nieuws is tegenwoordig.

642
00:34:43,400 --> 00:34:45,200
Ik vind het ook heel erg moeilijk om bij te blijven.

643
00:34:45,200 --> 00:34:47,640
Juist omdat er zoveel nieuws is.

644
00:34:47,640 --> 00:34:49,640
Voor mijn gevoel was het één, twee jaar geleden nog wat makkelijker.

645
00:34:49,640 --> 00:34:52,880
Dus ik kon mijn LinkedIn-feed openen of mijn nieuwsbronnen.

646
00:34:52,880 --> 00:34:56,120
Dan had ik wel een relatief overzicht van die week wat er gebeurd was.

647
00:34:56,120 --> 00:34:58,800
Ja, nu is het eigenlijk iedere dag booming.

648
00:34:58,800 --> 00:35:02,080
De beleving van ethiek, je merkt het over alle fronten.

649
00:35:02,080 --> 00:35:06,080
Dus de responsible AI-business, waar ik dan zelf in zit, is natuurlijk heel close verweven

650
00:35:06,080 --> 00:35:07,080
met ethiek.

651
00:35:07,080 --> 00:35:08,480
Is echt booming.

652
00:35:08,480 --> 00:35:10,400
Dus we hebben het hartstikke druk.

653
00:35:10,400 --> 00:35:13,560
Er zijn heel veel organisaties die ons vragen van "oké, kunnen jullie ons helpen?"

654
00:35:13,560 --> 00:35:15,160
Of "hoe doen we dit nou op verantwoorde wijze?"

655
00:35:15,160 --> 00:35:18,480
Als ik kijk naar het aantal vacatures ook, wat ik voorbij zie komen online.

656
00:35:18,480 --> 00:35:19,480
Aantal posities.

657
00:35:19,480 --> 00:35:26,000
Mensen die ik zou te responsible AI of digital ethics dataethiek wereld kunnen noemen.

658
00:35:26,000 --> 00:35:29,080
Die ik zie die die wereld ingaan of in willen gaan.

659
00:35:29,080 --> 00:35:31,320
Altijd een scriptant dat zich aanmeldt bij mij.

660
00:35:31,320 --> 00:35:33,000
Dat is explosief gestegen.

661
00:35:33,000 --> 00:35:34,720
Dus het is steeds meer in the picture.

662
00:35:34,720 --> 00:35:39,560
Je ziet ook, het is ook wel leuk om te vermelden, ik werk dus voor KPMG en wij publiceren ieder

663
00:35:39,560 --> 00:35:43,800
jaar de KPMG, wij noemen dat de algoritme vertrouwensmonitor.

664
00:35:43,800 --> 00:35:49,560
Dan gaan wij bij de gewone man eigenlijk op straat, dan gaan we naar de gewone Nederlander,

665
00:35:49,560 --> 00:35:54,720
peilen wij wat is nou jouw bekendheid met, we noemen dat even algoritmes, maar AI.

666
00:35:54,720 --> 00:35:56,760
En ook vertrouwen je AI ook.

667
00:35:56,760 --> 00:36:00,480
En wat je ziet over het afgelopen jaar is een trend dat men steeds bekender wordt met

668
00:36:00,480 --> 00:36:04,440
AI, dus men weet steeds meer wat het is.

669
00:36:04,440 --> 00:36:05,960
Maar het vertrouwen zie je dalen.

670
00:36:05,960 --> 00:36:11,760
Dus volgens mij afgelopen jaar zagen wij een daling over de breedte van 22 procent in vertrouwen.

671
00:36:11,760 --> 00:36:12,760
Dat is fors.

672
00:36:12,760 --> 00:36:17,320
Ja, dus men weet steeds meer wat het is en ook de impact op zijn of haar leven.

673
00:36:17,320 --> 00:36:24,320
Maar het vertrouwen van organisaties die AI inzetten daalt bij de burger.

674
00:36:24,320 --> 00:36:28,520
En dat is natuurlijk ingegeven door de lange traditie van schandalen die wij hebben in

675
00:36:28,520 --> 00:36:29,520
Nederland.

676
00:36:29,520 --> 00:36:34,760
Dus ik zag laatst dat Nederland volgens mij nummer 1 scoorde op een lijst van trustworthy

677
00:36:34,760 --> 00:36:37,800
of responsible AI op Europees niveau.

678
00:36:37,800 --> 00:36:39,360
Nee, wereldwijd was dat.

679
00:36:39,360 --> 00:36:41,200
Hebben jullie ook gezien.

680
00:36:41,200 --> 00:36:46,920
En enerzijds is dat goed, en ik denk dat, ik hoop dat ik daar zelf ook een steentje aan

681
00:36:46,920 --> 00:36:47,920
bij heb gedragen.

682
00:36:47,920 --> 00:36:53,440
Anderzijds kan je natuurlijk ook afvragen, maar gegeven de hoeveelheid schandalen, relatief

683
00:36:53,440 --> 00:36:57,880
gezien ook, voor zo'n klein land, ja, dat is dan weer niet zo goed.

684
00:36:57,880 --> 00:36:59,960
Dus het gaat natuurlijk hand in hand met elkaar.

685
00:36:59,960 --> 00:37:04,040
Ik wou net zeggen, ik denk dat het leren van wat er heeft plaatsgevonden dan essentieel

686
00:37:04,040 --> 00:37:05,760
is en het reflecteren erop.

687
00:37:05,760 --> 00:37:09,520
En misschien de openheid die we ook vaak hebben, denk ik, in Nederland.

688
00:37:09,520 --> 00:37:12,080
Dat het ook wel een middel is om weer meer te kunnen groeien.

689
00:37:12,080 --> 00:37:16,320
Ja, ik denk dat men er gewoon achter komt nu dat men toch heel veel in productie heeft

690
00:37:16,320 --> 00:37:17,320
draaien.

691
00:37:17,320 --> 00:37:20,000
Wat men misschien niet eens wist dat ze het in productie hadden draaien.

692
00:37:20,000 --> 00:37:25,880
AI-systemen waren die nou destijds toch niet helemaal, ik noem het even goed of verantwoord

693
00:37:25,880 --> 00:37:28,600
ontwikkeld zijn, misschien een beetje te snel, he.

694
00:37:28,600 --> 00:37:32,120
Datasets die toch niet helemaal balanced bleken te zijn.

695
00:37:32,120 --> 00:37:35,800
Niet alle groepen bijvoorbeeld even redelijk ondervertegenwoordigd waren, die dan gebruikt

696
00:37:35,800 --> 00:37:37,560
waren als input voor die modellen.

697
00:37:37,560 --> 00:37:42,720
Ook technieken die gebruikt zijn die misschien niet toereikend waren of niet de lange termijn

698
00:37:42,720 --> 00:37:43,720
kunnen doorstaan.

699
00:37:43,720 --> 00:37:48,600
Business case waar je kan afvragen, nou moet je dit eigenlijk wel willen doen?

700
00:37:48,600 --> 00:37:51,280
En ik zie dat mensen er steeds meer wakker worden over dat thema.

701
00:37:51,280 --> 00:37:53,680
De impact begrijpen op hun leven van algoritmes.

702
00:37:53,680 --> 00:37:57,440
En je ziet dus ook mensen die gemotiveerd worden om daar iets aan te doen.

703
00:37:57,440 --> 00:37:59,920
Ik had het zelf vijf jaar geleden ook.

704
00:37:59,920 --> 00:38:03,520
Ook geconfronteerd met, in mijn opinie, onethische business cases.

705
00:38:03,520 --> 00:38:08,920
En ik dacht van, ik wil andere organisaties helpen om goed data science te doen, goed

706
00:38:08,920 --> 00:38:09,920
AI te ontwikkelen.

707
00:38:09,920 --> 00:38:15,800
Je ziet steeds meer mensen over de as van ethiek, maar ook in de techniek, iets met

708
00:38:15,800 --> 00:38:16,800
dit thema willen doen.

709
00:38:16,800 --> 00:38:19,280
Zij het uitlegbaarheid, zij discriminatie willen voorkomen.

710
00:38:19,280 --> 00:38:21,560
Fantastisch om te zien.

711
00:38:21,560 --> 00:38:24,200
Het is niet langer meer een nis, denk ik.

712
00:38:24,200 --> 00:38:32,880
En wat jij ook zegt, mensen zijn er steeds meer bewust van dat er overal en nergens algoritmes

713
00:38:32,880 --> 00:38:33,880
zijn.

714
00:38:33,880 --> 00:38:39,040
Wat ik me wel afvraag, wij zien natuurlijk, de media leeft van schandalen.

715
00:38:39,040 --> 00:38:45,720
Dus je ziet altijd wat niet goed gaat, je ziet niet wat er wel goed gaat.

716
00:38:45,720 --> 00:38:52,400
En wat er niet goed gaat, is uiteindelijk een topje van de ijsberg van alle algoritmes

717
00:38:52,400 --> 00:38:53,400
die er lopen.

718
00:38:53,400 --> 00:38:58,560
Want ik heb in een andere uitzending ook wel eens gezegd van de meeste algoritmes deugen.

719
00:38:58,560 --> 00:39:04,160
Want er zijn natuurlijk heel veel algoritmes die helemaal niet over ons als mens gaan.

720
00:39:04,160 --> 00:39:11,440
Die gewoon hele eenvoudige voorspellingen doen of een apparaat kapot gaat, ja of nee.

721
00:39:11,440 --> 00:39:18,000
Bij Alliander bijvoorbeeld kijken ze naar of zijn kabels op het punt staan om beschadigd

722
00:39:18,000 --> 00:39:19,000
te raken.

723
00:39:19,000 --> 00:39:20,000
Kunnen we daar wat mee?

724
00:39:20,000 --> 00:39:24,160
Je spamfolder, zou ik niet een ondeugdelijk algoritme noemen.

725
00:39:24,160 --> 00:39:26,600
Het beste algoritme wat er is.

726
00:39:26,600 --> 00:39:27,600
Toch?

727
00:39:27,600 --> 00:39:30,480
Volgens mij 99% van alle e-mails komen er nooit aan.

728
00:39:30,480 --> 00:39:32,160
Precies, die zie je in ieder geval niet.

729
00:39:32,160 --> 00:39:33,160
Heerlijk is dat.

730
00:39:33,160 --> 00:39:34,160
Dat is nog te veel.

731
00:39:34,160 --> 00:39:35,160
Ja, toch?

732
00:39:35,160 --> 00:39:41,720
Dus we hebben ook wel te maken met, als je dan hebt over gebalanceerde data in dit geval,

733
00:39:41,720 --> 00:39:45,120
we hebben natuurlijk ook wel te maken met ongebalanceerd nieuws.

734
00:39:45,120 --> 00:39:47,720
Ja, daar wil ik wel wat over zeggen.

735
00:39:47,720 --> 00:39:53,920
Inderdaad, het merendeel van het nieuws wat ons bereikt is negatief nieuws.

736
00:39:53,920 --> 00:39:56,360
Negatieve dingen verkopen dan eenmaal.

737
00:39:56,360 --> 00:39:58,960
Dat komt door ons lizzard brains, zou je het kunnen noemen.

738
00:39:58,960 --> 00:40:03,200
Wij zijn natuurlijk de afstammelingen van, ik noem het maar even, neurotische apen.

739
00:40:03,200 --> 00:40:07,200
Laten we het zo zeggen, degenen die het meest bang waren voor de slangen in het gras, die

740
00:40:07,200 --> 00:40:09,080
hebben het overleefd en daar komen wij vandaan.

741
00:40:09,080 --> 00:40:13,560
Dus het is niet heel raar dat als je iets negatiefs ziet, al zij het aan de andere kant van de

742
00:40:13,560 --> 00:40:16,040
wereld, dat je dan getriggerd wordt om daarop te klikken.

743
00:40:16,040 --> 00:40:17,400
Dat je denkt, oh dat is interessant.

744
00:40:17,400 --> 00:40:21,640
Terwijl als je erover nadenkt, de impact op jouw leven van een ramp aan de andere kant

745
00:40:21,640 --> 00:40:24,680
van de wereld, het is natuurlijk verschrikkelijk dat dat gebeurt, maar de impact op jouw leven

746
00:40:24,680 --> 00:40:25,680
is vaak nieuw.

747
00:40:25,680 --> 00:40:30,680
Dus eigenlijk positief nieuws uit bijvoorbeeld een land als Duitsland, onze grootste trading

748
00:40:30,680 --> 00:40:35,280
partner als Nederland, is eigenlijk veel belangrijker, heeft waarschijnlijk een groter directe impact

749
00:40:35,280 --> 00:40:39,360
op jouw leven, de Duitse economie, dan als er een ramp is gebeurd in Azië.

750
00:40:39,360 --> 00:40:43,080
En nogmaals, dat is heel verschrikkelijk, maar de kans dat jij daar de dag daarna misschien

751
00:40:43,080 --> 00:40:45,840
hinder of iets van ondervindt, is eigenlijk niet heel.

752
00:40:45,840 --> 00:40:48,280
En dat heeft natuurlijk inderdaad te maken met informatievoorziening.

753
00:40:48,280 --> 00:40:50,400
Misschien ook wel even een vraag aan jullie.

754
00:40:50,400 --> 00:40:54,320
Als ik nu zeg, Obama, waar denken jullie dan aan?

755
00:40:54,320 --> 00:40:55,320
In jullie gedachte?

756
00:40:55,320 --> 00:40:56,320
Of in jullie mind, zeg maar?

757
00:40:57,320 --> 00:41:00,320
Voor mij is het eerst de healthcare wat naar boven komt.

758
00:41:00,320 --> 00:41:01,320
Ja?

759
00:41:01,320 --> 00:41:06,320
Ja, ik als, zeg maar, dat hij zo'n onwaarschijnlijk goede redenaar is.

760
00:41:06,320 --> 00:41:08,640
Zien jullie ook een plaatje voor je van Obama?

761
00:41:08,640 --> 00:41:10,320
Of een mentale foto?

762
00:41:10,320 --> 00:41:11,320
Ja.

763
00:41:11,320 --> 00:41:12,320
Oké, dat is dus heel bijzonder.

764
00:41:12,320 --> 00:41:18,360
Als je teruggaat in de tijd, toen we nog kranten hadden bijvoorbeeld, de meeste mensen wisten

765
00:41:18,360 --> 00:41:20,600
bijvoorbeeld niet hoe de president van Amerika eruit zag.

766
00:41:20,600 --> 00:41:21,600
Oh ja, natuurlijk.

767
00:41:21,600 --> 00:41:22,600
Wisten ze niet.

768
00:41:22,600 --> 00:41:27,600
Wat we tegenwoordig doen, is wij consumeren heel veel fotografisch nieuws.

769
00:41:27,600 --> 00:41:33,520
Dus wij consumeren afbeeldingen, filmpjes, eigenlijk relatief korte tekst, waar men in

770
00:41:33,520 --> 00:41:36,080
het verleden vaak juist grote hoeveelheden tekst consumeerde.

771
00:41:36,080 --> 00:41:37,080
Bijvoorbeeld een krant.

772
00:41:37,080 --> 00:41:39,760
Dus heel veel mensen wisten niet hoe de president eruit zag.

773
00:41:39,760 --> 00:41:43,680
Maar het feit dat wij nu meteen aan een plaatje denken, of misschien wel die Obama-meme, wat

774
00:41:43,680 --> 00:41:47,600
je ook in je hoofd krijgt, dat geeft al aan dat wij in een tijd leven waarin we heel veel

775
00:41:47,600 --> 00:41:49,880
fotografisch nieuws consumeren.

776
00:41:49,880 --> 00:41:56,200
Het is ook erg logisch dat dat leidt tot bepaalde media, zoals bijvoorbeeld smartphones, tablets,

777
00:41:56,200 --> 00:41:57,560
waarop we dat nieuws consumeren.

778
00:41:57,560 --> 00:42:01,760
Want het is natuurlijk veel makkelijker om dat nieuws dan te consumeren dan via een krant.

779
00:42:01,760 --> 00:42:05,520
Omdat we smartphones gebruiken, tablets, wordt het nieuws ook korter.

780
00:42:05,520 --> 00:42:11,320
Je kan niet meer miljoenen regels of karakters op een tablet, ja kan wel, maar dat leest

781
00:42:11,320 --> 00:42:12,320
niet zo lekker.

782
00:42:12,320 --> 00:42:13,320
Dus het nieuws wordt korter.

783
00:42:13,320 --> 00:42:15,680
Daardoor word je ook minder goed geïnformeerd.

784
00:42:15,680 --> 00:42:18,400
En natuurlijk algoritmes spelen er een rol in.

785
00:42:18,400 --> 00:42:22,640
Als algoritmes pushen het negatieve nieuws naar boven, waar het meest op wordt geklikt.

786
00:42:22,640 --> 00:42:25,040
Dus wat trending gaat.

787
00:42:25,040 --> 00:42:30,760
Maar ook het medium waarop je nieuws consumeert, dus in dit geval een smartphone, een tablet,

788
00:42:30,760 --> 00:42:35,440
een laptop, maakt dat je minder snel geïnformeerd wordt.

789
00:42:35,440 --> 00:42:37,160
Minder goed geïnformeerd wordt.

790
00:42:37,160 --> 00:42:40,760
En ik denk dat de media daar ook een heel belangrijke rol in spelen en ook een verantwoordelijkheid

791
00:42:40,760 --> 00:42:41,760
hebben.

792
00:42:41,760 --> 00:42:43,680
Een beetje een talkshow nowadays.

793
00:42:43,680 --> 00:42:50,640
Misschien krijgen mensen maar 2, 3, 5 minuten de tijd om een onderwerp uit te leggen of

794
00:42:50,640 --> 00:42:52,040
over daarover te rapporteren.

795
00:42:52,040 --> 00:42:55,440
Dat is natuurlijk veel te kort om met een genuanceerde, goed geïnformeerde mening te

796
00:42:55,440 --> 00:42:56,440
komen.

797
00:42:56,440 --> 00:43:01,360
Dus als jij al je nieuws haalt van talkshows of van Twitter, wat volgens mij ook maar een

798
00:43:01,360 --> 00:43:05,320
aantal karakters is, wordt het gewoon heel erg lastig om een goed geïnformeerde mening

799
00:43:05,320 --> 00:43:06,320
te vormen.

800
00:43:06,320 --> 00:43:10,800
Dus in dat opzicht denk ik dat media qua vorm, inhoud, maar ook algoritme, bijdragen aan

801
00:43:10,800 --> 00:43:12,720
toch een bepaalde mate van polarisatie.

802
00:43:12,720 --> 00:43:14,560
In de samenleving.

803
00:43:14,560 --> 00:43:21,680
Als je alleen maar je nieuws uit kort formaat, nieuwsmedia haalt, digitale media vaak, geen

804
00:43:21,680 --> 00:43:23,680
wonder dat jij slecht geïnformeerd bent.

805
00:43:23,680 --> 00:43:26,720
En jouw politieke tegenstander is ook slecht geïnformeerd.

806
00:43:26,720 --> 00:43:29,760
Dus geen wonder dat jij een ruzie krijgt op Twitter.

807
00:43:29,760 --> 00:43:31,520
En dat vind ik persoonlijk heel erg jammer.

808
00:43:31,520 --> 00:43:33,240
Ook als je het in de Tweede Kamer ziet.

809
00:43:33,240 --> 00:43:37,920
Ik wil niet op een politieker wend gaan, maar dat de Tweede Kamer gebruikt wordt om YouTube

810
00:43:37,920 --> 00:43:38,920
filmpjes op te nemen.

811
00:43:38,920 --> 00:43:39,920
Ja, bizar.

812
00:43:39,920 --> 00:43:43,600
Van 1, 2, 3 minuten voor kliks op het YouTube kanaal van politieke partijen.

813
00:43:43,600 --> 00:43:45,080
Zowel links als rechts doet dat.

814
00:43:45,080 --> 00:43:47,800
Dat vind ik een heel erg jammerlijke trend.

815
00:43:47,800 --> 00:43:51,360
Je verwacht juist dat het publieke debat plaatsvindt in de Tweede Kamer.

816
00:43:51,360 --> 00:43:54,520
En dat kan niet met een 1 minuut YouTube fragment.

817
00:43:54,520 --> 00:43:55,720
Lijkt me ook niet, nee.

818
00:43:55,720 --> 00:43:58,120
Ik hoor hier de vraag naar diepgang, jongens.

819
00:43:58,120 --> 00:44:02,120
Diepgaande gesprekken met elkaar zorgen dat je verder kijkt dan scratching the surface.

820
00:44:02,120 --> 00:44:05,960
Dus we gaan niet alleen maar de oppervlakte bekrassen zoals we het in het Nederlands

821
00:44:05,960 --> 00:44:06,960
dan maar moeten vertalen.

822
00:44:06,960 --> 00:44:08,960
Dat is een slechte vertaal ik me trouwens.

823
00:44:08,960 --> 00:44:10,600
Dus dat klinkt toch niet zo lekker.

824
00:44:10,600 --> 00:44:11,600
Ja, scratching the surface.

825
00:44:11,600 --> 00:44:17,320
Maar ga met elkaar inderdaad de diepgang in en kijk even verder dan het ene bericht inderdaad.

826
00:44:17,320 --> 00:44:18,760
En laat je goed informeren.

827
00:44:18,760 --> 00:44:21,200
En het is ook prima om geen mening te hebben.

828
00:44:21,200 --> 00:44:23,280
Dus tegenwoordig moet je overal een mening over hebben.

829
00:44:23,280 --> 00:44:24,560
Je bent voor of tegen dit.

830
00:44:24,560 --> 00:44:26,520
Het is ook prima om te zeggen, ja, ik weet het niet.

831
00:44:26,520 --> 00:44:28,040
Ik ben niet genoeg ingelezen.

832
00:44:28,040 --> 00:44:30,600
Ik zou het ook van de andere kant eens een keer willen bekijken.

833
00:44:30,600 --> 00:44:34,360
Tegenwoordig is er heel veel pressure om een bepaalde mening te hebben.

834
00:44:34,360 --> 00:44:38,800
Misschien moet ik eerst eens een keer een paper lezen.

835
00:44:38,800 --> 00:44:42,240
Zou ook kunnen om tot een betere mening te komen in plaats van mijn nieuwsvoorziening

836
00:44:42,240 --> 00:44:43,520
alleen maar van YouTube te halen.

837
00:44:43,520 --> 00:44:44,520
Precies.

838
00:44:44,520 --> 00:44:45,520
En geen mening is ook een mening.

839
00:44:45,520 --> 00:44:51,400
Even terug naar de ethiek en de AI.

840
00:44:51,400 --> 00:44:56,320
Je hebt dan de ontwikkeling nu gezien van voor gen AI.

841
00:44:56,320 --> 00:45:01,960
We zitten nu eigenlijk in die transformatie van, ja, dat mensen echt wel na gaan denken

842
00:45:01,960 --> 00:45:07,320
over hoe zetten we dit nou responsabel in, wat zijn de ethische vraagstukken, waar gaan

843
00:45:07,320 --> 00:45:08,320
we naartoe?

844
00:45:08,320 --> 00:45:11,320
Waar gaan we naartoe?

845
00:45:11,320 --> 00:45:12,320
Ja.

846
00:45:12,320 --> 00:45:16,040
En dat hoeft geen tien jaar, want daar geloof ik helemaal niet in.

847
00:45:16,040 --> 00:45:18,320
Maar volgend jaar zit je hier weer.

848
00:45:18,320 --> 00:45:20,240
Wat is er veranderd volgens jou?

849
00:45:20,240 --> 00:45:24,920
Oeh, ja, nou, als data scientist moet ik weten dat voorspellingen over de toekomst, dan moet

850
00:45:24,920 --> 00:45:26,720
je een beetje voorzichtig zijn.

851
00:45:26,720 --> 00:45:30,680
Je mag het met een 78,3% zekerheid, mag je het voorspellen.

852
00:45:30,680 --> 00:45:32,680
Is dat de threshold?

853
00:45:32,680 --> 00:45:35,680
Misschien moeten we even een betrouwbaarheidinterval bepalen.

854
00:45:35,680 --> 00:45:38,880
Ja goed, ik vind het een heel moeilijke vraag die je stelt.

855
00:45:38,880 --> 00:45:41,280
Ik snap ook wat je vraag is hoor.

856
00:45:41,280 --> 00:45:45,840
Moet ik even terug naar de historische data kijken, de afgelopen jaar.

857
00:45:45,840 --> 00:45:50,360
Ik denk dat AI een enorme stroomversnelling is gekomen in de zin van de capabilities.

858
00:45:50,360 --> 00:45:53,680
Ik denk dat niemand de capabilities van gen AI aan zag komen.

859
00:45:53,680 --> 00:45:59,280
Je ziet nu wel weer een soort van tegenbeweging, een soort van gezond verstand beweging ontstaan.

860
00:45:59,280 --> 00:46:02,880
Dus altijd als een nieuwe technologie wordt geïntroduceerd ga je door zo'n golfbeweging

861
00:46:02,880 --> 00:46:05,880
van de capabilities worden overschat.

862
00:46:05,880 --> 00:46:07,760
Alles kan, alles is mogelijk.

863
00:46:07,760 --> 00:46:11,280
En dan zie je daarna zo'n soort van cooling down period waarin mensen weer wat rationeler

864
00:46:11,280 --> 00:46:16,040
worden en ook in zich krijgen wat wel kan, wat niet kan, waar die use cases wel en niet

865
00:46:16,040 --> 00:46:17,040
voor geschikt zijn.

866
00:46:17,040 --> 00:46:20,720
Dus ik denk nu dat we naar zo'n soort van cooling down periode gaan, waarin organisaties

867
00:46:20,720 --> 00:46:24,880
hopelijk ook verstandige afwegingen maken met betrekking tot de return on investment.

868
00:46:24,880 --> 00:46:28,680
Waar kan ik AI wel voor gebruiken, waar kan ik het niet voor gebruiken, waar kan ik beter

869
00:46:28,680 --> 00:46:35,000
traditionele methodes voor gebruiken, waar gaan AI en een mens samen goed, wat kan ik

870
00:46:35,000 --> 00:46:36,320
wel volledig automatiseren.

871
00:46:36,320 --> 00:46:41,320
Ik hoop eigenlijk een soort van gezond verstand, rustig aan modus.

872
00:46:41,320 --> 00:46:46,080
Ik hoop ook dat heel die discussie over existential risk, existentieel risico van AI technologie

873
00:46:46,080 --> 00:46:47,080
weer een beetje afkoelt.

874
00:46:47,080 --> 00:46:51,760
We hadden het eerder al over, je moet een mening hebben, je bent voor of tegen.

875
00:46:51,760 --> 00:46:54,760
Je zag bijvoorbeeld in de ethiekwereld duidelijk twee kampen.

876
00:46:54,760 --> 00:46:59,960
Je bent of bezorgd om de existentiële risico's van AI, de Terminator, of je bent er absoluut

877
00:46:59,960 --> 00:47:00,960
niet van.

878
00:47:00,960 --> 00:47:04,800
Ook weer een soort van tweedeling gecreëerd in de media, helaas.

879
00:47:04,800 --> 00:47:08,640
Ik denk dat dat inmiddels ook weer een beetje afkoelt, hopelijk.

880
00:47:08,640 --> 00:47:15,440
Wat ik vooral hoop volgend jaar is minder polarisatie, maar meer overeenstemming over de mogelijkheden,

881
00:47:15,440 --> 00:47:19,800
maar ook over de short-comings of de niet-mogelijkheden van AI.

882
00:47:19,800 --> 00:47:23,320
Dat we allemaal met een gezonder volwassenere blik naar kunnen kijken.

883
00:47:23,320 --> 00:47:28,000
Ik denk dat organisaties ook beseffen, innovatie om innovatie is niet altijd een goed idee.

884
00:47:28,000 --> 00:47:31,400
Er zijn veel schandalen geweest in Nederland, laten we dat vooral voorkomen.

885
00:47:31,400 --> 00:47:37,120
En dat we allemaal wat volwassener, hopelijk, volgend jaar naar kunnen kijken.

886
00:47:37,120 --> 00:47:40,600
Dat lijkt me echt een hele mooie, hoopvolle afsluiter.

887
00:47:40,600 --> 00:47:41,600
Dankjewel.

888
00:47:41,600 --> 00:47:43,520
Dankjewel Marc dat je hier weer wilde zijn.

889
00:47:43,520 --> 00:47:46,160
Het is altijd een genoegen om je in de studio te hebben.

890
00:47:46,160 --> 00:47:48,360
Dus nogmaals dank.

891
00:47:48,360 --> 00:47:49,360
Dankjewel.

892
00:47:49,360 --> 00:47:50,360
I'll be back.

893
00:47:50,360 --> 00:47:51,360
Kijk, heel graag.

894
00:47:51,360 --> 00:47:56,360
Om in The Terminator thema te blijven.

895
00:47:56,360 --> 00:48:00,560
Hartstikke leuk dat je weer luisterde naar een aflevering van AIToday Live.

896
00:48:00,560 --> 00:48:07,160
Vergeet je niet te abonneren via je favoriete podcast app, dan mis je geen aflevering.

897
00:48:07,160 --> 00:48:08,240
Tot de volgende keer.

898
00:48:08,240 --> 00:48:10,240
[Muziek]

