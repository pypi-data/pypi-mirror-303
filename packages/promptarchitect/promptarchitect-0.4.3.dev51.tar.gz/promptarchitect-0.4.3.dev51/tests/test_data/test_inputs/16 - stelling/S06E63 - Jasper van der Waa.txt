Hoi, leuk dat je weer luistert naar een nieuwe aflevering van AIToday Live. Met vandaag voor mij in ieder geval een hele bijzondere aflevering, want we gaan het met een gast hebben over Explainable AI. Hoe leuk is dat? Tenminste, ik vind het heel erg leuk. Mijn naam Joop Snijder, CTO bij Aigency. Mijn naam Niels Naglé, Area Lead Data & AI bij Info Support. En onze gast is Jasper van der Waa. Jasper, super bedankt dat je bij ons in de studio wilde zijn. Graag gedaan. Voordat we beginnen zou je jezelf eerst willen voorstellen aan de luisteraars. Ja, mijn naam is Jasper. Ik werk bij TNO als onderzoeker op het onderwerp hoe mensen en AI-systemen met elkaar moeten interacteren. In die context werk ik veel aan Explainable AI. En hoe moeten mensen interacteren met AI? Hoe? Ja, dat ligt heel erg aan natuurlijk aan wat voor AI-systeem. Een zelfrijdende auto is heel anders dan je willekeurige beslissingsondersteuningssysteem voor arts of iets dergelijks. En ook afhankelijk van welke gebruiker je hebt. Maar in het algemeen denk ik dat mensen en AI-systemen op zo'n manier moeten communiceren en interacteren dat het daadwerkelijk ook die mens helpt bij hetgeen wat die moet doen of wil gaan doen. Ja, en daar komt dan die uitlegbare Explainable AI om de hoek kijken, toch? Klopt, inderdaad. Want ik geloof wel sterk dat als je goed wilt kunnen werken met een AI-systeem, dan moet je ook weten wat je aan dat AI-systeem hebt. En voor mij is Explainable AI eigenlijk de methode waarmee zo'n AI-systeem kan uitleggen aan jou als gebruiker van wat je nou eigenlijk aan dat ding hebt. Wanneer het op kan vertrouwen, wanneer het goed advies geeft, wanneer het een slecht advies geeft, wanneer het misschien wat onzekerder is. Al dat soort aspecten vormen voor mij eigenlijk de uitdaging van Explainable AI. Ja, vooral ook mooi dat je zegt dat je ook kan herkennen dat het een slecht advies is. Ja. Want dan kan je ook besluiten om het advies niet over te nemen. Ja, klopt. Dus wat ik vaak hoor is dat Explainable AI echt nodig is voor het vertrouwen. En dan proef ik af en toe van hoe meer vertrouwen, hoe beter. Maar we weten ook altijd van zo'n systeem is ook niet altijd correct. Dus je moet ook heel goed weten wanneer dat systeem het wat minder goed weet. Of gewoon eigenlijk je verkeerd advies voorschotelt. En om die inschatting te maken heb je een zeker begrip nodig van hoe dat systeem werkt. Ja, en daardoor inderdaad ook dat je als je antwoord krijgt, dat je zegt van nou, deze slaan we even over. Exact, inderdaad. Zou je voorbeelden kunnen geven van hoe uitleg eruit zou kunnen zien bij een AI systeem? Oh ja, ja. Ik ben zelf groot fan van zorgen dat die uitleg die je ontwikkelt, mooi geïntegreerd zit in de taak die jij moet uitvoeren als mens. Dus terwijl je je eigen werk aan het doen bent, geconfronteerd wordt met af en toe een stukje kennis over dat systeem, hoe het werkt, hoe het niet werkt. Dit tegenover gewoon een laagje uitlegbaarheid er bovenop plakken, ergens in je dashboard weggestopt, dat je op een vraagtekentje kan klikken en dan zeggen van oh, nu krijg ik een mooi plaatje te zien. Want dat werkt vaak niet. Nee. Heb je een voorbeeld? Waar moet ik dan aan denken? Ja, dus wat zou een goed voorbeeld zijn? Stel je hebt een arts die werkt met een diagnostisch systeem, die helpt die arts met een bepaalde diagnose te stellen, dan weten we bijvoorbeeld dat artsen zo'n diagnose stellen door de voor- en tegens halverwege symptomen met elkaar te vergelijken. Naar een AI-systeem kan je die arts gewoon vertellen van dit is de diagnose die ik denk, maar dat past niet goed bij de werkwijze van die arts. Want die wil gewoon weten wat zijn überhaupt de symptomen, waar duiden die op. Dus de uitleg die je het systeem dan kan geven is ik denk aan deze diagnose, omdat ik deze symptomen zie, die zijn bijvoorbeeld in een argument voor die diagnose die ik geef, maar ik zie ook wel deze symptomen die daar minder op aansluiten. Dus daarom is mijn nummer twee advies meer dit. Dan heb je gelijk een uitleg waar dat systeem naar kijkt en je helpt die arts gewoon in dienst eigen taak uit te voeren door bijvoorbeeld die symptomen te schetsen. Ja, heel mooi. Wat bij mij ook meteen oppopt, en dat is denk ik een van de meest eenvoudige vormen van uitleg, is dat je krijgt natuurlijk ook recommendations, aanbevelingen bij Netflix of welke andere streamingdienst dan ook. En dan staat gewoon omdat je deze film hebt gekeken, bevelen we deze aan. Hoe goed dat is, dat is maar de vraag. Maar dat omdat, dat is bijvoorbeeld ook zo'n stukje uitleg. Dus je geeft eigenlijk meer context terug, waardoor je hem zelf meer begrijpt. En de arts voorbeeld dan nog zelfs meer context, omdat het ook zelfs nog in het denkproces zit, passend bij de actie, wat denk ik nog heel mooi is om daarin mee te nemen. Klopt, inderdaad. En zeker die kopjes die je bij Netflix ziet, is inderdaad een hele minimale uitleg. Ja, dat is echt de meest eenvoudige vorm van uitleg. Ik weet niet hoe het met jullie zit, maar bij mij is het hier ook regelmatig verkeerd. Dan denk ik, oh ja, dat kan wel wat beter. Maar met het juiste goed voorbeeld, van dat je zelf dan ook ziet van, nou, daar is nog behoorlijk wat verbetering nodig. Ja, inderdaad. Ja, ook daar zou ik al meer context willen. Dus bij zo'n voorbeeld al zou ik al meer willen weten, maar waarom dan? Nee, dat wil ik aanpassen. Dat past niet, zeg maar. Ja, dat is wel een goeie. Dus stel dat je dat... Het geval van Niels, wat hij zegt, van... Vanuit die uitleg, en dan zou je iets willen aanpassen. Hoe kijk je daar tegenaan? Nou, even de vraag van, wat zou je willen aanpassen? Wat denk je van, dit mis ik aan de uitleg. Nou ja, als de uitleg is van, omdat je deze film hebt gekeken. Nou ja, toevallig heeft mijn kind ook op Netflix gezeten. En dan kon die film niet op dat profiel, dus heeft hij op het andere profiel gekeken. Misschien ook security-wise moet ik daar nog wat dingetjes voor doen, maar dat is even wat anders. Maar die wil ik eigenlijk uit de model hebben, waardoor die dat mij adviseert. Ah ja, ja. Ik denk dat dit een mooi voorbeeld is van waar uitleg, inderdaad, in ieder geval de uitleg die we nu beschikbaar hebben, vaak al toegepast wordt. Alleen dat eerste stapje is. Want nu heb je een stukje kennis dat je denkt van, oh, ik krijg deze suggesties blijkbaar door deze film, maar jij weet de context erachter dat dit niet door je eigen gedrag bijvoorbeeld komt. En ik denk inderdaad, dan is het stapje daarvoor, of daarna eigenlijk, dat je tegen Netflix kan vertellen, nou ja, maar die film had eigenlijk op dit account gemoeten. En dat je dan een voorbeeld niet meer krijgt. En dat is dan voor mij niet zozeer de uitleg, maar gewoon wat uitleg jou in staat stelt om te kunnen doen. Wat wel een mooi stukje uitleg kan zijn, daarop is van wat de gevolgen zijn van de keuze. Dus dat je, als je inderdaad zegt van, nou, deze film hoort toch bij dat andere account, dat Netflix je dan ook vertelt, of welk systeem dan ook, wat dan uiteindelijk het advies of de categorie geweest zou zijn. Ja, van, oh, maar dan hadden we deze kinderfilm niet voor jou aan te raden, want dat past dan niet meer en waarom, daarom, daarom. Maar voor hetzelfde geldt dat Netflix alsnog je het aan kan raden, en dan moet je toch even achter je oren krabben van, nou ja, wat is er nu aan de hand. Sommige films kijk ik ook graag mee met de kinderen, moet ik eerlijk bekennen. Dus wie weet, weten ze meer dan wat ik denk. Maar dan nog is het goed om het uitgelegd te krijgen en het te kunnen begrijpen. En ik denk dat daar wel context en vertrouwen uiteindelijk toch wel vertrouwen is. En zeker ook dat je de foutieve keuzes ook kan toelichten, want dat is denk ik nog wel het belangrijkste. Niet alleen de goede, maar ook de fouten kan toelichten waarom, om vertrouwen te kunnen krijgen. Anders ga ik voor mij in mijn gevoel in mijn confirmation bias. Oh ja, dat dacht ik ook dat het goed was. Beschrijf dat het goed is, maar ook juist die andere kant, wanneer het verwacht wordt dat het niet goed is. Ja, zeker, want die confirmation bias of de menselijke bias ligt altijd best wel nauw om de hoek als het om uitleg gaat. Want in mijn onderzoek merk ik ook regelmatig dat de experimenten die wij doen, evaluaties, dat het feit dat het systeem plots een uitleg kan geven, heeft al een positief effect. Omdat mensen denken, oh het kan zich uitleggen, dan zal het wel een goed systeem zijn. Maar dan denk ik juist, nee we geven deze uitleg juist om jouw inzicht te geven dat het systeem soms niet werkt. En daar ligt ook wel een flinke uitdaging van hoe communiceer je nou die uitleg op de goede manier, dat mensen niet in die bias trappen. En dat is best wel een hele, dat is net wat jij zegt, dat ligt ook een beetje in de menselijke aard. Ik weet niet meer uit welk boek dat is waar ik dat uit had. Maar wat blijkt, dat als wij ergens een verklaring voor geven, ook al is die verklaring absurd, is het ook heel vaak genoeg voor iemand om daarmee om te gaan. En een voorbeeld wat in dat boek stond, dat is echt, ja hoe dan? Dat als jij ergens voordringt, vinden mensen je heel vervelend. En daar kunnen ze, zeg maar, de meesten zeggen daar wat van. Maar als jij voordringt en een verklaring geeft, welke dan ook, heb je hele grote kans dat mensen dat accepteren. En wat zij deden als tester bijvoorbeeld, was voordringen bij het kopieerapparaat. En ze stonden rij, en dan zeggen van, ja ik ga even voor, want ik moet kopiëren. Bij de kassa's deden ze, ik ga even voor, want ik moet even voor. Dus de 'want', uitleg, is dan genoeg. Dat sluit heel mooi aan bij wat we net zeiden, rondom de bias. Het onderstreept ook heel mooi dat uitleg gewoon een heel sociaal iets is. En dat wij mensen eigenlijk ook geprogrammeerd zijn om gewoon sociaal te reageren op die uitleg. Zoals inderdaad van, een uitleg die eigenlijk nergens op slaat. Van, ik wil voor, want ik wil kopiëren. Eigenlijk al voldoende is dat je plots als mens meer accepteert van zo'n systeem. Dan is het goed om te begrijpen, ook als data scientist of software engineer, dat juist dat soort biases eigenlijk bij je gebruiker ook gewoon spelen. En wat de gevolgen daarvan zijn als je plots een laagje uitleg over je systeem meeneemt. Ja, en als je, want ik denk een data scientist, technische mensen zijn hier misschien wat meer mee bezig. Stel je zou opdrachtgever zijn. Wat zou je daaraan mee willen geven, van hoe zij naar uitleg zouden moeten kijken? Ah ja, goede vraag. Wat ik veel merk, dus in de afgelopen jaren dat ik met het onderwerp bezig ben geweest, is dat er een zekere verwachting of begrip ontstaan is van explainable AI, dat het eigenlijk vooral vanuit wet- en regelgeving een dingetje is. Want je moet afvinken. Je moet het. Je moet het, inderdaad. En dan is een opdrachtgever toch een beetje op zoek naar van, oké, ik moet hier iets mee. Dus ik wil graag dat er iets mee gedaan wordt. Ja. En dan denk ik, nou dat is wel een hele open opdracht, denk ik dan. Ik zou zeggen van, zie het vooral, explainable AI, als meerwaarde voor je systeem. Het zorgt ervoor dat je gebruik er minder snel fouten maakt, dat die het systeem sneller accepteert, dat er gewoon een gebruiksvriendelijker systeem wordt. Maar dat vereist van die opdrachtgever wel ook dat die dat stukje besef heeft, dus waar die precies naar op zoek is. Zeg maar, doe je inderdaad alleen maar die uitleg omdat je het vinkje wilt zetten voor je wet- en regelgeving, of wil je ook dat je een degelijk goed systeem hebt? En dat komt gelijk ook mee kijken dat je mee moet gaan denken over hoe die uitleg eruit moet zijn. Samen met al die technische experts die vaak ook niet weten van, ja, wat voor uitleg wil deze gebruiker nou eigenlijk? Dat weet de opdrachtgever vaak wel. En hoe kan je iets zeggen over de kwaliteit van de uitleg? Want als mensen het nu gaan doen en daardoor vertrouwen ze hun systemen beter, dan zou iedereen op den duur nemen aan, zeg maar, ik leg wat uit. Maar hoe kan je wat zeggen over die kwaliteit van die uitleg? Want dat wil je dan op den duur wel controle over hebben. Ja, dat is ook een van de grootste vragen of uitdaging die ik nu zie. Technologieën te over. In heel veel manieren, er lopen in de duizenden al onderhand, van hoe je iets van een stukje informatie uit zo'n systeem kan halen. Wat nog mist inderdaad, is meten, is dit nou de uitleg waar men op zoek naar was? Het hele pragmatische en misschien de open deur is, een goede uitleg ontwerpen en communiceren naar je eindgebruiker, verwijst net zoals een willekeurige interface ontwerper, gewoon een goede ontwerper, UX designer, die in gesprek gaat met die gebruikers en gewoon probeert achterhalen wat zou kunnen werken. En als je het hebt, gewoon voor mij apart A/B testen, kijken hoe het werkt en wat niet werkt. Ja, dus eigenlijk gewoon heel logisch ermee omgaan. Dat is eigenlijk het idee. Ja, dat klinkt vaak zo makkelijk hè, die simpele dingen, maar het is altijd zo moeilijk om goed. Wat het hier heel lastig maakt, is dat we natuurlijk te maken hebben met de mystiek van AI, de complexiteit die daar omheen heerst en dan ben je heel snel geneigd om ook naar hele complexe oplossingen te grijpen. Klopt, toch? Ja, inderdaad. En ik snap ook wel de neiging dat als je als data scientist, je bouwt een mooi model en je opdrachtgever zegt wild explainable AI, nou dan pak je een bepaald Python package of een oplossing en die pas je erop toe en dan denk je ja, het ziet er wel goed uit. Het is nuttig voor mij als technisch expert, maar daar houdt het vaak ook op. En de opdrachtgever heeft vaak niet de kennis om dan in te schatten van ja, oké, is dit de state of the art? Is dit nou het beste wat we kunnen doen? Nou oké, het zal wel. Dus inderdaad, ik denk dat de oplossing toch vaak ligt in het gewoon logisch mee omgaan en blijven ook communiceren tussen die data scientist en die opdrachtgever. Of gewoon een goede product owner er tussen. Die een beetje die mystiek weg kan nemen. En je raakt hier volgens mij ook, je hebt natuurlijk als data scientist, heb je een andere uitleg nodig dan jij als opdrachtgever, als eindgebruiker, auditor misschien. Daar moet je natuurlijk ook rekening mee houden. En daar, ik als onderzoeker zie daar ook vooral dat er nog een groot kennishiaat ligt. Dat we gewoon nog niet goed weten van welke soort informatie of kennis hebben nou verschillende soorten rollen nodig om hun werk goed te doen. Die van de data scientist is dus redelijk afgedekt. Dat is heel technisch eigenlijk. En die hebben vaak ook zelf de expertise en de middelen om gewoon een beetje te proberen, uit te zoeken wat voor hunzelf werkt. Prima. Maar inderdaad, als je het hebt over een eindgebruiker of een auditor, die hebben hele andere rollen, hele andere verantwoordelijkheden. Om hun dezelfde uitleg te geven die die data scientist heeft gebruikt om te kijken of een model de moeite waard is om te deployen. Dat gaat natuurlijk niet werken. En dan zie je toch ook dat in het onderzoeksveld dat het toch voornamelijk gedomineerd wordt door de technische expertise. Die gewoon uit nieuwsgierigheid zelf dit soort techniek ontwikkelen. Ja. Want hoe komen jullie dan aan je onderzoek en de onderwerpen waarvan je zegt van dat ga ik onderzoeken? Ja, dus eigenlijk op twee manieren. Dus of we zien een sector of een partij die heeft gewoon echt een sterk behoefte om dit gewoon goed in te kunnen regelen voor onszelf. Dus denk aan de zorg bijvoorbeeld. Een ziekenhuis die denkt van wij willen AI toepassen of we hebben alle AI maar het moet explainable worden. Dat is één kant. Dus heel erg vraaggedreven. De andere kant is dat we het een beetje pushen. Dat we gewoon zien van dit is echt een sector waar wij als TNO vanuit onze maatschappelijke verantwoordelijkheid ook zien van hier moet gewoon explainable AI bij. Want anders heb je gewoon geen verantwoorde toepassingen van AI. Mag je daar eentje van noemen of niet? Ja hoor. Zelfrijdende auto's bijvoorbeeld. Daar is ook best een technocentrische trend. Hoe autonomer hoe beter. Er wordt wel nagedacht over hoe die interactie met hun inzittenden moet zijn. Maar dat is vaak toch een beetje oppervlakkig. Net zoals Google Maps waar je invult van A naar B en dat zou het moeten zijn. Maar gewoon die nuance vaak van hoe je auto zich dan gedraagt of wat je kan verwachten daarvan. Daar zit toch best veel onwetendheid en soms ook angst bij de maatschappij voordat ze in hun auto willen stappen. En daar ligt gewoon een potentiële winst voor explainable AI bij wijze van dat je auto aan de inzittenden uitlegt wat je kan verwachten van die auto. Dat is wel een mooi voorbeeld. En dat pushen jullie dan, want je zegt van daar wordt eigenlijk te weinig onderzoek gedaan. Laten wij dat starten. Ja. Soms niet eens dat er te weinig onderzoek naar gedaan wordt, maar dat het gewoon nog niet geadopteerd wordt door de bedrijven, door de IT sector die ermee bezig is. Dat is natuurlijk wel een andere stap. De adoptie. Want er wordt natuurlijk best wel heel veel onderzoek gedaan naar explainable AI. Wij maken natuurlijk zelf ook maatwerkmodellen. Er zijn nog maar heel weinig opdrachtgevers die dat aan ons vragen. Wij pushen eigenlijk ook bijna altijd. Ja. Hoe zie jij dat? Gaat dat veranderen? Ja, ik denk het wel. Heel pragmatisch. Wat ik net al zei. Er is een zekere behoefte, voelen mensen wel dat ze er iets mee moeten. Maar het moet nu nog niet. Neem bijvoorbeeld de AI Act, de AI verordening die eraan komt. Of nou ja, er is eigenlijk. Die heeft heel lang in de lucht gehangen en eigenlijk altijd dat onderwerp explainable AI aangepraat bij iedereen. Maar het hoeft nu nog niet. Nu is die er. Volgend jaar komt de standaardisering, als het goed is rond, die het heel concreet maakt. En dan moeten ze wat mee. Dus dan wordt het een beetje van bovenaf gepushed. Vind ik niet de ideale weg. Want zoals ik al zei, ik ben groot voorstander van dat die opdrachtgevers zelf inzien wat de meerwaarde is van XAI. Maar ik denk dat als eenmaal mensen eraan gewend zijn wat het is, wat het niet is, wat je er allemaal mee kan, dat wel die gevraag gaat groeien. Ja, dat hebben we natuurlijk eigenlijk rond de privacy wetgeving ook gezien. In het begin was natuurlijk ook gewoon een gedoe wat je hebt opgelegd. En je ziet daar steeds meer bewustwording in van waarom het misschien toch best wel belangrijk is. Ja, nou ja, inderdaad. Dus plat gezegd, het voelt dan als, oh, je moet plots extra werk gaan leveren. Maar achteraf denken toch veel mensen van, oh ja, er zit toch wel iets in. Ja, we gaan de kwaliteit er mee verbeteren. Ja, exact. En als bedrijven mee aan de slag gaan, heb je dan bepaalde tips of richtlijnen voor mensen die met explainer willen gaan verder aan de slag? Dat ligt een beetje aan het soort bedrijf. Dus ik weet niet wat je gedacht hebt. Nee, ik had vooral in het algemeen, als mensen hiermee dit onderwerp mee aan de slag gaan, waar moeten ze dan minimaal aan denken om de eerste stappen te kunnen zetten? Ja, nou de eerste tip die ik iedereen dan geef is, we vatten niet op alleen maar als een stuk technologie. Zie het niet als iets wat je je data scientist kan opleggen of kan overgeven aan je IT- bedrijf waar je mee in de samenwerking aan gaat. Maar zie het inderdaad gewoon iets als hetgeen wat je product kan verbeteren. Als je het goed ontwerpt, goed evalueert en ook goede technieken voor gebruikt, maar het complete plaatje, dat moet je wel overzien. Dus dat is vaak wel mijn tip, ongeacht welk soort bedrijf het is. Niet alleen maar te denken van, dit kan ik even bij mijn software engineer beleggen en dan komt het wel goed. Ja, nou echt als een onderdeel van je oplossing moet het een onderdeel zijn, wat je kwaliteit monitoring of inzicht over hoe goed is het product. Zo moet je hem echt meenemen. Ja, by design. Alles moet by design. Ja, vooral dit. En daarnaast zijn er natuurlijk ook sector specifieke frameworks waar je aan kan denken, toch? Nou ja, framework is een beetje een algemeen term. Of checklisten, dat je geholpen wordt. Ik weet bijvoorbeeld dat de Hoogschool Utrecht zo'n onderzoek heeft gedaan. Was jij volgens mij ook nog bij betrokken? Dat je dan in de financiële sector, dat er dan gezegd wordt van, denk hierover na, denk daarover na. Misschien kan jij het beter uitleggen dan ik het kan. Ja, inderdaad. Dus als we het specifiek hebben over checklists. Mijn mening daarover, ze hebben een waarde in bewustwording. Dat je inderdaad een lijstje van onderwerpen ziet die aangeven waar je aan zou moeten denken. Maar ze helpen je nog niet dat werk dan volgens die punten te adresseren. Maar goed, bewustwording is stap 1. Dus inderdaad, daar zijn raamwerken, frameworks voor. Dan denk ik ook wel gelijk van, oké, we worden af en toe een beetje doodgegooid door checklists. En de proof is in de pudding. Dus voor ons moet je het ook gaan doen. En dat verrijst ook eigenlijk gewoon tijd. Dat je als bedrijf je regelmatig probeert te doen, die checklist af probeert te gaan, te adresseren. En langzaamaan die kennis opbouwt van wat wel en niet werkt. Want wat ik in ieder geval zie uit onze onderzoekswereld, juist omdat die zo geconcentreerd is op die techniek. De kennis wat wel en niet werkt, moet vooral komen van de toegepaste onderzoekers en de bedrijven die het hands-on gaan uitproberen. Ja, echt uit de praktijk. Exact. En dit geldt natuurlijk heel veel voor machine learning. We hebben hier al veel machine learning modellen waarvan we weten welke data we erin stoppen. We hebben meestal dan zelf de algoritmes gekozen. Dus daar hebben we ook invloed op. Maar de meeste mensen die associëren nu AI met generatieve AI. En zelfs nog smaller ChatGPT. Ja. Ik zag even wat beweging bij je. Niels begint al te lachen. En zeg maar, dat heeft best een hele moeilijke verhouding. Dan zeg ik het volgens mij heel netjes. Een hele moeilijke verhouding met explainable AI. Ja. Zeker toen ChatGPT net uitkwam. Alweer een tijdje terug. Hoe snel het gaat. Toen dachten mensen al van, ja maar ChatGPT is toch al uitlegbaar. Kijk maar, ik hoef alleen maar te vragen aan hoe werken en dan krijg je een uitleg. En dat heeft wel enige muren opgeworpen. Ook in mijn eigen onderzoek. Dat dit idee dat mensen nu hebben, omdat generatieve AI zo communicatief vaardig is. Dat ze dan gelijk ook denken van, dat is de uitleg. Zou je kunnen uitleggen waarom dat geen uitleg is? Ja, want eigenlijk puntje bij paaltje. Generatieve AI fabriceert het ene woord na het andere woord. En dat kan het heel goed. En voornamelijk heel goed op basis van wat jij eraan vraagt. Dus als jij eraan vraagt van hoe werkt ChatGPT, dan gaat het een heel mooi rijtje woorden opsommen. Maar je hebt geen garantie dat het klopt. Het lijkt heel plausibel. Inderdaad, communicatief heel sterk. Dus dat tikkelt gelijk al onze biases als mens. Dat we denken van, oh ja, het zal wel kloppen. Maar je hebt er eigenlijk, puntje bij paaltje, helemaal niks aan. Het is eigenlijk die wand ik moet kopiëren. Exact. Want deze kennis helpt je niet inschatten wanneer je nou voor een ander antwoord moet bepalen. Is het nou echt op feiten gebaseerd of niet? Het helpt je niet inschatten van wat de privacy risico's zijn. Wanneer jij er iets bij wijze van je rapport aan geeft, dat je eigenlijk alles wat erin staat weggeeft aan de ander. Dus ik vind dat geen uitleg, omdat het geen informatiebehoefte van de persoon echt vervult. Als het een echte, zeg maar, ik doe even zo met mijn vingers quote quote, echte uitleg zou geven, dan zou ik moeten zeggen, ja, ik heb deze woorden in deze volgorde gekozen, omdat die statistisch gezien het beste bij elkaar past bij wat jij nu net dat stukje gegeven hebt. Maar ja, daar hebben we geen bied aan. Nee, inderdaad. Het is een antwoord en geen uitleg. Ja, dat is een goede. Ja. Dus ja, er is natuurlijk nog een lopend onderzoek van wat is nou wel een goede uitleg die werkt. Dus ik zelf werk nu recent ook aan een project van, dat gaat over kritisch denken. Toch iets wat vaak mist wanneer we met dit soort modellen werken. Want het blijkt toch dat kritisch denken gewoon het eerste is wat we vergeten om te doen, omdat we gewoon puur onder de indruk zijn over hoe goed deze technologie is. Dus dan kijken we naar van, oké, kan misschien uitleg af en toe jou helpen om toch weer terug te gaan naar van, oh ja, dit systeem dat fabriceert gewoon af en toe feiten. Oh ja, is dat hier het geval of niet het geval? En dan kan je denken bijvoorbeeld aan uitleg die eigenlijk de zekerheid geeft, of naar bronvermelding verwijst, dat soort dingen. Maar ook subtielere dingetjes, dat als je een XAI technologie hebt, die aangeeft dat bij een bepaalde prompt, dus een bepaalde vraag, het taalmodel toch waarschijnlijk wat minder goed gaat presteren, dat je dat taalmodel wat dwingt om in onzekere verwoording bijvoorbeeld te komen. Oh zo ja, dat zou wel mooi zijn ja. Dat zou wel heel menselijk zijn ja. Of in ieder geval in de communicatiestijl menselijk zijn, hoe we het interpreteren. Nou juist omdat je denkt dat je een dialoog hebt, zou het inderdaad helpen om dan wat jij zegt, in andere woorden, want die komen gewoon onbewust binnen. Ja, en dan gebruik je dus eigenlijk die menselijke bias ten goede, in plaats van het alleen maar verergert. In plaats van dit is het, die zegt van nou in dit geval zou het wel eens zo kunnen zijn. Ja, misschien kun je hier... En dan met jibber jabber, dan worden het wel hele lange teksten inderdaad. Ja, maar zo hebben we er eigenlijk nog nooit naar gekeken, wat cool. Nog een stap verder is het, nou ja, wat wij mensen continu doen, is als ik iets vraag aan een van jullie bijvoorbeeld, en jullie snappen de vraag niet goed, krijg ik een vraag terug. Dat zie je generatieve AI nooit doen. Tenzij je er om vraagt. Dan wel, maar goed. Dus dat zou ook een manier kunnen zijn. Het ligt wat verder af van uitleg natuurlijk, maar is daar ook mee verweven, want je kan bijvoorbeeld zo'n systeem, zo'n generatieve AI, zo'n wedervraag stellen, wanneer je merkt dat er toch een stuk onzekerheid in dat systeem zit, of dat die nu een vraag gesteld krijgt, die toch wel een beetje een outlier is op basis van de data die erop getraind is. Wat mooi. Of zelfs teveel overlap heeft, waardoor die niet weet waar te pinpointen. Interessant. Heel interessant. Zo had ik er nog niet naar gekeken. Misschien te technisch, de huidige architectuur staat het gewoon niet toe, dat je een uitleg krijgt van waarom die nou dat volgende woord gekozen heeft. Uiteindelijk zit dat er gewoon niet in. Maar als je dit soort vormen mee kan trainen, net zo goed als uiteindelijk kan die meerdere antwoorden geven, en er is getraind wat zogenaamd het beste antwoord is, dat zouden ze natuurlijk inderdaad mee kunnen trainen. De onzekerheid. Je mag je wel in een andere bewoording, is een onzekere bewoording eigenlijk het beste antwoord? Bijvoorbeeld inderdaad. Het tegenovergestelde geldt natuurlijk ook, dat als je weet dat het systeem het gewoon bij het rechte eind heeft, en je weet juist dat de mens altijd in dit soort situaties toch altijd een beetje onzekerder is, dat je misschien dan juist sterkere bewoording gebruikt. Op die manier krijgen we dus ook weer, met een mooi woord, de synergie tussen je systeem en de gebruiker, dat ze gezamenlijk toch de taak toch beter kunnen uitvoeren. Dat triggert mij wel wat. Het past bij de gebruiker, de mensen. Normaal gesproken, generiek, zijn hier wat stelliger in, dus dan kan ik dat zo doen. Maar ieder persoon heeft een eigen perspectief. Hoe ga je dan richting zo'n personalisatie, want dat is ook weer in de eye of the beholder, of degene die je ontvangt, hoe dat overkomt. Ja, dus personalisatie, dat is wel een onderwerp waar ik in XCI heel graag een keer aan zou willen werken. Ook omdat ik gewoon zie dat er bijna niks mee gedaan wordt. Nou weet ik dat bij de Universiteit Twente onder andere, het is een Europees programma, daar hebben we ook in gezeten, Natural Language for Explainable AI, oftewel natuurlijke taal voor uitlegbare AI. Daar is gekeken naar personalisatie juist. Dus er is gekeken in de zorg, want het kan zijn, als ik bij een arts zit, wil ik zoveel mogelijk informatie hebben, dan mag hij of zij zelfs een klein beetje jargon gebruiken, als ik maar heel duidelijk heb wat er is. Terwijl misschien weet je, als een ander, die kan zeggen van nou vertel maar of het goed komt, ja of nee. Ja, als uitleg. Dus daar is onderzoek naar gedaan van ja, hoe zou je nou zeg maar in een dialoog die je hebt, net zo goed als dat wij zeg maar dat op elkaar afstemmen, van hoe ver gaan wij bijvoorbeeld vandaag de diepte in. Dan ben je eigenlijk een soort van onbewust, ben je dat aan het afstemmen, dat je dat met je systeem dus eigenlijk ook hebt. Daar is heel veel onderzoek naar gedaan vanuit dat programma. Ja, dus dan heb je het vooral over de personalisatie in hoe gecommuniceerd wordt. Ja, ja. Ik denk dat daar inderdaad wel ook interessante, nog echt kennishiaten, blinde vlekken liggen. Dus ja, zeker in natuurlijke taal heb je lekker veel de ruimte ook om dit uit te zoeken. Een ander personalisatie aspect is ook van gewoon weten wat die persoon die het systeem tegen zich overgeeft, wel en niet al weet. Ja, context daarvan. Dat is wat ik op doelde inderdaad op dat stuk inderdaad. Maar die is wel interessant inderdaad. Want dat zit in bewoording, dat zit hem in terminologie. Diepgang. Ja, hoe snel kan je ook gaan. Want als we efficiëntie willen, dan willen we niet elke keer meer moeten zeggen, nou dat mag wel wat specifieker, kom maar even door, kom maar door. Dat wil je op de duur dat het aanvoelt. Sommige mensen hebben wat langer nodig om het te laten bezinken. Ja, maar ook inderdaad, de ene persoon begrijpt iets veel sneller als je een analogie gebruikt bijvoorbeeld. De ander die wil gewoon lekker vanaf begin af aan het hele logische pad, terwijl de andere persoon het weer alleen het einde wil weten. Ja, dat soort dingen dat zijn heel persoonlijk inderdaad. En op dit moment zien we gewoon dat, als we nu naar het algehele veld van Explainable AI kijken, gewoon nagenoeg bijna geen aandacht aan besteedt worden. Dat is wel een groot gemis. Dat is zeker een groot gemis, ja. Daar ben ik helemaal met je eens. Ik ben heel nieuwsgierig over Explainable AI, eigenlijk een vraag misschien voor jullie allebei. Stel we zijn drie jaar verder. Wat is dan het mooiste wat veranderd zou kunnen zijn in die drie jaar, als jullie nu vooruit mogen kijken? Op het gebied van Explainable AI. Ja, ik denk vanuit mijn eigen onderzoek zou mijn hoop zijn dat we toch eigenlijk wat afstappen van altijd die technocentrische blik. Laatst weer eens door de literatuur gespitst en dan zie je toch echt heel duidelijk de trend dat er bijna vier, vijf publicaties per dag gepubliceerd worden op het technisch vak van Explainable AI. Terwijl een mens gericht is misschien een honderdtal per jaar. Dus ik hoop dat daar een verschuiving in komt. Omdat, en dat is mijn tweede punt wat ik hoop over drie jaar, is dat we als maatschappij ook een openere blik hebben naar wat XAI allemaal voor ons kan doen. Niet alleen maar zien als een regulering of een dingetje wat we vanuit de wet zouden moeten afvinken. Maar echt met elkaar de meerwaarden ervan zien als we dat gewoon integreren in het ontwikkelproces, evaluatie en gebruik van onze AI systemen. Ik denk dat ik dat in het verlengde dan meeneem van dat het eigenlijk een soort van impliciete kwaliteit is geworden van als jij een AI systeem maakt. Zo van we hebben het er eigenlijk niet over. Het is logisch dat je uitleg krijgt. Dat er niet meer gepusht hoeft te worden, niet meer gevraagd hoeft te worden. Net zo goed als jij bepaalde verwachtingen hebt, zeg maar wat jouw auto doet, verwacht je ook gewoon dat een AI systeem uitleg geeft. Ja en dan ook de uitleg waar je op zit te wachten. Precies. Die je nodig hebt, ook als je nog niet van tevoren wist dat je die eigenlijk nodig had. Ja, normaal is een zaak van de wereld eigenlijk. Inderdaad. Ja. Ja, bij betreft moeten we dat nu hebben als ik het zo hoor. Het liefst wel. Jasper, wij hebben een nieuw onderdeel. In de vorige aflevering is die spontaan ontstaan. En dat gaat er eigenlijk over. We hebben een kaartspel ontwikkeld. We zullen ook een linkje opnemen in de show notes. Dan kan je hem zelf bestellen. Jij niet, jij krijgt hem straks. Maar als luisteraar. En dan gaan we je stellingen voorleggen. Dus hierbij. *muziek speelt* En dat kaartspel heeft zes categorieën met allemaal vragen. En zoals jullie kunnen zien heb ik er nu gewoon eentje voor jou eruit gehaald. Dus ik heb zelf eigenlijk ook geen idee. Laten we eens kijken. Het zit in de technologie en innovatie bestelling. Onze organisatie heeft een strategisch leider voor generatieve AI aangesteld om onze AI initiatieven te sturen. Dat vind ik heel voortvarend. Ja? Even kijken. Is dat mijn organisatie? Ja, laten we daarmee beginnen. Oké, dan even denken. Ja, dat zou ik al heel voortvarend vinden. Ik zou benieuwd zijn naar wat die persoon precies zal gaan doen. Want bij TNO doen we onderzoek naar generatieve AI natuurlijk. Gaat die persoon zich daar ook tegenaan bemoeien? Of gaat deze persoon het alleen maar hebben over hoe wij generatieve AI gebruiken in bijvoorbeeld het doen van onderzoek? Als we dat laatste eens pakken. Bijvoorbeeld is de beleid rondom mag je generatieve AI bij jullie toepassen bij het onderzoek? Daar hebben we nog geen beleid over. Dat wordt nu eigenlijk op ad hoc basis inderdaad besloten. Het komt er wel aan, dus ik ben heel benieuwd. Ik weet dus ook niet of er daar zo'n één persoon bij zit die dat gaat doen voor onze organisatie. Ik zou wel denken dat het een heel capabele persoon is. Ja, toch? Dat is een behoorlijk vraagstuk. Wat je bij ons ziet, als ik dat mag vertellen, is dat wij dat ook niet bij één persoon hebben belegd. Er is een soort van denktank waarin vanuit verschillende disciplines gekeken wordt. Waar lopen we eigenlijk tegenaan? We hebben natuurlijk ook te maken met AI beleid van onze klanten waar wij ons werk voor doen. Die verschillend zijn. Bij sommigen mogen we generatieve AI prima inzetten. Anderen zeggen nee, totaal verbod. En daar moeten wij natuurlijk ook mee omgaan. En we moeten ook nadenken over onze eigen medewerkers. Straks in de opleiding bijvoorbeeld. Waar mogen ze wel generatieve AI gebruiken, maar waar niet? Dat je de basis gaat leren, dat soort zaken. Ja. Ik zelf zou ook meer voorstander zijn voor een comité, een denktank. In ieder geval een groep mensen die verschillende perspectieven inderdaad kan inbrengen in zo'n discussie. Niet dat je inderdaad één persoon hebt die al het mandaat bepaalt rondom wat wel en niet mag. Zeker inderdaad in een organisatie zoals die van jullie waar je ook te maken hebt met verschillende beleiden van je klant. Precies. Ik denk dat die persoon dan een behoorlijke kluif heeft om dat allemaal te gaan afstemmen. Ja, zeker. Je hebt natuurlijk ook met allerlei juridische dingen te maken. Exact. Die persoon moet ook de technische beperkingen kennen, de mogelijkheden. Er zijn heel veel aspecten die komen kijken. Wat ik denk wel belangrijk vind, om hier nog even op in te haken, is dat je wel weet dat er een denktank is of dat er iemand het op zijn bordje heeft. Dat de verantwoordelijkheid gevoeld wordt. Want het onderwerp, als ik jullie zo hoor, is dus wel van essentieel belang om het hier met elkaar over te hebben. Dus daarmee helpt het denk ik wel als je zo'n kwartiermaker hebt, of aanspreekpunt, of eindverantwoordelijke, die een denktank nodig heeft. Dat denk ik wel mee. Alle verschillende aspecten zijn echt heel veel. Maar ik denk dat het wel helpt om daar iemand te hebben die daar echt voor gaat staan. En ook goed benaderbaar is. Ja, zeker. Dit is dat kaartspel, als het leuk is, tussendoortje. Zo, ik zit tegen mijn microfoon aan te tikken. Waar ik nog benieuwd was, is met de onderzoeken die jij doet, die TNO doet, rondom explainable AI, XAI, is er een rode draad in te vinden in bevindingen? Of onderzoek wat je doet? De rode draad in mijn eigen onderzoek, en dat van mijn team, is in principe ook dat mensen gericht te proberen te pushen in het XAI onderzoek. Want ik ben er heel van overtuigd dat al die techniek pas nuttig is wanneer we weten hoe we moeten toepassen. En daarvoor doen we de projecten die wij doen. Dus dat is eigenlijk de rode draad in mijn eigen onderzoek en dat van mijn team. Wat de algemene bevindingen die ik ook merk, en ook bij mijn andere collega's die met XAI werken, is dat juist die stap zetten naar een stuk technologie inzetten voor een bepaalde toepassing, dat je dan nog tegen zoveel problemen aanloopt. En een van die grootste problemen is dat toevoegen van uitleg nog niet gelijk betekent dat je eindgebruiker er gebruik van gaat maken. Dus in de experimenten die wij doen, de evaluaties en de validaties, zien wij toch regelmatig dat een merendeel van de gebruikers denken van "ja, ik zit echt te wachten op de uitleg, oh dankjewel, nu heb ik een systeem dat uitleg geeft." En vervolgens gebruiken ze het niet meer. Dus dan vraag je achteraf aan ze van "hoe vond je de uitleg?" "Ja, heel goed, maar ik had er geen tijd voor of ik vond hem toch niet zo nuttig, maar het was wel heel goed dat hij er was, voor als ik hem nodig zou hebben." En dan kijk je achteraf hoe ze hun werk hebben gedaan en dan denk je gewoon van "oh ja, ze hebben inderdaad geen gebruik gemaakt van die uitleg." En dat is denk ik wel echt een van de grootste uitdagingen die ik zelf nu ook zie in TNO breed. Dat we als toegepaste onderzoeksinstantie daar toch nog wel een hele kluif aan hebben. En dat gaat dan voornamelijk over van "hoe communiceer je nou deze uitleg op zo'n manier dat mensen er gebruik van gaan maken?" En dan gaan we naar "hoe kunnen we dan ook de uitleg kunnen gebruiken om mensen er gebruik van te maken?" En dan gaan we naar "hoe kunnen we dan ook de uitleg kunnen gebruiken om mensen er gebruik van te maken?" En dan gaan we naar "hoe kunnen we dan ook de uitleg kunnen gebruiken om mensen er gebruik van te maken?" Ik zou zeggen, het is misschien de verkeerde vraag om te stellen, maar goed, dat is dan ook de wetenschapper in mij. Ik zou meer dan zeggen, heeft iedere gebruiker een uitleg nodig? Waar ook meespeelt welk model die persoon gebruikt. Ik zou zeggen, nee, niet altijd. De truc is, wanneer bepaal je nou wanneer dat wel of niet nodig is? Daar hebben we eigenlijk zo goed als geen antwoord op op dit moment nog. En dat is puur omdat niet elke persoon hoeft dat model te begrijpen, of hoeft te begrijpen wat er uitkomt. Het is zelden het geval, maar het gebeurt wel eens inderdaad. En ik denk dat in een merendeel van de gevallen is ook al een klein stukje uitleg of een klein stukje begrip in het model al voldoende. Dus voor een ontwikkelaar die heeft van natuur eigenlijk best wel veel inzicht nodig in het model, maar zijn eindgebruiker, zoals we net al zeiden, die heeft dan misschien alleen dat stukje informatie nodig om het besluit te nemen. En dan hoeft hij niet te weten op welke data het systeem is getraind, of het een decision tree is, of een neuraal netwerk. Dat maakt allemaal niet zoveel uit. Die wil misschien vooral weten van, ja, oké, wat heb je eigenlijk meegenomen in dit besluit? En heb je nog alternatieven overwogen? Oh ja, oké, nou, dan ga ik alsnog voor dit andere besluit. Dus ja, het is een hele moeilijke keus, denk ik, ook om dat te maken. Ja, oké, mooi. Ja, volgens mij hebben we zoveel geraakt. Heel veel dingen die ik nu bedenk inderdaad. En ik zou hier nog uren met je over door kunnen praten, maar dat doen we dan als de microfoon zo direct dicht zit. Jasper, echt ontzettend bedankt, zeg maar, voor al je inzicht op het gebied van uitlegbare AI. Graag gedaan. Heel fijn dat je wilde komen. Ja, en ik hoop, de voorspelling drie jaar, misschien dat het al eerder is, maar dat we daar heen gaan met de zaken die we net hebben genoemd. Een hele mooie. Dank je wel voor het luisteren. Vergeet je niet te abonneren via je favoriete podcast-app. Als je eventjes op vol gedrukt bij Spotify bijvoorbeeld, dan krijg je automatisch een seintje als er een nieuwe aflevering is. En dat is op de maandag met een gast, op donderdag, korte aflevering. Kan je niet missen. Tot snel! [Muziek] 