# 12. Glossary

This glossary defines the most important domain and technical terms used in the context of PromptArchitect. These terms are essential for ensuring a common understanding among all stakeholders involved in the development, deployment, and usage of the system.

| **Term**                   | **Definition**                                                                                                                                 |
|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| **Engineered Prompt**       | A carefully designed input used to generate specific responses from an AI model, developed to ensure precision, reusability, and scalability.   |
| **PromptArchitect**         | A tool designed to facilitate the development, deployment, and management of engineered prompts, ensuring consistent interactions with AI models.|
| **Ollama**                  | A framework that allows running open-source AI models locally, giving users greater control over their AI model deployments.                   |
| **Gemma2**                  | A robust, versatile AI model supported by Ollama, suitable for a wide range of natural language processing tasks.                              |
| **Llama3.1**                | An advanced AI model providing cutting-edge performance in language understanding and generation, supported by the Ollama framework.            |
| **Mistral**                 | A lightweight, efficient AI model designed for quick responses and lower resource usage, supported by the Ollama framework.                    |
| **Continuous Integration (CI)** | A development practice where code changes are automatically tested, built, and integrated into the main codebase, often using platforms like GitHub. |
| **Configuration Manager**   | A component of PromptArchitect responsible for managing and distributing configuration settings for various AI models.                        |
| **API Endpoint**            | A specific URL where an API service can be accessed by a client application to perform operations, such as sending a prompt to an AI model.     |
| **Locally Hosted Model**    | An AI model that is run on the userâ€™s local hardware rather than in the cloud, providing enhanced data privacy and control.                    |
| **Testing Framework**       | The set of tools and practices used to ensure that engineered prompts function correctly and efficiently across different AI models.            |
| **Versioning**              | The practice of tracking and managing changes to prompts and configurations, often involving the assignment of version numbers to manage updates. |
| **Python Package**          | A collection of Python modules bundled together, which can be installed via the Python Package Index (PyPI) using tools like pip.               |
| **Deployment**              | The process of installing and configuring PromptArchitect and its components in various environments, such as development, testing, and production. |
| **Performance Testing**     | The process of evaluating the efficiency of prompts, particularly in terms of speed and resource usage, to ensure optimal performance.          |
| **Security and Privacy**    | Measures taken to protect data and ensure that interactions with AI models are secure, especially important when running locally hosted models.  |
| **API**                     | Application Programming Interface; a set of protocols and tools for building software and applications, often used for interacting with AI models.|
| **GitHub**                  | A platform for version control and collaboration, allowing developers to manage code, track changes, and implement continuous integration workflows.|
| **Prompt Execution Engine** | The core component of PromptArchitect responsible for executing engineered prompts across different AI models, whether cloud-based or local.    |
| **Build Process**           | The sequence of steps that convert source code into a deployable package, often automated in a CI pipeline.                                     |
| **Local Server/Workstation**| The hardware on which locally hosted models are run, typically managed by the user to provide the necessary computational resources.            |
| **Network Security**        | The practice of securing the communication between systems, particularly important for protecting data when interacting with cloud-based AI models.|
| **Python**                  | A high-level programming language used for developing PromptArchitect, known for its readability and widespread use in AI and machine learning. |
| **CLI (Command Line Interface)** | A text-based interface that allows users to interact with PromptArchitect by typing commands, offering flexibility and automation capabilities.|
| **Custom Theme**                 | A user-defined set of styles and layouts applied to the PromptArchitect dashboard to match organizational branding or user preferences. |
| **GitHub Pajamas Theme**         | A predefined theme for the PromptArchitect dashboard, based on the GitHub Pajamas Design System, used as an example for custom theming. |
| **Template String Substitution** | A feature that allows placeholders in a prompt file to be replaced with specific values at runtime, enabling dynamic prompt generation. |
