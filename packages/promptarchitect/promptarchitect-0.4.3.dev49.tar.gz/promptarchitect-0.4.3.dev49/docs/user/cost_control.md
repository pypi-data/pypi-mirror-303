# Cost Control

Effective cost management is crucial when working with Large Language Models (LLMs), as usage costs can accumulate quickly, especially when processing large volumes of data or running extensive tests. PromptArchitect provides several features to help you monitor, control, and optimize costs associated with prompt executions and testing.

---

## Table of Contents

- [Cost Control](#cost-control)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Understanding Costs](#understanding-costs)
    - [Token-Based Pricing](#token-based-pricing)
    - [Model Selection](#model-selection)
  - [Cost Control Features in PromptArchitect](#cost-control-features-in-promptarchitect)
    - [Token Counting](#token-counting)
    - [Model Configuration](#model-configuration)
    - [Using Open-Source Models with Ollama](#using-open-source-models-with-ollama)
      - [Benefits of Using Open-Source Models](#benefits-of-using-open-source-models)
      - [Getting Started with Ollama](#getting-started-with-ollama)
    - [Caching Test Results](#caching-test-results)
    - [Monitoring and Reporting Costs](#monitoring-and-reporting-costs)
      - [OpenTelemetry Integration](#opentelemetry-integration)
    - [Implementing Caching in Production](#implementing-caching-in-production)
      - [Considerations for Caching in Production](#considerations-for-caching-in-production)
      - [Ways to Implement Caching](#ways-to-implement-caching)
    - [Batch Processing](#batch-processing)
  - [Best Practices](#best-practices)
  - [Examples](#examples)
    - [Example 1: Limiting Max Tokens](#example-1-limiting-max-tokens)
    - [Example 2: Using a Cheaper Model for Testing](#example-2-using-a-cheaper-model-for-testing)
    - [Example 3: Configuring OpenTelemetry with Azure Application Insights](#example-3-configuring-opentelemetry-with-azure-application-insights)
    - [Example 4: Implementing Caching in Production Code](#example-4-implementing-caching-in-production-code)
  - [Troubleshooting](#troubleshooting)
  - [Additional Resources](#additional-resources)

---

## Introduction

When using LLMs, each prompt execution and test can incur costs based on factors like model type, input and output token counts, and the number of API calls. PromptArchitect helps you manage these costs by providing tools and features designed to optimize usage and provide transparency into where costs are incurred.

## Understanding Costs

### Token-Based Pricing

Most LLM providers, such as OpenAI, charge based on the number of input and output tokens processed. The cost per token varies depending on the model used.

- **Input Tokens**: Tokens in the prompt sent to the model.
- **Output Tokens**: Tokens generated by the model as a response.

### Model Selection

Different models have varying capabilities and costs. Generally, more advanced models like GPT-4 are more expensive than simpler ones like GPT-3.5.

- **High-Capability Models**: Offer better performance but at higher costs.
- **Economical Models**: More cost-effective but may have limitations.
- **Open-Source Models**: Can be run locally at no per-use cost, providing significant cost savings.

## Cost Control Features in PromptArchitect

PromptArchitect provides several features to help you control costs:

### Token Counting

- **Automatic Counting**: PromptArchitect automatically counts the number of input and output tokens for each prompt execution.
- **Cost Calculation**: Uses token counts and model pricing to calculate the cost of each operation.

### Model Configuration

- **Default Models**: Specify default models per provider to ensure consistent and cost-effective usage.
- **Model Aliases**: Use model aliases to switch between models easily.
- **Provider Configuration Files**: Define input and output token costs for each model in `providers.json`.

**Example `providers.json`:**

```json
{
    "openai": {
        "gpt-3.5-turbo": {
            "input_tokens": 0.0000015,
            "output_tokens": 0.000002
        },
        "gpt-4": {
            "input_tokens": 0.00003,
            "output_tokens": 0.00006
        },
        "default_model": "gpt-3.5-turbo"
    }
}
```

### Using Open-Source Models with Ollama

To eliminate per-use costs entirely, PromptArchitect supports the use of open-source language models running locally via [Ollama](docs/ollama_models.md). By leveraging open-source models, you can significantly reduce or eliminate costs associated with API calls to commercial LLM providers.

#### Benefits of Using Open-Source Models

- **Cost Savings**: Run models locally without incurring per-use charges.
- **Privacy and Control**: Keep data on-premises, enhancing privacy and security.
- **Customization**: Fine-tune models to better suit your specific use cases.

#### Getting Started with Ollama

Refer to the [Ollama Models Documentation](docs/user/ollama_models.md) for detailed instructions on setting up and configuring open-source models with PromptArchitect.

**Key Steps:**

- **Installation**: Install Ollama on your local machine.
- **Model Download**: Download supported open-source models like Gemma2, Llama3.1, or Mistral.
- **Configuration**: Configure PromptArchitect to use Ollama as the provider and specify the local model.

**Example Prompt Configuration:**

```yaml
---
provider: ollama
model: gemma2
parameters:
  temperature: 0.7
  max_tokens: 2500
---
```

### Caching Test Results

To minimize costs during testing, PromptArchitect caches test results. This means that when you run tests multiple times, previously executed tests with the same inputs and configurations do not make additional API calls, thus saving on costs.

- **Automatic Test Result Caching**: Test results are automatically cached based on the test configuration and input samples.
- **Cache Invalidation**: If the prompt or test configuration changes, the cache is invalidated to ensure that tests are re-run with the updated settings.
- **No Manual Configuration Required**: The caching mechanism works behind the scenes without the need for manual setup.

### Monitoring and Reporting Costs

- **Cost Reports**: Generate detailed reports showing costs per prompt and test.
- **Dashboard Statistics**: View total costs, cost per model, and other metrics in the dashboard.
- **OpenTelemetry Integration**: Send cost statistics and performance metrics to a backend of your choice using OpenTelemetry, allowing for centralized monitoring and analysis.

#### OpenTelemetry Integration

PromptArchitect integrates with [OpenTelemetry](https://opentelemetry.io/) to export cost statistics and performance metrics. OpenTelemetry is an open-source observability framework that provides a standardized way to collect and export telemetry data.

- **Sending Cost Statistics**: PromptArchitect uses OpenTelemetry to send detailed cost and performance data for prompt executions and tests.
- **Configurable Backends**: You can configure a backend of your choice to collect and analyze the telemetry data. OpenTelemetry supports a wide range of backends and integrations, including Prometheus, Grafana, Azure Application Insights, Jaeger, Zipkin, and more. See the list of [OpenTelemetry integrations](https://opentelemetry.io/ecosystem/integrations/).
- **Real-Time Monitoring**: By integrating with a telemetry backend, you can monitor costs and performance in real-time, set up alerts, and visualize trends over time.
- **Configuration**: To enable OpenTelemetry in PromptArchitect, you need to configure the OpenTelemetry SDK and set up the desired exporters.

### Implementing Caching in Production

While PromptArchitect provides caching mechanisms during testing to minimize costs, caching prompt executions in a production environment depends on your specific use case. Implementing caching can significantly reduce costs and improve performance by avoiding redundant API calls to LLM providers.

#### Considerations for Caching in Production

- **Use Case Dependency**: Not all applications benefit from caching. Consider whether your application's responses are deterministic and can be safely cached without affecting user experience.
- **Data Freshness**: Determine how long cached responses remain valid. For dynamic data or time-sensitive information, caching might not be appropriate.
- **Cache Invalidation**: Implement strategies to invalidate or refresh cache entries when underlying data changes.

#### Ways to Implement Caching

- **Python Decorators**: Use decorators to add caching functionality to functions that execute prompts.
- **File Existence Checks**: Before executing a prompt, check if a response for the given input already exists on disk.
- **In-Memory Caching**: Use in-memory caching solutions like Redis or Memcached for distributed systems.
- **Custom Caching Logic**: Implement caching tailored to your application's needs, considering factors like concurrency, cache size, and eviction policies.

### Batch Processing

- **Bulk Execution**: Process multiple prompts or inputs in batches to optimize API usage.
- **Parallel Processing**: Use parallelism carefully to balance speed and cost.

## Best Practices

- **Choose the Right Model**: Use economical models for testing and development; reserve high-capability models for production. Consider using open-source models with Ollama to eliminate per-use costs.
- **Optimize Prompts**: Make prompts as concise as possible to reduce token counts.
- **Set Token Limits**: Use `max_tokens` parameter to limit the length of generated outputs.
- **Implement Caching in Production**: Assess your application's needs and consider implementing caching mechanisms in your own code to reduce costs and improve performance.
- **Leverage Test Result Caching**: Take advantage of the automatic caching of test results to prevent unnecessary API calls during testing.
- **Leverage OpenTelemetry**: Use OpenTelemetry integration to monitor costs and performance in real-time, allowing for proactive cost management.
- **Monitor Usage**: Regularly check cost reports and telemetry data to identify and address any unexpected expenses.
- **Automate Testing**: Incorporate cost checks into your testing pipelines to catch costly prompts early.

## Examples

### Example 1: Limiting Max Tokens

**Prompt Configuration:**

```yaml
---
provider: openai
model: gpt-3.5-turbo
parameters:
  temperature: 0.7
  max_tokens: 500  # Limit output to 500 tokens
---

Write a detailed summary of the following text:

{{input}}
```

### Example 2: Using a Cheaper Model for Testing

**Switch to a less expensive model during development:**

```yaml
---
provider: openai
model: gpt-3.5-turbo  # Use a cheaper model
parameters:
  temperature: 0.7
  max_tokens: 2500
---
```

### Example 3: Configuring OpenTelemetry with Azure Application Insights

**Python Code:**

```python
# Install the Azure Monitor OpenTelemetry Exporter
# You can install it using pip if it's available:
# pip install opentelemetry-exporter-azure-monitor

from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.resources import Resource
from opentelemetry.exporter.azure.monitor import AzureMonitorMetricExporter
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.metrics import set_meter_provider

import os

# Set your Azure Monitor connection string or instrumentation key
connection_string = "InstrumentationKey=YOUR_INSTRUMENTATION_KEY"

# Set up Azure Monitor exporter
exporter = AzureMonitorMetricExporter(
    connection_string=connection_string
)

# Create a MetricReader and MeterProvider
reader = PeriodicExportingMetricReader(exporter)
provider = MeterProvider(metric_readers=[reader])

# Set the MeterProvider
set_meter_provider(provider)

# Initialize PromptArchitect
from promptarchitect import EngineeredPrompt

prompt = EngineeredPrompt(
    prompt_file_path='path/to/prompt_file.prompt',
    output_path='output_directory'
)

# Execute the prompt
response = prompt.execute(input_file='path/to/input_file.txt')
```

**Notes:**

- **Installation**: Make sure to install the Azure Monitor OpenTelemetry exporter:

  ```bash
  pip install opentelemetry-exporter-azure-monitor
  ```

- **Configuration**:

  - Replace `"InstrumentationKey=YOUR_INSTRUMENTATION_KEY"` with your actual Azure Application Insights instrumentation key or connection string.
  - You can obtain the instrumentation key from the Azure portal under your Application Insights resource.

- **Exporter Setup**:

  - The `AzureMonitorMetricExporter` is used to export metrics to Azure Application Insights.
  - `PeriodicExportingMetricReader` is set up to periodically export metrics.

- **Resource Attributes**:

  - You can add resource attributes like service name or version if needed:

    ```python
    resource = Resource.create(attributes={
        "service.name": "PromptArchitectService",
        "service.instance.id": "Instance1"
    })

    provider = MeterProvider(metric_readers=[reader], resource=resource)
    ```

- **Run the Application**:

  - When you execute your prompts, metrics will be sent to Azure Application Insights.
  - You can view these metrics in the Azure portal under your Application Insights resource.

### Example 4: Implementing Caching in Production Code

**Example using file-based caching:**

```python
import os
import hashlib
from promptarchitect import EngineeredPrompt

def execute_prompt_with_file_cache(prompt_file, input_data, cache_dir='cache'):
    os.makedirs(cache_dir, exist_ok=True)
    # Create a unique hash for the input data
    cache_key = hashlib.md5(input_data.encode('utf-8')).hexdigest()
    cache_file = os.path.join(cache_dir, f'{cache_key}.txt')

    if os.path.exists(cache_file):
        # Read response from cache
        with open(cache_file, 'r') as f:
            response = f.read()
    else:
        # Execute prompt and save response to cache
        prompt = EngineeredPrompt(prompt_file_path=prompt_file)
        response = prompt.execute(input_data)
        with open(cache_file, 'w') as f:
            f.write(response)
    return response

# Usage
input_text = 'Your input data here'
response = execute_prompt_with_file_cache('path/to/prompt_file.prompt', input_text)
```

**Notes:**

- **Hashing Input Data**: Uses MD5 hashing to create a unique cache key for each unique input.
- **Cache Directory**: Stores cached responses in a specified directory.
- **Cache Retrieval**: Checks if the response exists in the cache before executing the prompt.

## Troubleshooting

- **Unexpected High Costs**:
  - **Check Token Counts**: Review prompts for unnecessary verbosity.
  - **Review Model Selection**: Ensure you're not unintentionally using a more expensive model.
  - **Consider Open-Source Models**: Evaluate if using an open-source model via Ollama suits your needs to eliminate per-use costs.

- **Test Results Not Cached**:
  - **Cache Invalidation**: Understand that the cache is invalidated if the prompt or test configuration changes.
  - **Input Changes**: If input samples change, tests will be re-run and costs may increase.

- **OpenTelemetry Not Sending Data**:
  - **Exporter Configuration**: Ensure that the Azure Monitor exporter is correctly configured and that your instrumentation key is valid.
  - **Dependencies**: Verify that all necessary OpenTelemetry packages are installed.

- **Cache Not Working in Production**:
  - **Cache Keys**: Ensure that cache keys are correctly generated and match for identical inputs.
  - **Concurrency Issues**: Be cautious of race conditions when multiple processes access the cache.
  - **Cache Storage**: Verify that the cache directory is accessible and has proper permissions.

- **Cost Reports Inaccurate**:
  - **Provider Configuration**: Confirm that token costs are correctly specified in `providers.json`.
  - **Updates**: Ensure that PromptArchitect is up to date to benefit from the latest features and fixes.

## Additional Resources

- **OpenTelemetry Integrations**:
  - [OpenTelemetry Ecosystem Integrations](https://opentelemetry.io/ecosystem/integrations/)

- **Azure Monitor Documentation**:
  - [Azure Monitor OpenTelemetry Exporter for Python](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/monitor/azure-monitor-opentelemetry-exporter)
  - [Quickstart: Use Python to connect to Azure Monitor](https://docs.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-python)

- **Using Open-Source Models**:
  - [Ollama Models Documentation](docs/ollama_models.md)

- **Caching in Python**:
  - [functools.lru_cache Documentation](https://docs.python.org/3/library/functools.html#functools.lru_cache)
  - [Implementing a Simple Cache in Python](https://realpython.com/lru-cache-python/)

- **Provider Pricing Documentation**:
  - [OpenAI Pricing](https://openai.com/pricing)
  - [Anthropic Pricing](https://www.anthropic.com/pricing#anthropic-api)

- **Prompt Optimization Techniques**:
  - [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/completion/prompt-design)

- **PromptArchitect Examples**:
  - See `examples/retrieving_cost_and_duration` in the PromptArchitect repository.
