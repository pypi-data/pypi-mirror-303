Wat mij betreft zeggen we vaarwel tegen de tijden waarin groter altijd beter was in de wereld van grote taalmodellen. Terwijl we nog steeds reikhalzend uitkijken naar de volgende generatie van de GPT's, broeit er een stele revolutie die onze aanpak van AI fundamenteel zal veranderen. Hoe? Dat hoor je in deze korte aflevering van AIToday Live. Mijn naam, Joop Snijder, CTO bij Aigency. Sinds november 2022, toen we ChatGPT zagen, hebben we een spectaculaire groei gezien in de ontwikkeling van taalmodellen. Namen als GPT-4, Google Gemini en Anthropic symboliseren een wedloop naar steeds grotere capaciteiten. Eerst waren we al tevreden met een context van 4 tokens, vergelijkbaar met het korte termijngeheugen van een model dat de kern van onze dialoog bijhoudt. Nu zien we een sprong naar 128k, soms wordt er gesproken over 1 miljoen tokens, het wordt maar groter, groter, groter. Parallel aan deze groei is de capaciteit van de modellen zelf ook exponentieel toegenomen. De miljarden parameters waarover we nu beschikken waren ondenkbaar in de eerste dagen van deze technologie. Op social media wordt al reikhalzend uitgekeken naar de onthulling van GPT-5 waarvan de verwachtingen torenhoog zijn. Ik verwacht dat deze trend van groot, groot, groot nog wel even zal aanhouden, maar deze race zal niet eindeloos doorgaan. Waarom denk ik dat? Omdat de wetenschap ons een blik in de toekomst biedt. De focus verschuift namelijk naar het ontwikkelen van kleinere, krachtigere modellen die zowel energiezuiniger als goedkoper zijn. Misschien denk je, dat klinkt tegenstrijdig, maar ik zal het uitleggen waarom dit een logische evolutie is. Namelijk, de noodzaak om operationele kosten te verlagen binnen de wereld van grote taalmodellen wordt steeds urgenter. Deze kosten zijn vaak hoog, moeilijk te voorspellen, wat de onderbouwing van de business cases juist zo kan ondermijnen. Naarmate taalmodellen groter en complexer worden, nemen de bijbehorende kosten evenredig toe. Tenminste, dat zien we tot nu toe. Want het meest geavanceerde model van openAI is momenteel maar liefst 80 keer duurder dan het instapmodel. Het reduceren van deze kosten is dus cruciaal, niet alleen om de technologie toegankelijk te maken, maar ook om de duurzame groei in de toekomst te waarborgen. Vandaag licht ik drie richtingen uit waarbij we zien dat er een beweging komt naar efficiëntie en verkleining. Het eerste is het efficiënte omgaan met tokens. Tokens is het afrekenmodel van alle grote taalmodellen. Hoe meer tokens je gebruikt, zowel wat je in een model stopt als eruit, daar betaal je voor. Zo heeft Microsoft software ontwikkeld waarmee een prompt 5 keer kleiner kan worden gemaakt door irrelevante woorden te verwijderen uit de prompt voordat deze naar het taalmodel wordt gestuurd. Dit verlaagt de kosten van de inputtokens aanzienlijk wat een groot verschil maakt in die operationele kosten. We zien ook steeds meer populariteit rondom RAG-systems. Dat is uiteindelijk ook een vorm waarbij je het helpt om de operationele kosten naar beneden te krijgen, in ieder geval qua tokens. In zo'n rack system combineer je zoeken en taalmodellen met elkaar. Dat systeem zorgt met de documenten die erin zitten voor een soort van lange termijn geheugen dat wordt aangesproken in zo'n dialoog. Wanneer nodig in die dialoog wordt namelijk eerst gezocht in het lange termijn geheugen. Dat zoeken is relatief goedkoop. En alleen de relevante informatie wordt dan met het taalmodel omgezet in een antwoord. Het rekenen aan het antwoord wordt zo goedkoper. De tweede richting die we zien is het kunnen hosten van een taalmodel op je eigen hardware. Daarvoor is al een techniek ontwikkeld en die heet quantization. Moeilijk woord, maar die quantization maakt het mogelijk om modellen in eigen datacenters te draaien. Die quantization is een techniek die is vergelijkbaar met het comprimeren van foto's naar jpeg formaat. Waarbij je een 24 megapixel foto zoveel kleiner maakt dat deze minder ruimte inneemt zonder dat het beeld eronder leidt. Je ziet in principe niet het verschil. Datzelfde kan gedaan worden met een taalmodel. Dat heet dan die quantization. Een taalmodel dat normaal met 32 bits berekeningen rekent. Een moeilijke zin. Die 32 bits voor berekeningen gebruikt kan worden teruggebracht naar 4 bits berekeningen. Klinkt heel technisch, maar betekent dat zo'n taalmodel tot wel 75 tot 85 procent kleiner kan worden gemaakt. Dus een omvang. Maar niet alleen een omvang, maar ook in de rekenkracht die die nodig heeft. Uiteraard met een trade-off tussen precisie en grootte. Maar de wetenschap en zelfs in de praktijk laat het zien dat deze modellen, die gecomprimeerde modellen, nog zeer goed werken. En daarmee biedt deze form, deze gecomprimeerde modellen, unieke mogelijkheden. Omdat ze minder ruimte in beslag nemen en minder rekenkracht nodig hebben, kun je ze bijvoorbeeld hosten op je eigen hardware. Het wordt daarmee zelfs mogelijk om krachtige large language models op mobiele apparaten te draaien. Zo heeft Apple afgelopen maand 8 van dit soort modellen open source beschikbaar gesteld. En komen deze modellen ook in de nieuwste iOS versie. En dan zijn je denken, waarom 8? En waarom 8 modellen? Dat heeft te maken met het volgende laatste punt, namelijk specialisatie met kleinere modellen. Dus door kleinere modellen te trainen combineer je het beste van twee werelden. Zo'n kleine model kan een specifieke taak extreem goed uitvoeren en kan daardoor kleiner blijven terwijl het toch de algemene kennis van een groot taalmodel gebruikt om getraind te worden tot expert. Het uiteindelijke model is dus veel kleiner. En kan veel minder. Het kan veel minder. Dit betekent dat zowel het hosten van het model als het berekenen van de antwoorden veel goed koper worden. Dat zijn de drie die ik eruit wilde lichten. En weet je, ons hoofd zal de aankomende tijd echt op hol worden gebracht door de producenten van grote taalmodellen. Met meer, groter, beter. Maar meer, groter, beter betekent automatisch ook duurder. Dus zal er een tegenkracht komen van het goedkoper en toegankelijker maken van modellen. Dat zal niet lang op zich laten wachten. Dankjewel voor het luisteren naar deze korte aflevering van AIToday. Vergeet je niet te abonneren via je favoriete podcast app en mis geen aflevering. Tot de volgende! [Muziek] [Muziek] [Muziek] [Muziek] 