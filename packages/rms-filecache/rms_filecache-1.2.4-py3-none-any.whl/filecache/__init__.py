"""The purpose of the `filecache` module is to abstract away the location where files
used or generated by a program are stored. Files can be on the local file system, in
Google Cloud Storage, on Amazon Web Services S3, or on a webserver. When files to be read
are on the local file system, they are simply accessed in-place. Otherwise, they are
downloaded from the remote source to a local temporary directory. When files to be written
are on the local file system, they are simply written in-place. Otherwise, they are
written to a local temporary directory and then uploaded to the remote location (it is not
possible to upload to a webserver). When a cache is no longer needed, it is deleted from
the local disk.

The top-level file organization is provided by the :class:`FileCache` class. A
:class:`FileCache` instance is used to specify a particular **sharing policy** and
**lifetime**. For example, a cache could be private to the current process and group a set
of files that all have the same basic purpose. Once these files have been (downloaded and)
read, they are deleted as a group. Another cache could be shared among all processes on
the current machine and group a set of files that are needed by multiple processes, thus
allowing them to be downloaded from a remote source only one time, saving time and
bandwidth.

A :class:`FileCache` can be instantiated either directly or as a context manager. When
instantiated directly, the programmer is responsible for calling
:meth:`FileCache.clean_up` directly to delete the cache when finished. In addition, a
non-shared cache will be deleted on program exit. When instantiated as a context manager,
a non-shared cache is deleted on exit from the context. See the class documentation for
full details.

Usage examples::

    from filecache import FileCache
    with FileCache() as fc:  # Use as context manager
        # Also use open() as a context manager
        with fc.open('gs://rms-filecache-tests/subdir1/subdir2a/binary1.bin', 'rb',
                     anonymous=True) as fp:
            bin1 = fp.read()
        with fc.open('s3://rms-filecache-tests/subdir1/subdir2a/binary1.bin', 'rb',
                     anonymous=True) as fp:
            bin2 = fp.read()
        assert bin1 == bin2
    # Cache automatically deleted here

    fc = FileCache()  # Use without context manager
    # Also retrieve file without using open context manager
    path1 = fc.retrieve('gs://rms-filecache-tests/subdir1/subdir2a/binary1.bin',
                        anonymous=True)
    with open(path1, 'rb') as fp:
        bin1 = fp.read()
    path2 = fc.retrieve('s3://rms-filecache-tests/subdir1/subdir2a/binary1.bin',
                        anonymous=True)
    with open(path2, 'rb') as fp:
        bin2 = fp.read()
    fc.clean_up()  # Cache manually deleted here
    assert bin1 == bin2

    # Write a file to a bucket and read it back
    with FileCache() as fc:
        with fc.open('gs://my-writable-bucket/output.txt', 'w') as fp:
            fp.write('A')
    # The cache will be deleted here so the file will have to be downloaded
    with FileCache() as fc:
        with fc.open('gs://my-writable-bucket/output.txt', 'r') as fp:
            print(fp.read())

A :class:`FileCachePrefix` instance can be used to encapsulate the storage prefix string,
as well as any subdirectories, plus various arguments such as `anonymous` and `time_out`
that can be specified to each `exists`, `retrieve`, or `upload` method. Thus using one
of these instances can simplify the use of a :class:`FileCache` by allowing the user to
only specify the relative part of the path to be operated on, and to not specify various
other parameters at each method call site.

Compare this example to the one above::

    from filecache import FileCache
    with FileCache() as fc:  # Use as context manager
        # Use GS by specifying the bucket name and one directory level
        pfx1 = fc.new_prefix('gs://rms-filecache-tests/subdir1', anonymous=True)
        # Use S3 by specifying the bucket name and two directory levels
        pfx2 = fc.new_prefix('s3://rms-filecache-tests/subdir1/subdir2a', anonymous=True)
        # Access GS using a directory + filename (since only one directory level
        # was specified by the prefix)
        # Also use open() as a context manager
        with pfx1.open('subdir2a/binary1.bin', 'rb') as fp:
            bin1 = fp.read()
        # Access S3 using a filename only (since two directory levels were already
        # specified by the prefix))
        with pfx2.open('binary1.bin', 'rb') as fp:
            bin2 = fp.read()
        assert bin1 == bin2
    # Cache automatically deleted here

A benefit of the abstraction is that different environments can access the same files in
different ways without needing to change the program code. For example, consider a program
that needs to access the file ``COISS_2xxx/COISS_2001/voldesc.cat`` from the NASA PDS
archives. This file might be stored on the local disk in the user's home directory in a
subdirectory called ``pds3-holdings``. Or if the user does not have a local copy, it is
accessible from a webserver at
``https://pds-rings.seti.org/holdings/volumes/COISS_2xxx/COISS_2001/voldesc.cat``.
Finally, it could be accessible from Google Cloud Storage from the ``rms-node-holdings``
bucket at
``gs://rms-node-holdings/pds3-holdings/volumes/COISS_2xxx/COISS_2001/voldesc.cat``. Before
running the program, an environment variable could be set to one of these values::

    $ export PDS3_HOLDINGS_SRC="~/pds3-holdings"
    $ export PDS3_HOLDINGS_SRC="https://pds-rings.seti.org/holdings"
    $ export PDS3_HOLDINGS_SRC="gs://rms-node-holdings/pds3-holdings"

Then the program could be written as::

    from filecache import FileCache
    import os
    with FileCache() as fc:
        pfx = fc.new_prefix(os.getenv('PDS3_HOLDINGS_SRC'))
        with pfx.open('volumes/COISS_2xxx/COISS_2001/voldesc.cat', 'r') as fp:
            contents = fp.read()
    # Cache automatically deleted here

If the program was going to be run multiple times in a row, or multiple copies were going
to be run simultaneously, marking the cache as shared would allow all of the processes to
share the same copy, thus requiring only a single download no matter how many times the
program was run::

    from filecache import FileCache
    import os
    with FileCache(shared=True) as fc:
        pfx = fc.new_prefix(os.getenv('PDS3_HOLDINGS_DIR'))
        with pfx.open('volumes/COISS_2xxx/COISS_2001/voldesc.cat', 'r') as fp:
            contents = fp.read()
    # Cache not deleted here; must be deleted manually using fc.clean_up(final=True)
    # If not deleted manually, the shared cache will persist until the temporary
    # directory is purged by the operating system (which may be never)

Finally, there are four classes that allow direct access to the four possible storage
locations without invoking any caching behavior: :class:`FileCacheSourceLocal`,
:class:`FileCacheSourceHTTP`, :class:`FileCacheSourceGS`, and :class:`FileSourceCacheS3`::

    from filecache import FileCacheSourceGS
    src = FileCacheSourceGS('gs://rms-filecache-tests', anonymous=True)
    src.retrieve('subdir1/subdir2a/binary1.bin', 'local_file.bin')
"""

import atexit
from concurrent import futures
from concurrent.futures import ThreadPoolExecutor
import contextlib
import logging
import os
from pathlib import Path
import requests
import sys
import tempfile
import time
import uuid

import filelock

import boto3
import botocore

from google.cloud import storage as gs_storage
import google.api_core.exceptions

try:
    from ._version import __version__
except ImportError:  # pragma: no cover - only present when building a package
    __version__ = 'Version unspecified'


# Default logger for all FileCache instances that didn't specify one explicitly
_GLOBAL_LOGGER = False


# Global cache of all instantiated FileCacheSource since they may involve opening
# a connection and are not specific to a particular FileCache
_SOURCE_CACHE = {}


def set_global_logger(logger):
    """Set the global logger for all FileCache instances that don't specify one."""
    global _GLOBAL_LOGGER
    logger = logger if logger else False  # Turn None into False
    _GLOBAL_LOGGER = logger


def set_easy_logger():
    """Set a default logger that outputs all messages to stdout."""
    easy_logger = logging.getLogger(__name__)
    easy_logger.setLevel(logging.DEBUG)

    while easy_logger.handlers:
        easy_logger.removeHandler(easy_logger.handlers[0])

    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    easy_logger.addHandler(handler)

    set_global_logger(easy_logger)


def get_global_logger():
    """Return the current global logger."""
    return _GLOBAL_LOGGER


class FileCache:
    """Class which manages the lifecycle of files from various sources."""

    _FILE_CACHE_PREFIX = '.file_cache_'
    _LOCK_PREFIX = '.__lock__'

    def __init__(self, temp_dir=None, shared=False, cache_owner=False, mp_safe=None,
                 atexit_cleanup=True, all_anonymous=False, lock_timeout=60, nthreads=8,
                 logger=None):
        r"""Initialization for the FileCache class.

        When specifying the full path to a file, if the prefix starts with
        ``gs://bucket-name`` it is from Google Storage. If the prefix starts with
        ``s3://bucket-name`` it is from AWS S3. If the prefix starts with ``http://``
        or ``https://`` it is from a website download. Anything else is considered to be
        in the local filesystem.

        Note:
            A shared cache will not be automatically deleted unless the `cache_owner`
            option is specified or :meth:`clean_up` is called with the `final` option. If
            neither of these is done, a shared cache will persist after program exit,
            potentially forever, if the operating system does not purge the temporary
            directory (this is, of course, also the advantage of a shared cache). Thus you
            should be careful with shared caches to prevent unduly filling up your disk.

        Parameters:
            temp_dir (str or Path, optional): The directory in which to cache files.
                In None, the system temporary directory is used, which involves checking
                the environment variables ``TMPDIR``, ``TEMP``, and ``TMP``, and if none
                of those are set then using ``C:\TEMP``, ``C:\TMP``, ``\TEMP``, or
                ``\TMP`` on Windows and ``/tmp``, ``/var/tmp``, or ``/usr/tmp`` on other
                platforms. The file cache will be stored in a sub-directory within this
                temporary directory. The temporary directory must already exist and be
                writeable.
            shared (bool or str, optional): If False, the file cache will be
                stored in a uniquely-named subdirectory of `temp_dir` with the prefix
                ``.file_cache_``. If True, the file cache will be stored in a subdirectory
                of `temp_dir` called ``.file_cache___global__``. If a string is specified,
                the file cache will be stored in a subdirectory of `temp_dir` called
                ``.file_cache_<shared>``. Shared caches will be available to all programs
                creating a :class:`FileCache` with the same name, and may persist after
                program exit and be available to programs running in the future.
            cache_owner (bool, optional): This option is only relevant if `shared` is not
                False. If `cache_owner` is True, this :class:`FileCache` instance is
                considered the owner of the shared cache directory, and the cache will be
                deleted on exit from a context manager, manual call to :meth:`clean_up`,
                or on program exit just as if this were a non-shared cache. This option
                should only be set to True if the thread/process creating this cache has
                control over all other threads/processes that might be accessing it to
                guarantee that the cache is not deleted until all potential users have
                finished executing.
            mp_safe (bool or None, optional): If False, never create new prefixes with
                multiprocessor-safety locking. If True, always create new prefixes with
                multiprocessor-safety locking. If None, safety locking is used if shared
                is not False, as it is assumed that multiple programs will be using the
                shared cache simultaneously.
            atexit_cleanup (bool, optional): If True, at program exit automatically call
                the clean_up method. Note that using this option will keep this class
                instance around for the entire duration of the program, potentially
                wasting memory. To be more memory conscious, but also be solely
                responsible for calling clean_up, set this parameter to False.
            all_anonymous (bool, optional): If True, all accesses to cloud resources will
                be anonymous, and it is not necessary to pass in the `anonymous` option
                separately to each method.
            lock_timeout (int, optional): The default value for lock timeouts for all
                methods; may be overridden by additional arguments. This is how long to
                wait, in seconds, if another process is marked as retrieving the file
                before raising an exception. 0 means to not wait at all. A negative value
                means to never time out.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload.
            logger (logger, optional): If False, do not do any logging. If None, use the
                global logger set with :func:`set_global_logger`. Otherwise use the
                specified logger.

        Notes:
            :class:`FileCache` can be used as a context, such as::

                with FileCache() as fc:
                    ...

            In this case, the cache directory is created on entry to the context and
            deleted on exit. However, if the cache is marked as shared, the directory will
            not be deleted on exit unless the ``cache_owner=True`` option is used.
        """

        # We try very hard here to make sure that no possible passed-in argument for
        # temp_dir or shared could result in a directory name that is anything other
        # than a new cache directory. In particular, since we may be deleting this
        # directory later, we want to make sure it's impossible for a bad actor to inject
        # a directory or filename that could result in the deletion of system or user
        # files. One key aspect of this is we do not allow the user to specify the
        # specific subdirectory name without the unique prefix, and we do not allow
        # the shared directory name to have additional directory components like "..".

        if temp_dir is None:
            temp_dir = tempfile.gettempdir()
        temp_dir = Path(temp_dir).expanduser().resolve()
        if not temp_dir.is_dir():
            raise ValueError(f'{temp_dir} is not a directory')

        self._is_shared = True
        if shared is False:
            sub_dir = Path(f'{self._FILE_CACHE_PREFIX}{uuid.uuid4()}')
            self._is_shared = False
        elif shared is True:
            sub_dir = Path(f'{self._FILE_CACHE_PREFIX}__global__')
        elif isinstance(shared, str):
            if '/' in shared or '\\' in shared:
                raise ValueError(f'shared argument {shared} has directory elements')
            sub_dir = Path(f'{self._FILE_CACHE_PREFIX}{shared}')
        else:
            raise TypeError(f'shared argument {shared} is of improper type')

        self._all_anonymous = all_anonymous
        self._lock_timeout = lock_timeout
        if not isinstance(nthreads, int) or nthreads <= 0:
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')
        self._nthreads = nthreads
        self._logger = logger
        self._is_cache_owner = cache_owner
        self._is_mp_safe = mp_safe if mp_safe is not None else self._is_shared
        self._upload_counter = 0
        self._download_counter = 0
        self._filecacheprefixes = {}

        self._cache_dir = temp_dir / sub_dir
        shared_str = 'shared ' if self._is_shared else ''
        self._log_debug(f'Creating {shared_str}cache {self._cache_dir}')
        self._cache_dir.mkdir(exist_ok=self._is_shared)

        if atexit_cleanup:
            atexit.register(self.clean_up)

    @property
    def cache_dir(self):
        """The top-level directory of the cache as a Path object."""
        return self._cache_dir

    @property
    def download_counter(self):
        """The number of actual file downloads that have taken place."""
        return self._download_counter

    @property
    def upload_counter(self):
        """The number of actual file uploads that have taken place."""
        return self._upload_counter

    @property
    def all_anonymous(self):
        """A bool indicating whether or not to make all cloud accesses anonymous."""
        return self._all_anonymous

    @property
    def lock_timeout(self):
        """The default timeout in seconds while waiting for a file lock."""
        return self._lock_timeout

    @property
    def nthreads(self):
        """The default number of threads to use for multiple-file operations."""
        return self._nthreads

    @property
    def is_shared(self):
        """A bool indicating whether or not this cache is shared."""
        return self._is_shared

    @property
    def is_cache_owner(self):
        """A bool indicating whether or not this FileCache owns its shared cache."""
        return self._is_cache_owner

    @property
    def is_mp_safe(self):
        """A bool indicating whether or not this FileCache is multi-processor safe."""
        return self._is_mp_safe

    def _log_debug(self, msg):
        logger = _GLOBAL_LOGGER if self._logger is None else self._logger
        if logger:
            logger.debug(msg)

    def _log_error(self, msg):
        logger = _GLOBAL_LOGGER if self._logger is None else self._logger
        if logger:
            logger.error(msg)

    def _get_source_and_paths(self, full_path, anonymous):
        if self.all_anonymous:  # Override if all_anonymous was specified
            anonymous = True

        src_str = ''  # Local is the default
        full_path = str(full_path).replace('\\', '/')
        if full_path.startswith(('http://', 'https://', 'gs://', 's3://')):
            # Break 'http://a.b.c.d/e/f' into:
            #   source   ('http://a.b.c.d')
            #   sub_path ('e/g')
            idx = full_path.index('//')
            try:
                idx = full_path.index('/', idx+2)
            except ValueError:  # No /
                raise ValueError(f'Invalid path {full_path}')
            src_str = full_path[:idx]
            sub_path = full_path[idx+1:]
        else:
            # Local
            sub_path = str(Path(full_path).expanduser().resolve())
        if not src_str.startswith(('gs://', 's3://')):
            # No such thing as needing credentials for a local file or HTTP
            # so don't overconstrain the source cache
            anonymous = False

        key = (src_str, anonymous)
        if key not in _SOURCE_CACHE:
            if src_str.startswith(('http://', 'https://')):
                _SOURCE_CACHE[key] = FileCacheSourceHTTP(src_str, anonymous=anonymous)
            elif src_str.startswith('gs://'):
                _SOURCE_CACHE[key] = FileCacheSourceGS(src_str, anonymous=anonymous)
            elif src_str.startswith('s3://'):
                _SOURCE_CACHE[key] = FileCacheSourceS3(src_str, anonymous=anonymous)
            else:
                _SOURCE_CACHE[key] = FileCacheSourceLocal(src_str, anonymous=anonymous)

        source = _SOURCE_CACHE[key]
        if source._src_type == 'local':
            local_path = Path(sub_path).expanduser().resolve()
        else:
            local_path = self._cache_dir / source._cache_subdir / sub_path

        return source, sub_path, local_path

    def _lock_path(self, path):
        return Path(path).parent / f'{self._LOCK_PREFIX}{path.name}'

    def exists(self, full_path, anonymous=False):
        """Check if a file exists without downloading it.

        Parameters:
            full_path (str): The full path of the file (including any source prefix).
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.

        Returns:
            bool: True if the file exists. Note that it is possible that a file could
            exist and still not be downloadable due to permissions. False if the file does
            not exist. This includes bad bucket or webserver names, lack of permission to
            examine a bucket's contents, etc.

        Raises:
            ValueError: If the path is invalidly constructed.
        """

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)
        if source._src_type != 'local' and local_path.is_file():
            # Already in the cache
            return True

        self._log_debug(f'Checking file for existence: {full_path}')

        ret = source.exists(sub_path)

        if ret:
            self._log_debug(f'  File exists: {full_path}')
        else:
            self._log_debug(f'  File does not exist: {full_path}')

        return ret

    def get_local_path(self, full_path, anonymous=False, create_parents=True):
        """Return the local path for the given full_path.

        Parameters:
            full_path (str): The full path of the file (including any source prefix).
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.
            create_parents (bool, optional): If True, create all parent directories. This
                is useful when getting the local path of a file that will be uploaded.

        Returns:
            Path: The Path of the filename in the temporary directory, or the `full_path`
            if the file source is local. The file does not have to exist because this path
            could be used for writing a file to upload. To facilitate this, a side effect
            of this call (if `create_parents` is True) is that the complete parent
            directory structure will be created by this function as necessary.
        """

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)
        if create_parents:
            local_path.parent.mkdir(parents=True, exist_ok=True)

        if source._src_type == 'local':
            self._log_debug(f'Returning local path for {full_path} (local file)')
        else:
            self._log_debug(f'Returning local path for {full_path} as {local_path}')

        return local_path

    def retrieve(self, full_path, anonymous=False, lock_timeout=None, nthreads=None,
                 exception_on_fail=True, **kwargs):
        """Retrieve file(s) from the given location(s) and store in the file cache.

        Parameters:
            full_path (str or list or tuple): The full path of the file, including any
                source prefix. If `full_path` is a list or tuple, all full paths are
                retrieved. This may be more efficient because files can be downloaded in
                parallel. It is OK to retrieve files from multiple sources using one call.
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.
            lock_timeout (int, optional): How long to wait, in seconds, if another process
                is marked as retrieving the file before raising an exception. 0 means to
                not wait at all. A negative value means to never time out. None means to
                use the default value for this :class:`FileCache` instance.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
            exception_on_fail (bool, optional): If True, if any file does not exist or
                download fails a FileNotFound exception is raised, and if any attempt to
                acquire a lock or wait for another process to download a file fails a
                TimeoutError is raised. If False, the function returns normally and any
                failed download is marked with the Exception that caused the failure in
                place of the returned Path.

        Returns:
            Path or Exception or list[Path or Exception]: The Path of the filename in the
            temporary directory (or the original full path if local). If `full_path` was a
            list or tuple of paths, then instead return a list of Paths of the filenames
            in the temporary directory (or the original full path if local). If
            `exception_on_fail` is False, any Path may be an Exception if that file does
            not exist or the download failed or a timeout occurred.

        Raises:
            FileNotFoundError: If a file does not exist or could not be downloaded, and
                exception_on_fail is True.
            TimeoutError: If we could not acquire the lock to allow downloading of a file
                within the given timeout or, for a multi-file download, if we timed out
                waiting for other processes to download locked files, and
                exception_on_fail is True.

        Notes:
            File download is normally an atomic operation; a program will never see a
            partially-downloaded file, and if a download is interrupted there will be no
            file present. However, when downloading multiple files at the same time, as
            many files as possible are downloaded before an exception is raised.
        """

        if lock_timeout is None:
            lock_timeout = self.lock_timeout
        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        # Technically we could just do everything as a locked multi-download, but we
        # separate out the cases for efficiency
        if isinstance(full_path, (list, tuple)):
            if nthreads is None:
                nthreads = self.nthreads
            sources = []
            sub_paths = []
            local_paths = []
            for path in full_path:
                source, sub_path, local_path = self._get_source_and_paths(path, anonymous)
                sources.append(source)
                sub_paths.append(sub_path)
                local_paths.append(local_path)
            if self.is_mp_safe:
                return self._retrieve_multi_locked(sources, sub_paths, local_paths,
                                                   lock_timeout, nthreads,
                                                   exception_on_fail)
            return self._retrieve_multi_unlocked(sources, sub_paths, local_paths,
                                                 nthreads, exception_on_fail)

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)

        if source._src_type == 'local':
            self._log_debug(f'Accessing local file {sub_path}')
            try:
                return source.retrieve(sub_path, local_path)
            except Exception as e:
                if exception_on_fail:
                    raise
                return e

        # If the file actually exists, it's always safe to return it
        if local_path.is_file():
            self._log_debug(f'Accessing cached file for {full_path} at {local_path}')
            return local_path

        if self.is_mp_safe:
            return self._retrieve_locked(source, sub_path, local_path, lock_timeout,
                                         exception_on_fail)
        return self._retrieve_unlocked(source, sub_path, local_path, exception_on_fail)

    def _retrieve_unlocked(self, source, sub_path, local_path, exception_on_fail):
        """Retrieve a single file from the storage location without lock protection."""

        self._log_debug(f'Downloading {source._src_prefix_}{sub_path} into {local_path}')
        try:
            ret = source.retrieve(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise
            return e

        self._download_counter += 1

        return ret

    def _retrieve_locked(self, source, sub_path, local_path, lock_timeout,
                         exception_on_fail):
        """Retrieve a single file from the storage location with lock protection."""

        lock_path = self._lock_path(local_path)
        lock = filelock.FileLock(lock_path, timeout=lock_timeout)
        try:
            lock.acquire()
        except filelock._error.Timeout as e:
            if exception_on_fail:
                raise
            return e
        self._log_debug(
            f'Downloading {source._src_prefix_}{sub_path} into {local_path} with locking')
        try:
            ret = source.retrieve(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise
            return e
        finally:
            # There is a potential race condition here in the case of a raised
            # exception, because after we release the lock but before we delete
            # it, someone else could notice the file isn't downloaded and lock
            # it for another download attempt, and then we would delete someone
            # else's lock (because on Linux locks are only advisory). However,
            # we have to do it in this order because otherwise it won't work on
            # Windows, where locks are not just advisory. However, the worst
            # that could happen is we end up attempting to download the file
            # twice.
            lock.release()
            lock_path.unlink(missing_ok=True)

        self._download_counter += 1

        return ret

    def _retrieve_multi_unlocked(self, sources, sub_paths, local_paths, nthreads,
                                 exception_on_fail):
        """Retrieve multiple files from storage locations without lock protection."""

        # Return Paths (or Exceptions) in the same order as sub_paths
        func_ret = [None] * len(sources)

        files_not_exist = []

        source_dict = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing multi-file retrieval of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            if source._src_type == 'local':
                self._log_debug(f'    Local file   {pfx}{sub_path}')
                try:
                    func_ret[idx] = source.retrieve(sub_path, local_path)
                except Exception as e:
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            if local_path.is_file():
                self._log_debug(f'    Cached file  {pfx}{sub_path} at {local_path}')
                func_ret[idx] = local_path
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))
            self._log_debug(f'    To download {pfx}{sub_path}')

        # Now go through the sources, package up the paths to retrieve, and retrieve
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            source_idxes, _, source_sub_paths, source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'  Performing multi-file download for prefix {source_pfx}:')
            for sub_path in source_sub_paths:
                self._log_debug(f'    {sub_path}')
            rets = source.retrieve_multi(source_sub_paths, source_local_paths,
                                         nthreads=nthreads)
            assert len(source_idxes) == len(rets)
            for ret, sub_path in zip(rets, source_sub_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Download failed: {sub_path} {ret}')
                    files_not_exist.append(f'{source_pfx}{sub_path}')
                else:
                    self._download_counter += 1

            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        if files_not_exist:
            self._log_debug('Multi-file retrieval completed with errors')
            if exception_on_fail:
                exc_str = f"File(s) do not exist: {', '.join(files_not_exist)}"
                raise FileNotFoundError(exc_str)
        else:
            self._log_debug('Multi-file retrieval complete')

        return func_ret

    def _retrieve_multi_locked(self, sources, sub_paths, local_paths, lock_timeout,
                               nthreads, exception_on_fail):
        """Retrieve multiple files from storage locations with lock protection."""

        start_time = time.time()

        # Return Paths (or Exceptions) in the same order as sub_paths
        func_ret = [None] * len(sources)

        files_not_exist = []

        wait_to_appear = []  # Locked by another process (they are downloading it)

        source_dict = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing locked multi-file retrieval of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            # No need to lock for local files
            if source._src_type == 'local':
                self._log_debug(f'    Local file   {pfx}{sub_path}')
                try:
                    func_ret[idx] = source.retrieve(sub_path, local_path)
                except Exception as e:
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            # Since all download operations for individual files are atomic, no need to
            # lock if the file actually exists
            if local_path.is_file():
                self._log_debug(f'    Cached file  {pfx}{sub_path} at {local_path}')
                func_ret[idx] = local_path
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))
            self._log_debug(f'    To download {pfx}{sub_path}')

        # Now go through the sources, package up the paths to retrieve, and retrieve
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            orig_source_idxes, _, orig_source_sub_paths, orig_source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'  Performing locked multi-file download for prefix {source_pfx}:')
            for sub_path in orig_source_sub_paths:
                self._log_debug(f'      {sub_path}')

            # We first loop through the local paths and try to acquire locks on all
            # the files. If we fail to get a lock on a file, it must be downloading
            # somewhere else, so we just remove it from the list of files to download
            # right now and then wait for it to appear later.
            lock_list = []
            source_idxes = []
            source_sub_paths = []
            source_local_paths = []
            for idx, sub_path, local_path in zip(orig_source_idxes,
                                                 orig_source_sub_paths,
                                                 orig_source_local_paths):
                lock_path = self._lock_path(local_path)
                # We don't actually want to wait for a lock to clear, we just want
                # to know if someone else is downloading the file right now
                lock = filelock.FileLock(lock_path, timeout=0)
                try:
                    lock.acquire()
                except filelock._error.Timeout:
                    self._log_debug(f'    Failed to lock: {sub_path}')
                    wait_to_appear.append((idx, f'{source_pfx}{sub_path}', local_path,
                                           lock_path))
                    continue
                lock_list.append((lock_path, lock))
                source_idxes.append(idx)
                source_sub_paths.append(sub_path)
                source_local_paths.append(local_path)

            # Now we can actually download the files that we locked
            rets = source.retrieve_multi(source_sub_paths, source_local_paths,
                                         nthreads=nthreads)
            assert len(source_sub_paths) == len(rets)
            for ret, sub_path in zip(rets, source_sub_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Download failed: {sub_path} {ret}')
                    files_not_exist.append(f'{source_pfx}{sub_path}')
                else:
                    self._log_debug(f'    Successfully downloaded: {sub_path}')
                    self._download_counter += 1

            # Release all the locks
            for lock_path, lock in lock_list:
                # There is a potential race condition here in the case of a raised
                # exception, because after we release the lock but before we delete
                # it, someone else could notice the file isn't downloaded and lock
                # it for another download attempt, and then we would delete someone
                # else's lock (because on Linux locks are only advisory). However,
                # we have to do it in this order because otherwise it won't work on
                # Windows, where locks are not just advisory. However, the worst
                # that could happen is we end up attempting to download the file
                # twice.
                lock.release()
                lock_path.unlink(missing_ok=True)

            # Record the results
            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        # If wait_to_appear is not empty, then we failed to acquire at least one lock,
        # which means that another process was downloading the file. So now we just
        # sit here and wait for all of the missing files to magically show up, or for
        # us to time out. If the lock file disappears but the destination file isn't
        # present, that means the other process failed in its download.
        timed_out = False
        while wait_to_appear:
            new_wait_to_appear = []
            for idx, full_path, local_path, lock_path in wait_to_appear:
                if local_path.is_file():
                    func_ret[idx] = local_path
                    self._log_debug(f'  Downloaded elsewhere: {full_path}')
                    continue
                if not lock_path.is_file():
                    func_ret[idx] = FileNotFoundError(
                        f'Another process failed to download {full_path}')
                    self._log_debug(f'  Download elsewhere failed: {full_path}')
                    continue
                new_wait_to_appear.append((idx, full_path, local_path, lock_path))

            if not new_wait_to_appear:
                break

            wait_to_appear = new_wait_to_appear
            if time.time() - start_time > lock_timeout:
                exc = TimeoutError(
                    'Timeout while waiting for another process to finish downloading')
                self._log_debug(
                    '  Timeout while waiting for another process to finish downloading:')
                for idx, full_path, local_path, lock_path in wait_to_appear:
                    func_ret[idx] = exc
                    self._log_debug(f'    {full_path}')
                if exception_on_fail:
                    raise exc
                timed_out = True
                break
            time.sleep(1)  # Wait 1 second before trying again

        if files_not_exist or timed_out:
            self._log_debug('Multi-file retrieval completed with errors')
            if exception_on_fail and files_not_exist:
                exc_str = f"File(s) do not exist: {', '.join(files_not_exist)}"
                raise FileNotFoundError(exc_str)
        else:
            self._log_debug('Multi-file retrieval complete')

        return func_ret

    def upload(self, full_path, anonymous=False, nthreads=None, exception_on_fail=True):
        """Upload file(s) from the file cache to the storage location(s).

        Parameters:
            full_path (str or list or tuple): The full path of the file, including any
                source prefix. If `full_path` is a list or tuple, the complete list of
                files is uploaded. This may be more efficient because files can be
                uploaded in parallel.
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
            exception_on_fail (bool, optional): If True, if any file does not exist or
                upload fails an exception is raised. If False, the function returns
                normally and any failed upload is marked with the Exception that caused
                the failure in place of the returned path.

        Returns:
            Path or Exception or list[Path or Exception]: The Path of the filename in the
            temporary directory (or the original full path if local). If `full_path` was a
            list or tuple of paths, then instead return a list of Paths of the filenames
            in the temporary directory (or the original full path if local). If
            `exception_on_fail` is False, any Path may be an Exception if that file does
            not exist or the upload failed.

        Raises:
            FileNotFoundError: If a file to upload does not exist or the upload failed,
            and exception_on_fail is True.
        """

        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        if isinstance(full_path, (list, tuple)):
            if nthreads is None:
                nthreads = self.nthreads
            sources = []
            sub_paths = []
            local_paths = []
            for path in full_path:
                source, sub_path, local_path = self._get_source_and_paths(path, anonymous)
                sources.append(source)
                sub_paths.append(sub_path)
                local_paths.append(local_path)
            return self._upload_multi(sources, sub_paths, local_paths, nthreads,
                                      exception_on_fail)

        source, sub_path, local_path = self._get_source_and_paths(full_path, anonymous)

        if source._src_type == 'local':
            self._log_debug(f'Uploading {local_path} (local file)')
        else:
            self._log_debug(f'Uploading {local_path} to {source._src_prefix_}{sub_path}')

        try:
            ret = source.upload(sub_path, local_path)
        except Exception as e:
            if exception_on_fail:
                raise e
            else:
                return e

        self._upload_counter += 1

        return ret

    def _upload_multi(self, sources, sub_paths, local_paths, nthreads, exception_on_fail):
        """Upload multiple files to storage locations."""

        func_ret = [None] * len(sources)

        files_not_exist = []
        files_failed = []

        source_dict = {}

        # First find all the files that are either local or that we have already cached.
        # For other files, create a list of just the files we need to retrieve and
        # organize them by source; we use the source prefix to distinguish among them.
        self._log_debug('Performing multi-file upload of:')
        for idx, (source, sub_path, local_path) in enumerate(zip(sources,
                                                                 sub_paths, local_paths)):
            pfx = source._src_prefix_
            if source._src_type == 'local':
                try:
                    func_ret[idx] = source.upload(sub_path, local_path)
                    self._log_debug(f'  Local file     {pfx}{sub_path}')
                except FileNotFoundError as e:
                    self._log_debug(f'  LOCAL FILE DOES NOT EXIST {pfx}{sub_path}')
                    files_not_exist.append(sub_path)
                    func_ret[idx] = e
                continue
            if not Path(local_path).is_file():
                self._log_debug(f'  LOCAL FILE DOES NOT EXIST {pfx}{sub_path}')
                files_not_exist.append(sub_path)
                func_ret[idx] = FileNotFoundError(
                    f'File does not exist: {pfx}{sub_path}')
                continue
            assert '://' in pfx
            if pfx not in source_dict:
                source_dict[pfx] = []
            source_dict[pfx].append((idx, source, sub_path, local_path))

        # Now go through the sources, package up the paths to upload, and upload
        # them all at once
        for source_pfx in source_dict:
            source = source_dict[source_pfx][0][1]  # All the same
            source_idxes, _, source_sub_paths, source_local_paths = list(
                zip(*source_dict[source_pfx]))
            self._log_debug(
                f'Performing multi-file upload for prefix {source_pfx}:')
            for sub_path in source_sub_paths:
                self._log_debug(f'  {sub_path}')
            rets = source.upload_multi(source_sub_paths, source_local_paths,
                                       nthreads=nthreads)
            assert len(source_idxes) == len(rets)
            for ret, local_path in zip(rets, source_local_paths):
                if isinstance(ret, Exception):
                    self._log_debug(f'    Upload failed: {sub_path} {ret}')
                    files_failed.append(local_path)
                else:
                    self._upload_counter += 1

            for source_ret, source_idx in zip(rets, source_idxes):
                func_ret[source_idx] = source_ret

        if exception_on_fail:
            exc_str = ''
            if files_not_exist:
                exc_str += f"File(s) do not exist: {', '.join(files_not_exist)}"
            if files_failed:
                if exc_str:
                    exc_str += ' AND '
                exc_str += f"Failed to upload file(s): {', '.join(files_failed)}"
            if exc_str:
                raise FileNotFoundError(exc_str)

        return func_ret

    @contextlib.contextmanager
    def open(self, full_path, mode='r', *args,
             anonymous=False, lock_timeout=None, **kwargs):
        """Retrieve+open or open+upload a file as a context manager.

        If `mode` is a read mode (like ``'r'`` or ``'rb'``) then the file will be first
        retrieved by calling :meth:`retrieve` and then opened. If the `mode` is a write
        mode (like ``'w'`` or ``'wb'``) then the file will be first opened for write, and
        when this context manager is exited the file will be uploaded.

        Parameters:
            filename (str or Path): The filename to open. mode (str): The mode string as
                you would specify to Python's `open()` function.
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.
            lock_timeout (int, optional): How long to wait, in seconds, if another process
                is marked as retrieving the file before raising an exception. 0 means to
                not wait at all. A negative value means to never time out. If None, use
                the default value for this :class:`FileCache` instance.

        Returns:
            file-like object: The same object as would be returned by the normal `open()`
            function.
        """

        if mode[0] == 'r':
            local_path = self.retrieve(full_path,
                                       anonymous=anonymous, lock_timeout=lock_timeout)
            with open(local_path, mode, *args, **kwargs) as fp:
                yield fp
        else:  # 'w', 'x', 'a'
            local_path = self.get_local_path(full_path, anonymous=anonymous)
            with open(local_path, mode, *args, **kwargs) as fp:
                yield fp
            self.upload(full_path, anonymous=anonymous)

    def new_prefix(self, prefix, anonymous=False, lock_timeout=None, nthreads=None,
                   **kwargs):
        """Create a new FileCachePrefix with the given prefix.

        Parameters:
            prefix (Path or str): The prefix for the storage location, which may include
                the source prefix was well as any top-level directories. All accesses made
                through this :class:`FileCachePrefix` instance will have this prefix
                prepended to their file path.
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`__init__` `all_anonymous` argument.
            lock_timeout (int, optional): How long to wait, in seconds, if another process
                is marked as retrieving the file before raising an exception. 0 means to
                not wait at all. A negative value means to never time out. None means to
                use the default value for this :class:`FileCache` instance.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use the default value for this
                :class:`FileCache` instance.
        """

        if isinstance(prefix, Path):
            prefix = str(prefix)
        if not isinstance(prefix, str):
            raise TypeError('prefix is not a str or Path')
        if not prefix.startswith(('http://', 'https://', 'gs://', 's3://')):
            prefix = prefix.replace('\\', '/').rstrip('/')
        if lock_timeout is None:
            lock_timeout = self.lock_timeout
        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')
        if nthreads is None:
            nthreads = self.nthreads
        key = (prefix, anonymous, lock_timeout)
        if key not in self._filecacheprefixes:
            self._filecacheprefixes[key] = FileCachePrefix(
                prefix, self, anonymous=anonymous, lock_timeout=lock_timeout,
                nthreads=nthreads, **kwargs)
        return self._filecacheprefixes[key]

    def clean_up(self, final=False):
        """Delete all files stored in the cache including the cache directory.

        Parameters:
            final (bool, optional): If False and this :class:`FileCache` is not marked as
                the `cache_owner` of the cache, a shared cache is left alone. If False and
                this :class:`FileCache` is marked as the `cache_owner` of the cache, or if
                True, a shared cache is deleted. Beware that this could affect other
                processes using the same cache!

        Notes:
            It is permissible to call :meth:`clean_up` more than once. It is also
            permissible to call :meth:`clean_up`, then perform more operations that place
            files in the cache, then call :meth:`clean_up` again.
        """

        self._log_debug(f'Cleaning up cache {self._cache_dir}')

        # Verify this is really a cache directory before walking it and deleting
        # every file. We are just being paranoid to make sure this never does a
        # "rm -rf" on a real directory like "/".
        if not Path(self._cache_dir).name.startswith(self._FILE_CACHE_PREFIX):
            raise ValueError(
                f'Cache directory does not start with {self._FILE_CACHE_PREFIX}')

        if not self._is_shared or (self._is_shared and (final or self.is_cache_owner)):
            # Delete all of the files and subdirectories we left behind, including the
            # file cache directory itself.
            # We would like to use Path.walk() but that was only added in Python 3.12.
            # We allow remove and rmdir to fail with FileNotFoundError because we could
            # have two programs cleaning up a shared cache at the same time fighting
            # each other, or someone could have asked for the local path to a file and
            # then never written anything there.
            for root, dirs, files in os.walk(self._cache_dir, topdown=False):
                for name in files:
                    if name.startswith(self._LOCK_PREFIX):
                        self._log_error(
                            'Cleaning up cache that has an active lock file:'
                            f'{root}/{name}')
                    self._log_debug(f'  Removing file {root}/{name}')
                    try:
                        os.remove(os.path.join(root, name))
                    except FileNotFoundError:  # pragma: no cover - race condition only
                        pass
                for name in dirs:
                    self._log_debug(f'  Removing dir {root}/{name}')
                    try:
                        os.rmdir(os.path.join(root, name))
                    except FileNotFoundError:  # pragma: no cover - race condition only
                        pass

            self._log_debug(f'  Removing dir {self._cache_dir}')
            try:
                os.rmdir(self._cache_dir)
            except FileNotFoundError:  # pragma: no cover - race condition only
                pass
        else:
            self._log_debug(f'  Ignoring shared cache {self._cache_dir}')

    def __enter__(self):
        """Enter the context manager for creating a FileCache."""
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Exit the context manage for a FileCache, executing any clean up needed."""
        self.clean_up()
        # Since the cache is cleaned up, no need to clean up later
        atexit.unregister(self.clean_up)


################################################################################

class FileCacheSource:
    """Superclass for all remote file source classes. Do not use directly.

    The :class:`FileCacheSource` subclasses (:class:`FileCacheSourceLocal`,
    :class:`FileCacheSourceHTTP`, :class:`FileCacheSourceGS`, and
    :class:`FileCacheSourceS3`) provide direct access to local and remote sources,
    bypassing the caching mechanism of :class:`FileCache`.
    """

    def __init__(self, src_prefix=None):
        """Initialization for the FileCacheSource superclass.

        Note:
            Do not instantiate :class:`FileCacheSource` directly. Instead use one of the
            subclasses (:class:`FileCacheSourceLocal`, :class:`FileCacheSourceHTTP`,
            :class:`FileCacheSourceGS`, and :class:`FileCacheSourceS3`).

        Parameters:
            src_prefix (str, optional): The prefix for the source, which can be one of
                ``http://<host>``, ``https://<host>``, ``gs://<bucket>``, or
                ``s3://<bucket>``. For a local source, this parameter should not be
                specified.
        """

        self._src_type = None
        self._src_prefix_ = (src_prefix if src_prefix else '').rstrip('/') + '/'

        # The _cache_subdir attribute is only used by the FileCache class
        self._cache_subdir = None

    def exists(self, sub_path):
        raise NotImplementedError

    def retrieve(self, sub_path, local_path):
        raise NotImplementedError

    def retrieve_multi(self, sub_paths, local_paths, nthreads=8):
        """Retrieve multiple files from the storage location using threads.

        Parameters:
            sub_paths (list or tuple): The path of the files to retrieve relative to the
                source prefix.
            local_paths (list or tuple): The paths to the destinations where the
                downloaded files will be stored.
            nthreads (int, optional): The maximum number of threads to use.

        Returns:
            list[Path or Exception]: A list containing the local paths of the retrieved
            files. If a file failed to download, the entry is the Exception that caused
            the failure. This list is in the same order and has the same length as
            `local_paths`.

        Notes:
            All parent directories in all `local_paths` are created even if a file
            download fails.

            The download of each file is an atomic operation. However, even if some files
            have download failures, all other files will be downloaded.
        """

        if not isinstance(nthreads, int) or nthreads <= 0:
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        results = {}
        for sub_path, result in self._download_object_parallel(sub_paths, local_paths,
                                                               nthreads):
            results[sub_path] = result

        ret = []
        for sub_path in sub_paths:
            ret.append(results[sub_path])

        return ret

    def _download_object(self, sub_path, local_path):
        self.retrieve(sub_path, local_path)
        return local_path

    def _download_object_parallel(self, sub_paths, local_paths, nthreads):
        with ThreadPoolExecutor(max_workers=nthreads) as executor:
            future_to_paths = {executor.submit(self._download_object, x[0], x[1]): x[0]
                               for x in zip(sub_paths, local_paths)}
            for future in futures.as_completed(future_to_paths):
                sub_path = future_to_paths[future]
                exception = future.exception()
                if not exception:
                    yield sub_path, future.result()
                else:
                    yield sub_path, exception

    def upload(self, sub_path, local_path):
        raise NotImplementedError

    def upload_multi(self, sub_paths, local_paths, nthreads=8):
        """Upload multiple files to a storage location.

        Parameters:
            sub_paths (list or tuple): The path of the destination files relative to the
                source prefix.
            local_paths (list or tuple): The paths of the files to upload.
            nthreads (int, optional): The maximum number of threads to use.

        Returns:
            list[Path or Exception]: A list containing the local paths of the uploaded
            files. If a file failed to upload, the entry is the Exception that caused the
            failure. This list is in the same order and has the same length as
            `local_paths`.
        """

        if not isinstance(nthreads, int) or nthreads <= 0:
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        results = {}
        for sub_path, result in self._upload_object_parallel(sub_paths, local_paths,
                                                             nthreads):
            results[sub_path] = result

        ret = []
        for sub_path in sub_paths:
            ret.append(results[sub_path])

        return ret

    def _upload_object(self, sub_path, local_path):
        self.upload(sub_path, local_path)
        return local_path

    def _upload_object_parallel(self, sub_paths, local_paths, nthreads):
        with ThreadPoolExecutor(max_workers=nthreads) as executor:
            future_to_paths = {executor.submit(self._upload_object, x[0], x[1]): x[0]
                               for x in zip(sub_paths, local_paths)}
            for future in futures.as_completed(future_to_paths):
                sub_path = future_to_paths[future]
                exception = future.exception()
                if not exception:
                    yield sub_path, future.result()
                else:
                    yield sub_path, exception


class FileCacheSourceLocal(FileCacheSource):
    """Class that provides direct access to local files.

    This class is unlikely to be directly useful to an external program, as it provides
    essentially no functionality on top of the standard Python filesystem functions.
    """

    def __init__(self, src_prefix=None, anonymous=False, **kwargs):
        """Initialization for the FileCacheLocal class.

        Parameters:
            src_prefix (str, optional): This parameter is only provided to mirror the
                signature of the other source classes. It should not be used.
            anonymous (bool, optional): This parameter is only provided to mirror the
                signature of the other source classes. It should not be used.
        """

        if src_prefix is not None and src_prefix != '':
            raise ValueError(f'Invalid prefix: {src_prefix}')
        super().__init__(**kwargs)

        self._src_type = 'local'
        self._cache_subdir = ''

    def exists(self, sub_path):
        """Check if a file exists without downloading it.

        Parameters:
            sub_path (str): The full path of the local file.

        Returns:
            bool: True if the file exists. Note that it is possible that a file could
            exist and still not be accessible due to permissions.
        """

        return Path(sub_path).is_file()

    def retrieve(self, sub_path, local_path):
        """Retrieve a file from the storage location.

        Parameters:
            sub_path (str or Path): The full path of the local file to retrieve.
            local_path (str or Path): The path to the desination where the file will
                be stored.

        Returns:
            Path: The Path of the filename, which is the same as the `sub_path`
            parameter.

        Raises:
            ValueError: If `sub_path` and `local_path` are not identical.
            FileNotFoundError: If the file does not exist.

        Notes:
            This method essentially does nothing except check for the existence of the
            file.
        """

        local_path = Path(local_path).expanduser().resolve()
        sub_path = Path(sub_path).expanduser().resolve()

        if local_path != sub_path:
            raise ValueError(
                f'Paths differ for local retrieve: {local_path} and {sub_path}')

        if not sub_path.is_file():
            raise FileNotFoundError(f'File does not exist: {sub_path}')

        # We don't actually do anything for local paths since the file is already in the
        # correct location.
        return local_path

    def upload(self, sub_path, local_path):
        """Upload a file from the local filesystem to the storage location.

        Parameters:
            sub_path (str or Path): The full path of the destination.
            local_path (str or Path): The full path of the local file to upload.

        Returns:
            Path: The Path of the filename, which is the same as the `local_path`
            parameter.

        Raises:
            ValueError: If `sub_path` and `local_path` are not identical.
            FileNotFoundError: If the file does not exist.
        """

        local_path = Path(local_path).expanduser().resolve()
        sub_path = Path(sub_path).expanduser().resolve()

        if local_path != sub_path:
            raise ValueError(
                f'Paths differ for local upload: {local_path} and {sub_path}')

        if not local_path.is_file():
            raise FileNotFoundError(f'File does not exist: {local_path}')

        # We don't actually do anything for local paths since the file is already in the
        # correct location.
        return local_path


class FileCacheSourceHTTP(FileCacheSource):
    """Class that provides access to files stored on a webserver."""

    def __init__(self, src_prefix, anonymous=False, **kwargs):
        """Initialization for the FileCacheHTTP class.

        Parameters:
            src_prefix (str): The prefix to all URL accesses, of the form
                ``http://<hostname>`` or ``https://<hostname>``.
            anonymous (bool, optional): This parameter is only provided to mirror the
                signature of the other source classes. It should not be used.
        """

        src_prefix = src_prefix.rstrip('/')
        if (not src_prefix.startswith(('http://', 'https://')) or
                src_prefix.count('/') != 2):
            raise ValueError(f'Invalid prefix: {src_prefix}')

        super().__init__(src_prefix)

        self._prefix_type = 'web'
        self._cache_subdir = (src_prefix
                              .replace('http://', 'http_')
                              .replace('https://', 'http_'))

    def exists(self, sub_path):
        """Check if a file exists without downloading it.

        Parameters:
            sub_path (str): The path of the file on the webserver given by the source
                prefix.

        Returns:
            bool: True if the file (including the webserver) exists. Note that it is
            possible that a file could exist and still not be downloadable due to
            permissions.
        """

        ret = True
        try:
            response = requests.head(f'{self._src_prefix_}{sub_path}')
            response.raise_for_status()
        except requests.exceptions.RequestException:
            ret = False

        return ret

    def retrieve(self, sub_path, local_path):
        """Retrieve a file from the webserver.

        Parameters:
            sub_path (str): The path of the file to retrieve relative to the source
                prefix.
            local_path (str or Path): The path to the destination where the downloaded
                file will be stored.

        Returns:
            Path: The Path where the file was stored (same as `local_path`).

        Raises:
            FileNotFoundError: If the remote file does not exist or the download fails for
                another reason.

        Notes:
            All parent directories in `local_path` are created even if the file download
            fails.

            The download is an atomic operation.
        """

        local_path = Path(local_path).expanduser().resolve()

        url = f'{self._src_prefix_}{sub_path}'

        local_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            raise FileNotFoundError(f'Failed to download file: {url}') from e

        temp_local_path = local_path.with_suffix(f'{local_path.suffix}.{uuid.uuid4()}')
        try:
            with open(temp_local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=1024*1024):
                    f.write(chunk)
            temp_local_path.rename(local_path)
        except Exception:
            temp_local_path.unlink(missing_ok=True)
            raise

        return local_path

    def upload(self, sub_path, local_path):
        """Upload a local file to a webserver. Not implemented."""

        raise NotImplementedError


class FileCacheSourceGS(FileCacheSource):
    """Class that provides access to files stored in Google Storage."""

    def __init__(self, src_prefix, anonymous=False, **kwargs):
        """Initialization for the FileCacheGS class.

        Parameters:
            src_prefix (str): The prefix for all Google Storage accesses, of the form
                ``gs://<bucket>``.
            anonymous (bool, optional): If True, access Google Storage without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment.
        """

        src_prefix = src_prefix.rstrip('/')
        if (not src_prefix.startswith('gs://') or
                src_prefix.count('/') != 2):
            raise ValueError(f'Invalid prefix: {src_prefix}')

        super().__init__(src_prefix)

        self._src_type = 'gs'
        self._client = (gs_storage.Client.create_anonymous_client()
                        if anonymous else gs_storage.Client())
        self._bucket_name = src_prefix.lstrip('gs://')
        self._bucket = self._client.bucket(self._bucket_name)
        self._cache_subdir = src_prefix.replace('gs://', 'gs_')

    def exists(self, sub_path, logger=None):
        """Check if a file exists without downloading it.

        Parameters:
            sub_path (str): The path of the file in the Google Storage bucket given by the
                source prefix.

        Returns:
            bool: True if the file (including the bucket) exists. Note that it is possible
            that a file could exist and still not be downloadable due to permissions.
            False will also be returned if the bucket itself does not exist or is not
            accessible.
        """

        blob = self._bucket.blob(sub_path)
        try:
            return blob.exists()
        except Exception:
            return False

    def retrieve(self, sub_path, local_path):
        """Retrieve a file from a Google Storage bucket.

        Parameters:
            sub_path (str): The path of the file in the Google Storage bucket given by the
                source prefix.
            local_path (str or Path): The path to the destination where the downloaded
                file will be stored.

        Returns:
            Path: The Path where the file was stored (same as `local_path`).

        Raises:
            FileNotFoundError: If the remote file does not exist or the download fails for
                another reason.

        Notes:
            All parent directories in `local_path` are created even if the file download
            fails.

            The download is an atomic operation.
        """

        local_path = Path(local_path).expanduser().resolve()

        local_path.parent.mkdir(parents=True, exist_ok=True)

        blob = self._bucket.blob(sub_path)

        temp_local_path = local_path.with_suffix(f'{local_path.suffix}.{uuid.uuid4()}')
        try:
            blob.download_to_filename(str(temp_local_path))
            temp_local_path.rename(local_path)
        except (google.api_core.exceptions.BadRequest,  # bad bucket name
                google.resumable_media.common.InvalidResponse,  # bad bucket name
                google.cloud.exceptions.NotFound):  # bad filename
            # The google API library will still create the file before noticing
            # that it can't be downloaded, so we have to remove it here
            temp_local_path.unlink(missing_ok=True)
            raise FileNotFoundError(
                f'Failed to download file: {self._src_prefix_}{sub_path}')
        except Exception:  # pragma: no cover
            temp_local_path.unlink(missing_ok=True)
            raise

        return local_path

    def upload(self, sub_path, local_path):
        """Upload a local file to a Google Storage bucket.

        Parameters:
            sub_path (str): The path of the destination file in the Google Storage bucket
                given by the source prefix.
            local_path (str or Path): The full path of the local file to upload.

        Returns:
            Path: The Path of the filename, which is the same as the `local_path`
            parameter.

        Raises:
            FileNotFoundError: If the local file does not exist.
        """

        local_path = Path(local_path).expanduser().resolve()

        if not local_path.exists():
            raise FileNotFoundError(f'File does not exist: {local_path}')

        blob = self._bucket.blob(sub_path)
        blob.upload_from_filename(str(local_path))

        return local_path


class FileCacheSourceS3(FileCacheSource):
    """Class that provides access to files stored in AWS S3."""

    def __init__(self, src_prefix, anonymous=False, **kwargs):
        """Initialization for the FileCacheS3 class.

        Parameters:
            src_prefix (str): The prefix for all AWS S3 accesses, of the form
                ``s3://<bucket>``.
            anonymous (bool, optional): If True, access AWS S3 without specifying
                credentials. Otherwise, credentials must be initialized in the program's
                environment.
        """

        src_prefix = src_prefix.rstrip('/')
        if (not src_prefix.startswith('s3://') or
                src_prefix.count('/') != 2):
            raise ValueError(f'Invalid prefix: {src_prefix}')

        super().__init__(src_prefix)

        self._prefix_type = 's3'
        self._client = (boto3.client('s3',
                                     config=botocore.client.Config(
                                         signature_version=botocore.UNSIGNED))
                        if anonymous else boto3.client('s3'))
        self._bucket_name = src_prefix.lstrip('s3://')
        self._cache_subdir = src_prefix.replace('s3://', 's3_')

    def exists(self, sub_path):
        """Check if a file exists without downloading it.

        Parameters:
            sub_path (str): The path of the file in the AWS S3 bucket given by the
                source prefix.

        Returns:
            bool: True if the file (including the bucket) exists. Note that it is possible
            that a file could exist and still not be downloadable due to permissions.
            False will also be returned if the bucket itself does not exist or is not
            accessible.
        """

        ret = True
        try:
            self._client.head_object(Bucket=self._bucket_name, Key=sub_path)
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == '404':
                ret = False
            else:  # pragma: no cover
                raise

        return ret

    def retrieve(self, sub_path, local_path):
        """Retrieve a file from an AWS S3 bucket.

        Parameters:
            sub_path (str): The path of the file in the AWS S3 bucket given by the
                source prefix.
            local_path (str or Path): The path to the destination where the downloaded
                file will be stored.

        Returns:
            Path: The Path where the file was stored (same as `local_path`).

        Raises:
            FileNotFoundError: If the remote file does not exist or the download fails for
                another reason.

        Notes:
            All parent directories in `local_path` are created even if the file download
            fails.

            The download is an atomic operation.
        """

        local_path = Path(local_path).expanduser().resolve()

        local_path.parent.mkdir(parents=True, exist_ok=True)

        temp_local_path = local_path.with_suffix(f'{local_path.suffix}.{uuid.uuid4()}')
        try:
            self._client.download_file(self._bucket_name, sub_path,
                                       str(temp_local_path))
            temp_local_path.rename(local_path)
        except botocore.exceptions.ClientError:
            temp_local_path.unlink(missing_ok=True)
            raise FileNotFoundError(
                f'Failed to download file: {self._src_prefix_}{sub_path}')
        except Exception:  # pragma: no cover
            temp_local_path.unlink(missing_ok=True)
            raise

        return local_path

    def upload(self, sub_path, local_path):
        """Upload a local file to an AWS S3 bucket.

        Parameters:
            sub_path (str): The path of the destination file in the AWS S3 bucket
                given by the source prefix.
            local_path (str or Path): The full path of the local file to upload.

        Returns:
            Path: The Path of the filename, which is the same as the `local_path`
            parameter.

        Raises:
            FileNotFoundError: If the local file does not exist.
        """

        local_path = Path(local_path).expanduser().resolve()

        self._client.upload_file(str(local_path), self._bucket_name, sub_path)

        return local_path


class FileCachePrefix:
    """Class for interfacing to a FileCache using a path prefix.

    This class provides a simpler way to abstract away remote access in a FileCache by
    collecting common parameters (`anonymous`, `lock_timeout`) and a more complete prefix
    (not just the bucket name or URL, but the first part of the access path as well) into
    a single location.
    """

    def __init__(self, prefix, filecache, anonymous=False, lock_timeout=None,
                 nthreads=None):
        """Initialization for the FileCachePrefix class.

        Parameters:
            prefix (str or Path): The prefix for the storage location. If the prefix
                starts with ``gs://bucket-name`` it is from Google Storage. If the prefix
                starts with ``s3://bucket-name`` it is from AWS S3. If the prefix starts
                with ``http://`` or ``https://`` it is from a website download. Anything
                else is considered to be in the local filesystem and can be a str or Path
                object.
            file_cache (FileCache): The :class:`FileCache` in which to store files
                retrieved from this prefix.
            anonymous (bool, optional): If True, access cloud resources (GS and S3)
                without specifying credentials. Otherwise, credentials must be initialized
                in the program's environment. This parameter can be overridden by the
                :meth:`FileCache.__init__` `all_anonymous` argument.
            lock_timeout (int, optional): How long to wait, in seconds, if another process
                is marked as retrieving the file before raising an exception. 0 means to
                not wait at all. A negative value means to never time out. None means to
                use the default value for the associated :class:`FileCache` instance.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use the default value for the
                associated :class:`FileCache` instance.

        Notes:
            Within a given :class:`FileCache`, :class:`FileCachePrefix` instances that
            reference the same local/remote source will be stored in the same location on
            the local disk. Files downloaded into one instance will thus be visible in the
            other instance.

            Any logging will be made to the `file_cache`'s logger.
        """

        self._filecache = filecache
        self._anonymous = anonymous
        self._lock_timeout = lock_timeout
        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')
        self._nthreads = nthreads
        self._upload_counter = 0
        self._download_counter = 0

        if not isinstance(prefix, (str, Path)):
            raise TypeError('prefix is not a str or Path')

        self._prefix_ = str(prefix).replace('\\', '/').rstrip('/') + '/'

        self._filecache._log_debug(f'Initializing prefix {self._prefix_}')

    def exists(self, sub_path):
        """Check if a file exists without downloading it.

        Parameters:
            sub_path (str): The path of the file relative to the prefix.

        Returns:
            bool: True if the file exists. Note that it is possible that a file could
            exist and still not be downloadable due to permissions. False if the file does
            not exist. This includes bad bucket or webserver names, lack of permission to
            examine a bucket's contents, etc.

        Raises:
            ValueError: If the path is invalidly constructed.
        """

        return self._filecache.exists(f'{self._prefix_}{sub_path}',
                                      anonymous=self._anonymous)

    def get_local_path(self, sub_path, create_parents=True):
        """Return the local path for the given sub_path relative to the prefix.

        Parameters:
            sub_path (str): The path of the file relative to the prefix.
            create_parents (bool, optional): If True, create all parent directories. This
                is useful when getting the local path of a file that will be uploaded.

        Returns:
            Path: The Path of the filename in the temporary directory, or the `full_path`
            if the file source is local. The file does not have to exist because this path
            could be used for writing a file to upload. To facilitate this, a side effect
            of this call (if `create_parents` is True) is that the complete parent
            directory structure will be created by this function as necessary.
        """

        return self._filecache.get_local_path(f'{self._prefix_}{sub_path}',
                                              anonymous=self._anonymous,
                                              create_parents=create_parents)

    def retrieve(self, sub_path, nthreads=None, exception_on_fail=True):
        """Retrieve a file(s) from the given sub_path and store it in the file cache.

        Parameters:
            sub_path (str or list or tuple): The path of the file relative to the prefix.
                If `sub_path` is a list or tuple, the complete list of files is retrieved.
                Depending on the storage location, this may be more efficient because
                files can be downloaded in parallel.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use
                the default value given when this :class:`FileCachePrefix` was created.
            exception_on_fail (bool, optional): If True, if any file does not exist or
                download fails a FileNotFound exception is raised, and if any attempt to
                acquire a lock or wait for another process to download a file fails a
                TimeoutError is raised. If False, the function returns normally and any
                failed download is marked with the Exception that caused the failure in
                place of the returned Path.

        Returns:
            Path or Exception or list[Path or Exception]: The Path of the filename in the
            temporary directory (or the original full path if local). If `full_path` was a
            list or tuple of paths, then instead return a list of Paths of the filenames
            in the temporary directory (or the original full path if local). If
            `exception_on_fail` is False, any Path may be an Exception if that file does
            not exist or the download failed or a timeout occurred.

        Raises:
            FileNotFoundError: If a file does not exist or could not be downloaded, and
                exception_on_fail is True.
            TimeoutError: If we could not acquire the lock to allow downloading of a file
                within the given timeout or, for a multi-file download, if we timed out
                waiting for other processes to download locked files, and
                exception_on_fail is True.

        Notes:
            File download is normally an atomic operation; a program will never see a
            partially-downloaded file, and if a download is interrupted there will be no
            file present. However, when downloading multiple files at the same time, as
            many files as possible are downloaded before an exception is raised.
        """

        old_download_counter = self._filecache.download_counter

        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        if nthreads is None:
            nthreads = self._nthreads

        try:
            if isinstance(sub_path, (list, tuple)):
                new_sub_path = [f'{self._prefix_}{p}' for p in sub_path]
                ret = self._filecache.retrieve(new_sub_path,
                                               anonymous=self._anonymous,
                                               lock_timeout=self._lock_timeout,
                                               nthreads=nthreads,
                                               exception_on_fail=exception_on_fail)
            else:
                ret = self._filecache.retrieve(f'{self._prefix_}{sub_path}',
                                               anonymous=self._anonymous,
                                               lock_timeout=self._lock_timeout,
                                               exception_on_fail=exception_on_fail)
        finally:
            self._download_counter += (self._filecache.download_counter -
                                       old_download_counter)

        return ret

    def upload(self, sub_path, nthreads=None, exception_on_fail=True):
        """Upload file(s) from the file cache to the storage location(s).

        Parameters:
            sub_path (str or list or tuple): The path of the file relative to the prefix.
                If `sub_path` is a list or tuple, the complete list of files is uploaded.
                This may be more efficient because files can be uploaded in parallel.
            nthreads (int, optional): The maximum number of threads to use when doing
                multiple-file retrieval or upload. If None, use
                the default value given when this :class:`FileCachePrefix` was created.
            exception_on_fail (bool, optional): If True, if any file does not exist or
                upload fails an exception is raised. If False, the function returns
                normally and any failed upload is marked with the Exception that caused
                the failure in place of the returned path.

        Returns:
            Path or Exception or list[Path or Exception]: The Path of the filename in the
            temporary directory (or the original full path if local). If `full_path` was a
            list or tuple of paths, then instead return a list of Paths of the filenames
            in the temporary directory (or the original full path if local). If
            `exception_on_fail` is False, any Path may be an Exception if that file does
            not exist or the upload failed.

        Raises:
            FileNotFoundError: If a file to upload does not exist or the upload failed,
            and exception_on_fail is True.
        """

        old_upload_counter = self._filecache.upload_counter

        if nthreads is not None and (not isinstance(nthreads, int) or nthreads <= 0):
            raise ValueError(f'nthreads must be a positive integer, got {nthreads}')

        if nthreads is None:
            nthreads = self._nthreads

        try:
            if isinstance(sub_path, (list, tuple)):
                new_sub_paths = [f'{self._prefix_}{p}' for p in sub_path]
                ret = self._filecache.upload(new_sub_paths,
                                             anonymous=self._anonymous,
                                             nthreads=nthreads,
                                             exception_on_fail=exception_on_fail)
            else:
                ret = self._filecache.upload(f'{self._prefix_}{sub_path}',
                                             anonymous=self._anonymous,
                                             exception_on_fail=exception_on_fail)
        finally:
            self._upload_counter += (self._filecache.upload_counter -
                                     old_upload_counter)

        return ret

    @contextlib.contextmanager
    def open(self, sub_path, mode='r', *args, **kwargs):
        """Retrieve+open or open+upload a file as a context manager.

        If `mode` is a read mode (like ``'r'`` or ``'rb'``) then the file will be first
        retrieved by calling :meth:`retrieve` and then opened. If the `mode` is a write
        mode (like ``'w'`` or ``'wb'``) then the file will be first opened for write, and
        when this context manager is exited the file will be uploaded.

        Parameters:
            sub_path (str): The path of the file relative to the prefix.
            mode (str): The mode string as you would specify to Python's `open()`
                function.

        Returns:
            file-like object: The same object as would be returned by the normal `open()`
            function.
        """

        if mode[0] == 'r':
            local_path = self.retrieve(sub_path)
            with open(local_path, mode, *args, **kwargs) as fp:
                yield fp
        else:  # 'w', 'x', 'a'
            local_path = self.get_local_path(sub_path)
            with open(local_path, mode, *args, **kwargs) as fp:
                yield fp
            self.upload(sub_path)

    @property
    def download_counter(self):
        """The number of actual file downloads that have taken place."""
        return self._download_counter

    @property
    def upload_counter(self):
        """The number of actual file uploads that have taken place."""
        return self._upload_counter

    @property
    def prefix(self):
        """The URI prefix including a trailing slash."""
        return self._prefix_

    @property
    def is_local(self):
        """A bool indicating whether or not the prefix refers to the local filesystem."""
        return not self._prefix_.startswith(('http://', 'https://', 'gs://', 's3://'))
