{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5189fbc1",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eeb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm, tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from mqt.predictor import ml\n",
    "\n",
    "predictor = ml.Predictor()\n",
    "figure_of_merit = \"expected_fidelity\"\n",
    "\n",
    "training_data = predictor.get_prepared_training_data(figure_of_merit=figure_of_merit, save_non_zero_indices=True)\n",
    "\n",
    "X_train = training_data.X_train\n",
    "X_test = training_data.X_test\n",
    "y_train = training_data.y_train\n",
    "y_test = training_data.y_test\n",
    "indices_train = training_data.indices_train\n",
    "indices_test = training_data.indices_test\n",
    "names_list = training_data.names_list\n",
    "scores_list = training_data.scores_list\n",
    "\n",
    "scores_filtered = [scores_list[i] for i in indices_test]\n",
    "names_filtered = [names_list[i] for i in indices_test]\n",
    "\n",
    "performance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = \"#21918c\"\n",
    "color2 = \"#440154\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddba2b-afda-4ae3-89d2-0c54b6056756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0932e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453a219",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "tree_param = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"max_depth\": list(range(8, 30, 6)),\n",
    "        \"min_samples_split\": list(range(2, 20, 6)),\n",
    "        \"min_samples_leaf\": list(range(2, 20, 6)),\n",
    "        \"bootstrap\": [True, False],\n",
    "    },\n",
    "]\n",
    "clf = GridSearchCV(clf, tree_param, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"RandomForestClassifier\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Feature Importance: \", clf.best_estimator_.feature_importances_)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Random Forest\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.generate_eval_all_datapoints(\n",
    "    names_filtered, scores_filtered, y_pred, y_test, color_all=color1, color_pred=color2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89553ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab679fa",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ml.helper.get_path_trained_model(figure_of_merit, return_non_zero_indices=True)\n",
    "non_zero_indices = np.load(str(path), allow_pickle=True)\n",
    "\n",
    "openqasm_qc_list = ml.helper.get_openqasm_gates()\n",
    "feature_names = [openqasm_qc_list[i] for i in range(len(openqasm_qc_list))]\n",
    "feature_names.extend((\"num qubits\", \"depth\", \"prog. comm.\", \"crit. dept\", \"entang. ratio\", \"parallelism\", \"liveness\"))\n",
    "feature_names = [feature_names[i] for i in non_zero_indices]\n",
    "\n",
    "importances = clf.best_estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.best_estimator_.estimators_], axis=0)\n",
    "\n",
    "idx = np.argsort(-importances)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(np.array(feature_names)[idx], np.array(importances)[idx], color=color1, width=0.9)\n",
    "plt.errorbar(\n",
    "    np.array(feature_names)[idx],\n",
    "    np.array(importances)[idx],\n",
    "    np.array(std)[idx],\n",
    "    fmt=\"o\",\n",
    "    color=color2,\n",
    ")\n",
    "plt.xticks(rotation=90, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.ylabel(\"Relative feature importance\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/feature_importances.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd88baf",
   "metadata": {},
   "source": [
    "#### Check the relative importances per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b77209",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = zip(np.array(feature_names)[idx], np.array(importances)[idx], strict=False)\n",
    "for feature, importance in list(summary):\n",
    "    print(feature, np.round(importance, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4253d80e",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aca3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"GradientBoostingClassifier\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Gradient Boosting\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9aacd",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f88a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "tree_param = [\n",
    "    {\n",
    "        \"criterion\": [\"entropy\", \"gini\"],\n",
    "        \"max_depth\": list(range(1, 15, 1)),\n",
    "        \"min_samples_split\": list(range(2, 20, 4)),\n",
    "        \"min_samples_leaf\": list(range(2, 20, 4)),\n",
    "        \"max_leaf_nodes\": list(range(2, 200, 40)),\n",
    "        \"max_features\": list(range(1, len(non_zero_indices), 10)),\n",
    "    },\n",
    "]\n",
    "clf = GridSearchCV(clf, tree_param, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"DecisionTreeClassifier\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "print(\"Feature Importance: \", clf.best_estimator_.feature_importances_)\n",
    "performance.append((\"Decision Tree\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b7f2a",
   "metadata": {},
   "source": [
    "# Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "param_grid = {\"n_neighbors\": range(1, 10, 1)}\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"KNeighborsClassifier\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Nearest Neighbor\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a215cbd",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "    \"solver\": [\"sgd\", \"adam\"],\n",
    "    \"alpha\": [0.0001, 0.05],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"MLPClassifier\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Multilayer Perceptron\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e7f1a",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb259b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [1, 0.1, 0.01], \"kernel\": [\"rbf\", \"sigmoid\"]}\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"SVM\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Support Vector Machine\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbac0a1",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04146e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "param_grid = {\"var_smoothing\": np.logspace(0, -9, num=100)}\n",
    "clf = GridSearchCV(clf, param_grid, cv=5, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.array(list(clf.predict(X_test)))\n",
    "res, rel_scores = predictor.calc_performance_measures(scores_filtered, y_pred, y_test)\n",
    "predictor.generate_eval_histogram(res, filename=\"GaussianNB\", color=color1)\n",
    "rel_goodness = np.round(np.mean(rel_scores), 4)\n",
    "rel_goodness_std = np.round(np.std(rel_scores), 4)\n",
    "\n",
    "print(\"Best Accuracy: \", clf.best_score_)\n",
    "top3 = (res.count(1) + res.count(2) + res.count(3)) / len(res)\n",
    "print(\"Top 3: \", top3)\n",
    "print(\"Rel Goodness: \", rel_goodness)\n",
    "print(\"Rel Goodness Std: \", rel_goodness_std)\n",
    "performance.append((\"Naive Bayes\", clf.best_score_, top3, max(res), rel_goodness, rel_goodness_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea3c3",
   "metadata": {},
   "source": [
    "# Save Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e522a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance)\n",
    "from pathlib import Path\n",
    "\n",
    "file = Path(\"results/performances_\" + figure_of_merit + \".csv\")\n",
    "with file.open(\"w\") as f:\n",
    "    f.write(\"Classifier, Accuracy, Top3, Worst Rank, Eval. Score Diff., Std\\n\")\n",
    "    for sublist in performance:\n",
    "        line = f\"{sublist[0]}, {sublist[1]}, {sublist[2]}, {sublist[3]}, {sublist[4]}, {sublist[5]} \\n\"\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
