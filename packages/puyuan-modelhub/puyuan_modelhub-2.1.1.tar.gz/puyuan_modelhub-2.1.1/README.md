# Description

[PyPI](https://pypi.org/project/puyuan-modelhub/#description), [Github](https://github.com/puyuantech/modelhub)

ğŸ“¦ ModelhubClient: A Python client for the Modelhub. Support various models including LLMs, embedding models, audio models and multi-modal models. These models are implemented by either 3rdparty APIs or self-host instances.

# Installation

```shell
pip install puyuan_modelhub --user
```

# Current Supported Models

| name                         | pricing                     | context_window | is_local | description                                                                                                                                                                                                                                                             |
| :--------------------------- | :-------------------------- | -------------: | :------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Baichuan4                    | INPUT: 100Â¥, OUTPUT: 100Â¥   |          32768 | False    | æ¨¡å‹è°ƒç”¨ Baichuan4                                                                                                                                                                                                                                                      |
| Baichuan3-Turbo              | INPUT: 12Â¥, OUTPUT: 12Â¥     |          32768 | False    | æ¨¡å‹è°ƒç”¨ Baichuan3-Turbo                                                                                                                                                                                                                                                |
| Baichuan3-Turbo-128k         | INPUT: 24Â¥, OUTPUT: 24Â¥     |         128000 | False    | æ¨¡å‹è°ƒç”¨ Baichuan3-Turbo-128k                                                                                                                                                                                                                                           |
| Baichuan2-Turbo              | INPUT: 8Â¥, OUTPUT: 8Â¥       |          32768 | False    | æ¨¡å‹è°ƒç”¨ Baichuan2-Turbo                                                                                                                                                                                                                                                |
| Baichuan2-Turbo-192k         | INPUT: 16Â¥, OUTPUT: 16Â¥     |         192000 | False    | æ¨¡å‹è°ƒç”¨ Baichuan2-Turbo-192k                                                                                                                                                                                                                                           |
| Baichuan2-53B                | INPUT: 20Â¥, OUTPUT: 20Â¥     |          32768 | False    | æ¨¡å‹è°ƒç”¨ Baichuan2-53B                                                                                                                                                                                                                                                  |
| chatglm-66b                  | free                        |           8192 | False    | ChatGLM-66B æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚                                                                                                                                                                                                     |
| glm-4-0520                   | INPUT: 100Â¥, OUTPUT: 100Â¥   |         128000 | False    | æˆ‘ä»¬å½“å‰çš„æœ€å…ˆè¿›æœ€æ™ºèƒ½çš„æ¨¡å‹ï¼ŒæŒ‡ä»¤éµä»èƒ½åŠ›å¤§å¹…æå‡18.6%ï¼Œå‘å¸ƒäº20240605ã€‚                                                                                                                                                                                               |
| glm-4                        | INPUT: 100Â¥, OUTPUT: 100Â¥   |         128000 | False    | æœ€æ–°çš„ GLM-4 ã€æœ€å¤§æ”¯æŒ 128k ä¸Šä¸‹æ–‡ã€æ”¯æŒ Function Call ã€Retreivalã€‚                                                                                                                                                                                                   |
| glm-4-air                    | INPUT: 1Â¥, OUTPUT: 1Â¥       |         128000 | False    | æ€§ä»·æ¯”æœ€é«˜çš„ç‰ˆæœ¬ï¼Œç»¼åˆæ€§èƒ½æ¥è¿‘GLM-4ï¼Œé€Ÿåº¦å¿«ï¼Œä»·æ ¼å®æƒ ã€‚                                                                                                                                                                                                                 |
| glm-4-airx                   | INPUT: 10Â¥, OUTPUT: 10Â¥     |         128000 | False    | GLM-4-Air çš„é«˜æ€§èƒ½ç‰ˆæœ¬ï¼Œæ•ˆæœä¸å˜ï¼Œæ¨ç†é€Ÿåº¦è¾¾åˆ°å…¶2.6å€ã€‚                                                                                                                                                                                                                 |
| glm-4-flash                  | INPUT: 0.1Â¥, OUTPUT: 0.1Â¥   |         128000 | False    | é€‚ç”¨ç®€å•ä»»åŠ¡ï¼Œé€Ÿåº¦æœ€å¿«ï¼Œä»·æ ¼æœ€å®æƒ çš„ç‰ˆæœ¬ã€‚                                                                                                                                                                                                                              |
| glm-3-turbo                  | INPUT: 1Â¥, OUTPUT: 1Â¥       |         128000 | False    | æœ€æ–°çš„glm-3-turboã€æœ€å¤§æ”¯æŒ 128kä¸Šä¸‹æ–‡ã€æ”¯æŒFunction Callã€Retreivalã€‚                                                                                                                                                                                                  |
| zhipu-embedding-2            | INPUT: 0.5Â¥, OUTPUT: 0.5Â¥   |            nan | False    | Embeddingæ˜¯å°†è¾“å…¥çš„æ–‡æœ¬ä¿¡æ¯è¿›è¡Œå‘é‡åŒ–è¡¨ç¤ºã€‚Embeddingé€‚ç”¨äºæœç´¢ã€èšç±»ã€æ¨èã€å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ä»»åŠ¡ç­‰ä»»åŠ¡ã€‚                                                                                                                                                                  |
| deepseek-chat-v2             | INPUT: 1Â¥, OUTPUT: 2Â¥       |          32768 | False    | æ“…é•¿é€šç”¨å¯¹è¯ä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º 32Kã€‚DeepSeek-V2 å¼€æºç‰ˆæœ¬æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼ŒAPI/ç½‘é¡µç‰ˆæœ¬æ”¯æŒ 32K ä¸Šä¸‹æ–‡ã€‚                                                                                                                                                                 |
| deepseek-coder               | INPUT: 1Â¥, OUTPUT: 2Â¥       |          16384 | False    | æ“…é•¿å¤„ç†ç¼–ç¨‹ä»»åŠ¡ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º 16Kã€‚                                                                                                                                                                                                                                    |
| gemini-pro                   | free                        |            nan | False    |                                                                                                                                                                                                                                                                         |
| abab6.5-chat                 | INPUT: 30Â¥, OUTPUT: 30Â¥     |           8192 | False    |                                                                                                                                                                                                                                                                         |
| abab6.5s-chat                | INPUT: 10Â¥, OUTPUT: 10Â¥     |         245760 | False    |                                                                                                                                                                                                                                                                         |
| abab6.5g-chat                | INPUT: 5Â¥, OUTPUT: 5Â¥       |           8192 | False    |                                                                                                                                                                                                                                                                         |
| abab6-chat                   | INPUT: 100Â¥, OUTPUT: 100Â¥   |          32768 | False    |                                                                                                                                                                                                                                                                         |
| abab5.5-chat                 | INPUT: 15Â¥, OUTPUT: 15Â¥     |          16384 | False    |                                                                                                                                                                                                                                                                         |
| moonshot-v1-8k               | INPUT: 12Â¥, OUTPUT: 12Â¥     |           8192 | False    | æ¨¡å‹è°ƒç”¨ moonshot-v1-8k                                                                                                                                                                                                                                                 |
| moonshot-v1-32k              | INPUT: 24Â¥, OUTPUT: 24Â¥     |          32768 | False    | æ¨¡å‹è°ƒç”¨ moonshot-v1-32k                                                                                                                                                                                                                                                |
| moonshot-v1-128k             | INPUT: 60Â¥, OUTPUT: 60Â¥     |         128000 | False    | æ¨¡å‹è°ƒç”¨ moonshot-v1-128k                                                                                                                                                                                                                                               |
| gpt-4o                       | INPUT: 5$, OUTPUT: 15$      |         128000 | False    | Our most advanced, multimodal flagship model thatâ€™s cheaper and faster than GPT-4 Turbo. Currently points to gpt-4o-2024-05-13.                                                                                                                                         |
| gpt-4o-2024-05-13            | INPUT: 5$, OUTPUT: 15$      |         128000 | False    | gpt-4o currently points to this version.                                                                                                                                                                                                                                |
| gpt-4-turbo                  | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 Turbo with Vision. The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling. Currently points to gpt-4-turbo-2024-04-09.                                                                                 |
| gpt-4-turbo-2024-04-09       | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 Turbo with Vision model. Vision requests can now use JSON mode and function calling. gpt-4-turbo currently points to this version.                                                                                                                                |
| gpt-4-turbo-preview          | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 Turbo preview model. Currently points to gpt-4-0125-preview.                                                                                                                                                                                                      |
| gpt-4-0125-preview           | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 Turbo preview model intended to reduce cases of â€œlazinessâ€ where the model doesnâ€™t complete a task. Returns a maximum of 4,096 output tokens. Learn more.                                                                                                         |
| gpt-4-1106-preview           | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 Turbo preview model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. This is a preview model. Learn more.                                                |
| gpt-4-vision-preview         | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 model with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. This is a preview model, we recommend developers to now use gpt-4-turbo which includes vision capabilities. Currently points to gpt-4-1106-vision-preview.        |
| gpt-4-1106-vision-preview    | INPUT: 10$, OUTPUT: 30$     |         128000 | False    | GPT-4 model with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. This is a preview model, we recommend developers to now use gpt-4-turbo which includes vision capabilities. Returns a maximum of 4,096 output tokens. Learn more. |
| gpt-4                        | INPUT: 30$, OUTPUT: 60$     |           8192 | False    | Currently points to gpt-4-0613. See continuous model upgrades.                                                                                                                                                                                                          |
| gpt-4-0613                   | INPUT: 30$, OUTPUT: 60$     |           8192 | False    | Snapshot of gpt-4 from June 13th 2023 with improved function calling support.                                                                                                                                                                                           |
| gpt-3.5-turbo-0125           | INPUT: 0.5$, OUTPUT: 1.5$   |          16385 | False    | The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls. Returns a maximum of 4,096 output tokens. Learn more.                            |
| gpt-3.5-turbo                | INPUT: 0.5$, OUTPUT: 1.5$   |          16385 | False    | Currently points to gpt-3.5-turbo-0125.                                                                                                                                                                                                                                 |
| gpt-3.5-turbo-1106           | INPUT: 0.5$, OUTPUT: 1.5$   |          16385 | False    | GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.                                                                                    |
| text-embedding-3-small       | INPUT: 0.02$, OUTPUT: 0.02$ |            nan | False    | Embedding V3 small. Increased performance over 2nd generation ada embedding model                                                                                                                                                                                       |
| text-embedding-3-large       | INPUT: 0.13$, OUTPUT: 0.13$ |            nan | False    | Embedding V3 large. Most capable embedding model for both english and non-english tasks                                                                                                                                                                                 |
| text-embedding-ada-002       | INPUT: 0.1$, OUTPUT: 0.1$   |            nan | False    | Most capable 2nd generation embedding model, replacing 16 first generation models                                                                                                                                                                                       |
| whisper-v3                   | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| funasr                       | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| baichuan2-7b                 | free                        |           8192 | True     | Baichuan 2 æ˜¯ç™¾å·æ™ºèƒ½æ¨å‡ºçš„æ–°ä¸€ä»£å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿ Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒã€‚                                                                                                                                                                                |
| chatglm3                     | free                        |           8192 | True     | ChatGLM3 æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚                                                                                                                                                                                                        |
| chatglm3-32k                 | free                        |          32768 | True     | ChatGLM3 æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚                                                                                                                                                                                                        |
| deepseek-coder-6.7b-instruct | free                        |          16384 | True     | Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.                                                                            |
| deepseek-coder-6.7b-base     | free                        |          16384 | True     | Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.                                                                            |
| m3e                          | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| m3e-large                    | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| text2vec-large-chinese       | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| gte-large-zh                 | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| stella-large-zh-v2           | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| jina-embeddings-v2-base-zh   | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| bge-m3                       | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| multilingual-e5-large        | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| lingua                       | free                        |           4096 | True     |                                                                                                                                                                                                                                                                         |
| oneke                        | free                        |            nan | True     |                                                                                                                                                                                                                                                                         |
| qwen-14b-chat                | free                        |           8192 | True     |                                                                                                                                                                                                                                                                         |
| bge-reranker-base            | free                        |           8192 | True     |                                                                                                                                                                                                                                                                         |
| bge-reranker-v2-m3           | free                        |           8192 | True     |                                                                                                                                                                                                                                                                         |
| xverse-13b-256k              | free                        |         256000 | True     |                                                                                                                                                                                                                                                                         |
| yi-6b-base                   | free                        |           4096 | True     |                                                                                                                                                                                                                                                                         |
| yi-6b-200k                   | free                        |         200000 | True     |                                                                                                                                                                                                                                                                         |
| yi-6b-chat                   | free                        |           4096 | True     |                                                                                                                                                                                                                                                                         |


# Quick Start 

## OpenAI Client

```python
from openai import OpenAI

client = OpenAI(
    api_key=f"{user_name}:{user_password}",
    base_url="https://modelhub.puyuan.tech/api/v1"
)

client.chat.completions.create(..., model="self-host-models")
```

## ModelhubClient

### Initialization
```python
from modelhub import ModelhubClient

client = ModelhubClient(
    host="https://modelhub.puyuan.tech/api/",
    user_name="xxxx",
    user_password="xxxx",
    model="xxx", # Optional
)
```

### Get supported models

```python
client.supported_models
```

### Create a stateless chat
```python
response = client.chat(
    query,
    model="xxx", # Optional(be None to use the model specific in initialization)
    history=history,
    parameters=dict(
        key1=value1,
        key2=value2
    )
)
```

## Get embeddings

```python
client.get_embeddings(["ä½ å¥½", "Hello"], model="m3e")
```

### Jina Embedding V3

Task-Specific Embedding: Customize embeddings through the task argument with the following options:
- retrieval.query: Used for query embeddings in asymmetric retrieval tasks
- retrieval.passage: Used for passage embeddings in asymmetric retrieval tasks
- separation: Used for embeddings in clustering and re-ranking applications
- classification: Used for embeddings in classification tasks
- text-matching: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks

```python
client.get_embeddings(["ä½ å¥½", "Hello"], model="jina-embedding-v3", parameters={
    "task_type": "retrieval.query"
})
```

## Context Compression/Distillation

Chat using [lingua](https://github.com/microsoft/LLMLingua) will return a compressed/distillated context. Currently we use `Llama-2-7B-Chat-GPTQ` as LLMlingua backend. Theorically, any local model(Baichuan, ChatGLM, etc.) which can be loaded using `AutoModelForCasualLM` can be used as the backend, thus should provide a `compress` API for every local model, this is a future work since `LLMlingua` doesn't support it naively.

Parameters for `lingua` model:

```python
client.chat(
    prompt: str,
    model = "lingua",
    history: List[Dict[str, str]],
    parameters = dict(
        question: str = "",
        ratio: float = 0.5,
        target_token: float = -1,
        iterative_size: int = 200,
        force_context_ids: List[int] = None,
        force_context_number: int = None,
        use_sentence_level_filter: bool = False,
        use_context_level_filter: bool = True,
        use_token_level_filter: bool = True,
        keep_split: bool = False,
        keep_first_sentence: int = 0,
        keep_last_sentence: int = 0,
        keep_sentence_number: int = 0,
        high_priority_bonus: int = 100,
        context_budget: str = "+100",
        token_budget_ratio: float = 1.4,
        condition_in_question: str = "none",
        reorder_context: str = "original",
        dynamic_context_compression_ratio: float = 0.0,
        condition_compare: bool = False,
        add_instruction: bool = False,
        rank_method: str = "llmlingua",
        concate_question: bool = True,
    )
)
```

## Async Support

Every sync method has the corresponding async one starts with "a"(See [API Documentation](#ModelhubClient) below). For example:

Use async mechanism to make concurrent requests.

**Note** Unlike API models, local models are now single threaded, requested will be queued when using async. In the future, we need to adopt a more flexible inference pipeline. [Github Topic](https://github.com/topics/llm-inference)

```python
import anyio

async with anyio.create_task_group() as tg:
    async def query(question):
        print(await client.achat(question, model="gpt-3.5-turbo"))
    questions = ["hello", "nihao", "test", "test1", "test2"]
    for q in questions:
        tg.start_soon(query, q)
```

### `gemini-pro` embedding need extra parameters

Use the `embed_content` method to generate embeddings. The method handles embedding for the following tasks (`task_type`):

| Task Type           | Description                                                                                                    |
| ------------------- | -------------------------------------------------------------------------------------------------------------- |
| RETRIEVAL_QUERY     | Specifies the given text is a query in a search/retrieval setting.                                             |
| RETRIEVAL_DOCUMENT  | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`. |
| SEMANTIC_SIMILARITY | Specifies the given text will be used for Semantic Textual Similarity (STS).                                   |
| CLASSIFICATION      | Specifies that the embeddings will be used for classification.                                                 |
| CLUSTERING          | Specifies that the embeddings will be used for clustering.                                                     |


## Response

```yaml
generated_text: response_text from model
history: generated history, **only chatglm3 return this currently.**
details: generation details. Include tokens used, request duration, ...
```

## History Parameter

You can either use list of pre-defined message types or raw dicts containing `role` and `content` KV as history.

**Note that not every model support role type like `system`**

```python
# import some pre-defined message types
from modelhub.common.types import SystemMessage, AIMessage, UserMessage
# construct history of your own
history = [
    SystemMessage(content="xxx", other_value="xxxx"),
    UserMessage(content="xxx", other="xxxx"),
]
```


# Examples

## Use ChatCLM3 for tools calling

```python
from modelhub import ModelhubClient, VLMClient
from modelhub.common.types import SystemMessage

client = ModelhubClient(
    host="https://xxxxx/api/",
    user_name="xxxxx",
    user_password="xxxxx",
)
tools = [
    {
        "name": "track",
        "description": "è¿½è¸ªæŒ‡å®šè‚¡ç¥¨çš„å®æ—¶ä»·æ ¼",
        "parameters": {
            "type": "object",
            "properties": {"symbol": {"description": "éœ€è¦è¿½è¸ªçš„è‚¡ç¥¨ä»£ç "}},
            "required": ["symbol"],
        },
    },
    {
        "name": "text-to-speech",
        "description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³",
        "parameters": {
            "type": "object",
            "properties": {
                "text": {"description": "éœ€è¦è½¬æ¢æˆè¯­éŸ³çš„æ–‡æœ¬"},
                "voice": {"description": "è¦ä½¿ç”¨çš„è¯­éŸ³ç±»å‹ï¼ˆç”·å£°ã€å¥³å£°ç­‰ï¼‰"},
                "speed": {"description": "è¯­éŸ³çš„é€Ÿåº¦ï¼ˆå¿«ã€ä¸­ç­‰ã€æ…¢ç­‰ï¼‰"},
            },
            "required": ["text"],
        },
    },
]

# construct system history
history = [
    SystemMessage(
        content="Answer the following questions as best as you can. You have access to the following tools:",
        tools=tools,
    )
]
query = "å¸®æˆ‘æŸ¥è¯¢è‚¡ç¥¨10111çš„ä»·æ ¼"

# call ChatGLM3
response = client.chat(query, model="ChatGLM3", history=history)
history = response.history
print(response.generated_text)
```
```shell
Output:
{"name": "track", "parameters": {"symbol": "10111"}}
```

```python
# generate a fake result for track function call

result = {"price": 1232}

res = client.chat(
    json.dumps(result),
    parameters=dict(role="observation"), # Tell ChatGLM3 this is a function call result
    model="ChatGLM3",
    history=history,
)
print(res.generated_text)
```

```shell
Output:
æ ¹æ®APIè°ƒç”¨ç»“æœï¼Œæˆ‘å¾—çŸ¥å½“å‰è‚¡ç¥¨çš„ä»·æ ¼ä¸º1232ã€‚è¯·é—®æ‚¨éœ€è¦æˆ‘ä¸ºæ‚¨åšä»€ä¹ˆï¼Ÿ
```
# Contact
