{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/MADS-DAV/blob/main/notebooks/6.4_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use transfer learning to extract more types of features from our texts. There are a few things to consider when taking this approach:\n",
    "\n",
    "- the models are pretrained on a corpus of texts. This can have a lot of impact; consider for example the difference between a \"sentiment\" model trained on movie reviews and one trained on tweets. The might grasps sort of the same concept, but the words used and the way they are used are different. If you are going to use this model to estimate the sentiment of, let's say, emails sent in a business context, you might get unexpected results.\n",
    "- There are really a lot of different models on huggingface. It can be usefull to browse around in the model hub to see what is available, and try to find something that is close to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment models\n",
    "Sentiment models follow the following recipe:\n",
    "\n",
    "1. map strings into tokens (arbitray integers)\n",
    "2. map tokens into embeddings; this are high dimensional (eg 784 dimensions) vectors that represent the meaning of the words\n",
    "3. do a lot of non-linear transformations on the embedding \n",
    "4. the final embedding is reduced from 784 dimension back to either a single value, sometimes into three values (positive, neutral, negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# models are downloaded to ~/.cache/huggingface/hub.\n",
    "# you might want to clean up that location after you are done with the models\n",
    "model = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    top_k=None,\n",
    "    truncation=True,           # Truncate long inputs automatically\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# english\n",
    "model(\"I love this movie and i would watch it again and again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this the first time, it will download the model from huggingface hub.\n",
    "The second run will be much faster. You will get three outputs: positive, neutral and negative. The sum of these three is 1, because it is a probability distribution.\n",
    "\n",
    "It would be straightforward to use this on your own dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "configfile = Path(\"../config.toml\").resolve()\n",
    "with configfile.open(\"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "datafile = (Path(\"..\") / Path(config[\"processed\"]) / config[\"current\"]).resolve()\n",
    "if not datafile.exists():\n",
    "    logger.warning(\"Datafile does not exist. First run src/preprocess.py, and check the timestamp!\")\n",
    "df = pd.read_parquet(datafile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Author:\n",
    "    name: str\n",
    "    alltext: str\n",
    "    chunked: list[str]\n",
    "    sentiment: dict = None\n",
    "\n",
    "# extract the data from the dataframe\n",
    "datadict = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    author = row[\"author\"]\n",
    "    message = row[\"message\"]\n",
    "    if author not in datadict:\n",
    "        datadict[author] = Author(name=author, alltext=message, chunked=[])\n",
    "    else:\n",
    "        datadict[author].alltext += message\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size=512):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Create chunks of the specified size\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# For every author, try to split their text into chunks of 512 tokens\n",
    "for author in datadict:\n",
    "    # Get the combined text for the author\n",
    "    text = datadict[author].alltext\n",
    "    # Split the text into chunks\n",
    "    # we want 512 tokens, so lets guess about 50%\n",
    "    datadict[author].chunked = split_into_chunks(text, chunk_size=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_sentiment(model, author: Author) -> Author:\n",
    "    sentiment = [model(chunk)[0] for chunk in author.chunked]\n",
    "    result = {}\n",
    "    for item in sentiment:\n",
    "        for entry in item:\n",
    "            # Get the label and the score\n",
    "            label = entry['label']\n",
    "            score = round(entry['score'], 4)  # Round the score to 4 decimal places\n",
    "\n",
    "            # Append the score to the corresponding list in the dictionary\n",
    "            if label not in result:\n",
    "                result[label] = []  # Initialize a list if the label is not in the dictionary\n",
    "            result[label].append(score)\n",
    "    author.sentiment = result\n",
    "    return author\n",
    "\n",
    "for key, item in tqdm(datadict.items(), total=len(datadict)):\n",
    "    try:\n",
    "        datadict[key] = fill_sentiment(model, item)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to process message {key}\")\n",
    "        logger.warning(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate collective positive average\n",
    "total = []\n",
    "for item in datadict.values():\n",
    "    total.extend(item.sentiment[\"positive\"])\n",
    "avg = np.mean(total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_format = []\n",
    "mood = \"negative\"\n",
    "for key, item in datadict.items():\n",
    "    # only keep authors with more than 10 chunks of data\n",
    "    if len(item.sentiment[mood]) < 5:\n",
    "        continue\n",
    "    for val in item.sentiment[mood]:\n",
    "        long_format.append({\"name\" : key, mood: val})\n",
    "long_df = pd.DataFrame(long_format).sort_values(by=mood, ascending=False)\n",
    "long_df.head()\n",
    "sns.stripplot(x='name', y=mood, data=long_df, jitter=True, alpha=0.5)\n",
    "plt.axhline(avg, color='red', linestyle='--')\n",
    "plt.xticks(rotation=90, ha='center');\n",
    "plt.title(f\"Sentiment Analysis: {mood} sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg(x):\n",
    "    return (\n",
    "        len(x.sentiment[\"positive\"]),\n",
    "        np.mean(x.sentiment[\"positive\"]),\n",
    "        np.median(x.sentiment[\"positive\"]),\n",
    "        np.std(x.sentiment[\"positive\"]),\n",
    "        np.mean(x.sentiment[\"negative\"]),\n",
    "        np.median(x.sentiment[\"negative\"]),\n",
    "        np.std(x.sentiment[\"negative\"]),\n",
    "        )\n",
    "\n",
    "aggregated = [((item.name,) + agg(item))for item in datadict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.DataFrame(aggregated, columns=[\"author\", \"count\", \"mean_pos\", \"median_pos\", \"std_pos\", \"mean_neg\", \"median_neg\", \"std_neg\"])\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"_pos\"\n",
    "min_count = 2\n",
    "df_filtered = df_sentiment[df_sentiment[\"count\"] > min_count].\\\n",
    "    sort_values(\"mean\" + key, ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting for mean_pos\n",
    "sns.pointplot(\n",
    "    data=df_filtered,\n",
    "    x='author',\n",
    "    y=\"mean\" + key,\n",
    "    linestyle='none',\n",
    "    capsize=0.2,\n",
    "    err_kws={'linewidth': 1},\n",
    "    errorbar=None,\n",
    "    color='blue'\n",
    ");\n",
    "\n",
    "plt.errorbar(\n",
    "    x=df_filtered['author'],\n",
    "    y=df_filtered[\"mean\" + key],\n",
    "    yerr=df_filtered[\"std\" + key] * 2,\n",
    "    fmt='o',\n",
    "    color='blue',\n",
    "    ecolor='blue',\n",
    "    elinewidth=1,\n",
    "    capsize=4,\n",
    "    label=f'{key} Mean with Error Bars'\n",
    ");\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=1)\n",
    "plt.xticks(rotation=45, ha='right');  # 'ha' stands for horizontal alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping text to a semantic vectorspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of mapping the result of the non-linear transformations to just one dimension of sentiment, we can pick a more general model that doesnt do this. This model is \"just\" trained on a lot of textual data, and the output vectors will represent the meaning of the text in a high dimensional space. This can be used to compare the meaning of different texts, or to use as input for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we input two sentences, and the output in this case are two vectors, 384 dimensions each. \n",
    "\n",
    "I will try to filter the text that are too short (eg more than just \"hi\" and \"hello\") to see if we can get a bit more interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_length'] = df['message'].str.len()\n",
    "sns.histplot(x=np.log(df[\"message_length\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/message_length.png\" width=450 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My dataset seems to have a median message length of log(x) = 4, so lets take 5 as a cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[np.log(df[\"message_length\"]) > 3].reset_index(drop=True)\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new class to keep metadata and the output neatly together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Embedding:\n",
    "    metadata: list\n",
    "    vectors: np.ndarray\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return (self.vectors[idx], self.metadata[idx])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Embedding, dims={self.vectors.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "metadata = {}\n",
    "text = []\n",
    "for idx, row in tqdm(subset.iterrows(), total=len(subset)):\n",
    "    author = row[\"author\"]\n",
    "    message = row[\"message\"]\n",
    "    timestamp = row[\"timestamp\"]\n",
    "    metadata[idx] = {\"author\": author, \"message\": message, \"timestamp\": timestamp}\n",
    "    text.append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the model to encode every message. \n",
    "if the length of your text is not too big, this will work in one go\n",
    "otherwise, you might want to split the text into smaller chunks, encode the chunks,\n",
    "and then concatenate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.encode(text)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And store it in our dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(metadata, vectors)\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our `__getitem__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = emb[1]\n",
    "X.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualise this would be with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(emb.vectors)\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = [emb.metadata[i][\"author\"] for i in range(len(emb.metadata))]\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tSNE is often better for visualising high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X = tsne.fit_transform(emb.vectors)\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = [emb.metadata[i][\"author\"] for i in range(len(emb.metadata))]\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be the case that you just get a blob of points with no clear clustering. Some things to consider:\n",
    "\n",
    "- you might have just too much authors, and if you look more closely you might still find that some authors are more similar than others\n",
    "- you text messages might overall be too short and too similar. You might need to filter out more messages, or group messages of the same author together and then encode them.\n",
    "- you might want to add more structure. Eg, label some messages by hand (or with a regex) and use that as a coloring. You might find that some type of message actually do cluster together in a relevant way, just not clustered by author but more by subject.\n",
    "\n",
    "Please keep in mind that normally, when doing unsupervised clustering, you will have some idea of what you are looking for. For example, you might be looking for fraud, or you are looking for a certain sentiment, or for a specific topic. A typical strategy would be to hand-label a few items and then calculate the distance to find \"close\" items you didn't label yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_author = {}\n",
    "for i in range(len(emb)):\n",
    "    # for every embedding\n",
    "    X, y = emb[i]\n",
    "    # we store the embedding in a list per author, and average it later\n",
    "    avg_author[y[\"author\"]] = avg_author.get(y[\"author\"], []) + [X]\n",
    "\n",
    "for author, vectors in avg_author.items():\n",
    "    # take the average of all embeddings per author\n",
    "    avg_author[author] = np.mean(vectors, axis=0)\n",
    "# We extract all values as a single matrix\n",
    "A = np.array(list(avg_author.values()))\n",
    "labels = list(avg_author.keys())\n",
    "A.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(A, yticklabels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the distance between the average vector for every author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "D = distance_matrix(A, A)\n",
    "sns.heatmap(D, yticklabels=labels, xticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reduce the \"distance\" fingerprint to two dimensions and plot it.\n",
    "This will show us which authors are in a similar way close to other authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(D)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels)\n",
    "plt.legend(title='Author', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
